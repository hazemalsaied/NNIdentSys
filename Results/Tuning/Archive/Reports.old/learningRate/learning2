INFO: activate-binutils_linux-64.sh made the following environmental changes:
+ADDR2LINE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-addr2line
+AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ar
+AS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-as
+CXXFILT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++filt
+ELFEDIT=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-elfedit
+GPROF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gprof
+HOST=x86_64-conda_cos6-linux-gnu
+LD_GOLD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld.gold
+LD=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ld
+NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-nm
+OBJCOPY=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objcopy
+OBJDUMP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-objdump
+RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-ranlib
+READELF=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-readelf
+SIZE=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-size
+STRINGS=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strings
+STRIP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-strip
INFO: activate-gxx_linux-64.sh made the following environmental changes:
+CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+CXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-c++
+DEBUG_CXXFLAGS=-fvisibility-inlines-hidden -std=c++17 -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+GXX=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-g++
INFO: activate-gcc_linux-64.sh made the following environmental changes:
+CC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cc
+CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -pipe
+_CONDA_PYTHON_SYSCONFIGDATA_NAME=_sysconfigdata_x86_64_conda_cos6_linux_gnu
+CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2
+CPP=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-cpp
+DEBUG_CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-all -fno-plt -Og -g -Wall -Wextra -fvar-tracking-assignments -pipe
+DEBUG_CPPFLAGS=-D_DEBUG -D_FORTIFY_SOURCE=2 -Og
+GCC_AR=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ar
+GCC=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc
+GCC_NM=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-nm
+GCC_RANLIB=/home/halsaied/miniconda2/bin/x86_64-conda_cos6-linux-gnu-gcc-ranlib
+LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 4, 48)        705264      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 4, 24)        5640        input_2[0][0]                    
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 192)          0           embedding_1[0][0]                
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 96)           0           embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 288)          0           flatten_1[0][0]                  
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 24)           6936        concatenate_1[0][0]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 24)           0           dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 8)            200         dropout_1[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.2111 - acc: 0.9417 - val_loss: 8.3390e-05 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0726 - acc: 0.9840 - val_loss: 7.7486e-06 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0626 - acc: 0.9863 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0580 - acc: 0.9873 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0548 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0528 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0510 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0496 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0485 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0476 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0468 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0462 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0457 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0452 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0448 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0443 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0439 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0435 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0432 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0430 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0427 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0423 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0423 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0418 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0417 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0416 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0414 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0411 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0406 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 8.3528e-04 - acc: 0.9998
Epoch 2/40
 - 3s - loss: 5.9052e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 6.5500e-04 - acc: 0.9998
Epoch 4/40
 - 3s - loss: 6.4386e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 5.7290e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 5.3626e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 6.0579e-04 - acc: 0.9998
Epoch 8/40
 - 2s - loss: 4.6266e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 4.0936e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 4.3485e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 2.7950e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 3.8292e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 2.4955e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 3.7681e-04 - acc: 0.9998
Epoch 15/40
 - 2s - loss: 3.1898e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 2.5804e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 3.0112e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 2.4662e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 2.7672e-04 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 1.9519e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 3.0980e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 1.7244e-04 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.4369e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 2.0445e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 2.2348e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 2.7996e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 1.4806e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.6536e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 2.4312e-04 - acc: 0.9999
Epoch 30/40
 - 2s - loss: 1.7494e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 1.4469e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 1.5225e-04 - acc: 0.9999
Epoch 33/40
 - 2s - loss: 1.6052e-04 - acc: 0.9999
Epoch 34/40
 - 2s - loss: 1.7375e-04 - acc: 0.9999
Epoch 35/40
 - 2s - loss: 9.7488e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.6634e-04 - acc: 0.9999
Epoch 37/40
 - 2s - loss: 1.0690e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.6136e-04 - acc: 0.9999
Epoch 39/40
 - 2s - loss: 1.3928e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 1.6047e-04 - acc: 0.9999
# Training time = 0:18:10.324874
# F-Score(Ordinary) = 0.682, Recall: 0.73, Precision: 0.64
# F-Score(lvc) = 0.512, Recall: 0.564, Precision: 0.47
# F-Score(ireflv) = 0.772, Recall: 0.73, Precision: 0.82
# F-Score(id) = 0.698, Recall: 0.814, Precision: 0.611
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 4, 48)        705264      input_3[0][0]                    
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 4, 24)        5640        input_4[0][0]                    
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 192)          0           embedding_3[0][0]                
__________________________________________________________________________________________________
flatten_4 (Flatten)             (None, 96)           0           embedding_4[0][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 288)          0           flatten_3[0][0]                  
                                                                 flatten_4[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 24)           6936        concatenate_2[0][0]              
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 24)           0           dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 8)            200         dropout_2[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.2181 - acc: 0.9398 - val_loss: 1.0366e-04 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0729 - acc: 0.9841 - val_loss: 8.7023e-06 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0632 - acc: 0.9863 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0583 - acc: 0.9875 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0552 - acc: 0.9881 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0529 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0513 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0498 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0487 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0478 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0470 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0462 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0456 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0451 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0446 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0442 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0439 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0434 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0433 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0429 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0425 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0422 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0421 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0418 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0416 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0415 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0410 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0407 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 9.0821e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 7.6869e-04 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 6.5672e-04 - acc: 0.9998
Epoch 4/40
 - 3s - loss: 5.8573e-04 - acc: 0.9998
Epoch 5/40
 - 3s - loss: 7.2491e-04 - acc: 0.9998
Epoch 6/40
 - 3s - loss: 3.7423e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 5.9913e-04 - acc: 0.9998
Epoch 8/40
 - 2s - loss: 4.0723e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 4.8279e-04 - acc: 0.9998
Epoch 10/40
 - 3s - loss: 3.4398e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 3.0933e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 4.1768e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 2.1444e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 3.5835e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 3.8539e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 2.1247e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 2.8866e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 2.4898e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 2.7941e-04 - acc: 0.9998
Epoch 20/40
 - 2s - loss: 2.1302e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 2.2189e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 1.2309e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.8548e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 2.1457e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 1.9266e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 2.3945e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 2.2542e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 1.5806e-04 - acc: 0.9999
Epoch 29/40
 - 2s - loss: 1.6696e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 2.2285e-04 - acc: 0.9999
Epoch 31/40
 - 2s - loss: 2.0729e-04 - acc: 0.9999
Epoch 32/40
 - 2s - loss: 3.0867e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.7783e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 2.0652e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 3.3030e-04 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 1.8766e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 2.1834e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 1.4460e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 1.3296e-04 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 2.4084e-04 - acc: 0.9999
# Training time = 0:17:54.150186
# F-Score(Ordinary) = 0.694, Recall: 0.659, Precision: 0.732
# F-Score(lvc) = 0.516, Recall: 0.445, Precision: 0.614
# F-Score(ireflv) = 0.76, Recall: 0.64, Precision: 0.934
# F-Score(id) = 0.76, Recall: 0.919, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_6 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, 4, 48)        705264      input_5[0][0]                    
__________________________________________________________________________________________________
embedding_6 (Embedding)         (None, 4, 24)        5640        input_6[0][0]                    
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 192)          0           embedding_5[0][0]                
__________________________________________________________________________________________________
flatten_6 (Flatten)             (None, 96)           0           embedding_6[0][0]                
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 288)          0           flatten_5[0][0]                  
                                                                 flatten_6[0][0]                  
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 24)           6936        concatenate_3[0][0]              
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 24)           0           dense_5[0][0]                    
__________________________________________________________________________________________________
dense_6 (Dense)                 (None, 8)            200         dropout_3[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.2142 - acc: 0.9395 - val_loss: 9.4061e-05 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0721 - acc: 0.9843 - val_loss: 1.7703e-05 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0635 - acc: 0.9862 - val_loss: 6.9142e-06 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0589 - acc: 0.9874 - val_loss: 4.2915e-06 - val_acc: 1.0000
Epoch 5/40
 - 26s - loss: 0.0561 - acc: 0.9882 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0541 - acc: 0.9886 - val_loss: 1.7285e-06 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0521 - acc: 0.9890 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0507 - acc: 0.9893 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0497 - acc: 0.9896 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0488 - acc: 0.9898 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0480 - acc: 0.9900 - val_loss: 8.9407e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0471 - acc: 0.9901 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0466 - acc: 0.9902 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0461 - acc: 0.9903 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0455 - acc: 0.9905 - val_loss: 2.5034e-06 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0450 - acc: 0.9906 - val_loss: 2.2650e-06 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0447 - acc: 0.9906 - val_loss: 2.5034e-06 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0443 - acc: 0.9907 - val_loss: 3.8147e-06 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0438 - acc: 0.9908 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0436 - acc: 0.9910 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0434 - acc: 0.9909 - val_loss: 3.5763e-06 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0430 - acc: 0.9911 - val_loss: 4.1723e-06 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0429 - acc: 0.9911 - val_loss: 4.2319e-06 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0426 - acc: 0.9912 - val_loss: 3.1591e-06 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0422 - acc: 0.9913 - val_loss: 3.7551e-06 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0421 - acc: 0.9913 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0419 - acc: 0.9913 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0418 - acc: 0.9914 - val_loss: 4.1723e-06 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0415 - acc: 0.9915 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0414 - acc: 0.9916 - val_loss: 2.5630e-06 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0413 - acc: 0.9915 - val_loss: 3.8147e-06 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0410 - acc: 0.9916 - val_loss: 3.8743e-06 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0409 - acc: 0.9916 - val_loss: 2.0862e-06 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0409 - acc: 0.9917 - val_loss: 2.4438e-06 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0407 - acc: 0.9917 - val_loss: 1.9670e-06 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0405 - acc: 0.9918 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0404 - acc: 0.9918 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0401 - acc: 0.9919 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 5.7100e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 3.9884e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 3.0012e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 2.6455e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 2.1483e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 2.0449e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 2.7680e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 1.7555e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 1.2316e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 6.7259e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.6816e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 9.1132e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.3910e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 5.9424e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 5.3380e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 6.5525e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 2.4660e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 7.0961e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 4.9933e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 4.0632e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.4702e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 5.4093e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 6.8965e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 8.0609e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 5.7678e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.8953e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 7.5736e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 6.1133e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 4.1334e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 3.7654e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 2.5228e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 6.7995e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 2.6306e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 5.3664e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 4.0799e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 4.1820e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.7374e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.9149e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 5.1293e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 5.7301e-05 - acc: 1.0000
# Training time = 0:18:35.396549
# F-Score(Ordinary) = 0.697, Recall: 0.673, Precision: 0.723
# F-Score(lvc) = 0.553, Recall: 0.488, Precision: 0.636
# F-Score(ireflv) = 0.801, Recall: 0.727, Precision: 0.893
# F-Score(id) = 0.695, Recall: 0.772, Precision: 0.632
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_7 (Embedding)         (None, 4, 48)        705264      input_7[0][0]                    
__________________________________________________________________________________________________
embedding_8 (Embedding)         (None, 4, 24)        5640        input_8[0][0]                    
__________________________________________________________________________________________________
flatten_7 (Flatten)             (None, 192)          0           embedding_7[0][0]                
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 96)           0           embedding_8[0][0]                
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 288)          0           flatten_7[0][0]                  
                                                                 flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_7 (Dense)                 (None, 24)           6936        concatenate_4[0][0]              
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 24)           0           dense_7[0][0]                    
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 8)            200         dropout_4[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.2294 - acc: 0.9367 - val_loss: 8.5656e-05 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0769 - acc: 0.9835 - val_loss: 7.3910e-06 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0658 - acc: 0.9858 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0606 - acc: 0.9870 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0572 - acc: 0.9878 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0547 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0528 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0511 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0498 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0487 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0479 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0473 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0465 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0458 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0454 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0449 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0444 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0440 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0438 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0435 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0430 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0427 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0425 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0423 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0420 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0418 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0416 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0414 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0411 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0410 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0409 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0407 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0406 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0405 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0397 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 7.4566e-04 - acc: 0.9996
Epoch 2/40
 - 3s - loss: 4.3194e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 4.6097e-04 - acc: 0.9998
Epoch 4/40
 - 3s - loss: 2.7999e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 2.0003e-04 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 2.0531e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 2.1197e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 1.8508e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 1.4033e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.5069e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 1.1677e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 9.5470e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.2869e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.2882e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 8.3521e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.0710e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 7.2642e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.0270e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 8.0984e-05 - acc: 0.9999
Epoch 20/40
 - 2s - loss: 7.6237e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 7.6099e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 8.9747e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 5.9630e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 6.4760e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 5.9803e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 6.0000e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 8.7576e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1197e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 4.2237e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 3.5164e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 4.4639e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 6.5510e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.1331e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 5.2984e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 7.3920e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 3.4855e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 4.4991e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 3.5859e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 3.1566e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 3.3739e-05 - acc: 1.0000
# Training time = 0:18:28.894400
# F-Score(Ordinary) = 0.571, Recall: 0.466, Precision: 0.736
# F-Score(lvc) = 0.481, Recall: 0.39, Precision: 0.629
# F-Score(ireflv) = 0.769, Recall: 0.671, Precision: 0.902
# F-Score(id) = 0.479, Recall: 0.38, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_9 (InputLayer)            (None, 4)            0                                            
__________________________________________________________________________________________________
input_10 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_9 (Embedding)         (None, 4, 48)        705264      input_9[0][0]                    
__________________________________________________________________________________________________
embedding_10 (Embedding)        (None, 4, 24)        5640        input_10[0][0]                   
__________________________________________________________________________________________________
flatten_9 (Flatten)             (None, 192)          0           embedding_9[0][0]                
__________________________________________________________________________________________________
flatten_10 (Flatten)            (None, 96)           0           embedding_10[0][0]               
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 288)          0           flatten_9[0][0]                  
                                                                 flatten_10[0][0]                 
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 24)           6936        concatenate_5[0][0]              
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 24)           0           dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 8)            200         dropout_5[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.2183 - acc: 0.9398 - val_loss: 6.8071e-05 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0752 - acc: 0.9836 - val_loss: 4.4108e-06 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0653 - acc: 0.9859 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0603 - acc: 0.9871 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0573 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0551 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0532 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0516 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0503 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0491 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0483 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0475 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0467 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0462 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0455 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0451 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0448 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0444 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0439 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0436 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0434 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0430 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0427 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0425 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0423 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0420 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0419 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0416 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0414 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0414 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0411 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0407 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0406 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0401 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0011 - acc: 0.9997
Epoch 2/40
 - 3s - loss: 9.8119e-04 - acc: 0.9996
Epoch 3/40
 - 3s - loss: 6.1209e-04 - acc: 0.9998
Epoch 4/40
 - 3s - loss: 4.5257e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 3.2696e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 3.3987e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 4.9114e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 2.3211e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 3.1892e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 2.1354e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 1.3917e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 2.2484e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 2.8694e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 1.0038e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 1.7659e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 1.3776e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 1.2514e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.3735e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.8470e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.4586e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.6076e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 3.3986e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 4.6239e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 9.6464e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 3.2100e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 6.5814e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.3011e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1417e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 6.2156e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 3.8243e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 3.6801e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 9.0772e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 4.7792e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 4.6142e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 8.1038e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 3.4048e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 8.8559e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.9488e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.8467e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 7.7122e-05 - acc: 1.0000
# Training time = 0:18:16.882373
# F-Score(Ordinary) = 0.691, Recall: 0.684, Precision: 0.698
# F-Score(lvc) = 0.528, Recall: 0.57, Precision: 0.492
# F-Score(ireflv) = 0.767, Recall: 0.639, Precision: 0.959
# F-Score(id) = 0.688, Recall: 0.761, Precision: 0.627
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_11 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_12 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_11 (Embedding)        (None, 4, 48)        705264      input_11[0][0]                   
__________________________________________________________________________________________________
embedding_12 (Embedding)        (None, 4, 24)        5640        input_12[0][0]                   
__________________________________________________________________________________________________
flatten_11 (Flatten)            (None, 192)          0           embedding_11[0][0]               
__________________________________________________________________________________________________
flatten_12 (Flatten)            (None, 96)           0           embedding_12[0][0]               
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 288)          0           flatten_11[0][0]                 
                                                                 flatten_12[0][0]                 
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 24)           6936        concatenate_6[0][0]              
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 24)           0           dense_11[0][0]                   
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 8)            200         dropout_6[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.1491 - acc: 0.9604 - val_loss: 1.0133e-05 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0616 - acc: 0.9865 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0547 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0511 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0489 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0475 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0463 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0453 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0445 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0439 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0433 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0429 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0425 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0422 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0419 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0416 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0413 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0408 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0393 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0393 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 4.4448e-04 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 2.7826e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 3.6031e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 1.8983e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 3.1499e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 2.6438e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 2.8739e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 2.1520e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 1.4416e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.7137e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 1.3014e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 2.1893e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 6.1479e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.8223e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 8.8298e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.2033e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.7069e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 1.3855e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 1.9711e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 1.0844e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.9622e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 4.9702e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 7.5661e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.6142e-04 - acc: 0.9999
Epoch 25/40
 - 2s - loss: 2.1602e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 2.3976e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 1.0158e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.3048e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 2.0981e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 1.5292e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 1.1955e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 1.1912e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.1804e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 1.4711e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 6.5790e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.6956e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 8.8811e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.6725e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 1.4540e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 1.6673e-04 - acc: 0.9999
# Training time = 0:18:05.503111
# F-Score(Ordinary) = 0.654, Recall: 0.626, Precision: 0.685
# F-Score(lvc) = 0.511, Recall: 0.486, Precision: 0.538
# F-Score(ireflv) = 0.775, Recall: 0.748, Precision: 0.803
# F-Score(id) = 0.637, Recall: 0.608, Precision: 0.668
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_13 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_14 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_13 (Embedding)        (None, 4, 48)        705264      input_13[0][0]                   
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 4, 24)        5640        input_14[0][0]                   
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 192)          0           embedding_13[0][0]               
__________________________________________________________________________________________________
flatten_14 (Flatten)            (None, 96)           0           embedding_14[0][0]               
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 288)          0           flatten_13[0][0]                 
                                                                 flatten_14[0][0]                 
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 24)           6936        concatenate_7[0][0]              
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 24)           0           dense_13[0][0]                   
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 8)            200         dropout_7[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.1492 - acc: 0.9606 - val_loss: 5.6625e-06 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0617 - acc: 0.9866 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0551 - acc: 0.9881 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0514 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0491 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0473 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0463 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0452 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0445 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0439 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0433 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0429 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0425 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0421 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0418 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0415 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0409 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0400 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0393 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0390 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 6.5096e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 5.9612e-04 - acc: 0.9998
Epoch 3/40
 - 2s - loss: 5.1618e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 4.5283e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 6.3460e-04 - acc: 0.9998
Epoch 6/40
 - 2s - loss: 2.9986e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 4.8781e-04 - acc: 0.9998
Epoch 8/40
 - 2s - loss: 3.1844e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 3.9876e-04 - acc: 0.9998
Epoch 10/40
 - 2s - loss: 2.5237e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 2.1211e-04 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 3.3414e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 1.5012e-04 - acc: 0.9999
Epoch 14/40
 - 2s - loss: 2.6347e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 3.0731e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 1.5226e-04 - acc: 0.9999
Epoch 17/40
 - 2s - loss: 1.8810e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 1.4923e-04 - acc: 0.9999
Epoch 19/40
 - 2s - loss: 1.6751e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.6078e-04 - acc: 0.9999
Epoch 21/40
 - 2s - loss: 1.4315e-04 - acc: 0.9999
Epoch 22/40
 - 2s - loss: 8.7256e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.0027e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.2524e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.2511e-04 - acc: 0.9999
Epoch 26/40
 - 2s - loss: 1.1132e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 9.9479e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 6.2130e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 9.2083e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 7.5841e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 9.8330e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.5104e-04 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.4195e-04 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.1678e-04 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.0483e-04 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.0185e-04 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 4.4756e-05 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 3.0639e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 3.0859e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 4.2720e-05 - acc: 1.0000
# Training time = 0:17:38.208177
# F-Score(Ordinary) = 0.683, Recall: 0.707, Precision: 0.66
# F-Score(lvc) = 0.484, Recall: 0.527, Precision: 0.447
# F-Score(ireflv) = 0.775, Recall: 0.679, Precision: 0.902
# F-Score(id) = 0.708, Recall: 0.832, Precision: 0.617
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_15 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_16 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, 4, 48)        705264      input_15[0][0]                   
__________________________________________________________________________________________________
embedding_16 (Embedding)        (None, 4, 24)        5640        input_16[0][0]                   
__________________________________________________________________________________________________
flatten_15 (Flatten)            (None, 192)          0           embedding_15[0][0]               
__________________________________________________________________________________________________
flatten_16 (Flatten)            (None, 96)           0           embedding_16[0][0]               
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 288)          0           flatten_15[0][0]                 
                                                                 flatten_16[0][0]                 
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 24)           6936        concatenate_8[0][0]              
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 24)           0           dense_15[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 8)            200         dropout_8[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.1468 - acc: 0.9607 - val_loss: 1.6570e-05 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0623 - acc: 0.9866 - val_loss: 3.3379e-06 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0560 - acc: 0.9881 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0523 - acc: 0.9890 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0501 - acc: 0.9894 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0484 - acc: 0.9898 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0469 - acc: 0.9901 - val_loss: 1.8477e-06 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0459 - acc: 0.9903 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0452 - acc: 0.9905 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0446 - acc: 0.9907 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0440 - acc: 0.9909 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0434 - acc: 0.9909 - val_loss: 8.9407e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0431 - acc: 0.9910 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0427 - acc: 0.9911 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0423 - acc: 0.9912 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0419 - acc: 0.9914 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0417 - acc: 0.9914 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0414 - acc: 0.9915 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0411 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0409 - acc: 0.9917 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0405 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0400 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0393 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 3.2486e-04 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 2.0792e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 1.6118e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 1.6572e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 1.0247e-04 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.4390e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.5164e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 1.1673e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 7.0406e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.9438e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.1290e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 3.3737e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.0610e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 2.4639e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 2.1800e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 2.3441e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.9781e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 4.6644e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 3.4385e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.7854e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.0810e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 3.5805e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 4.7122e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 2.4536e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 2.7958e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.6062e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 5.1541e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 3.8373e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.9062e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.5967e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.2674e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 2.5260e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.0619e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 2.3790e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.1688e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.6207e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.6971e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.0133e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 2.8087e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 3.3301e-05 - acc: 1.0000
# Training time = 0:17:41.902101
# F-Score(Ordinary) = 0.676, Recall: 0.631, Precision: 0.727
# F-Score(lvc) = 0.55, Recall: 0.48, Precision: 0.644
# F-Score(ireflv) = 0.787, Recall: 0.713, Precision: 0.877
# F-Score(id) = 0.658, Recall: 0.668, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_17 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_18 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_17 (Embedding)        (None, 4, 48)        705264      input_17[0][0]                   
__________________________________________________________________________________________________
embedding_18 (Embedding)        (None, 4, 24)        5640        input_18[0][0]                   
__________________________________________________________________________________________________
flatten_17 (Flatten)            (None, 192)          0           embedding_17[0][0]               
__________________________________________________________________________________________________
flatten_18 (Flatten)            (None, 96)           0           embedding_18[0][0]               
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 288)          0           flatten_17[0][0]                 
                                                                 flatten_18[0][0]                 
__________________________________________________________________________________________________
dense_17 (Dense)                (None, 24)           6936        concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout_9 (Dropout)             (None, 24)           0           dense_17[0][0]                   
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 8)            200         dropout_9[0][0]                  
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.1583 - acc: 0.9580 - val_loss: 3.6359e-06 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0644 - acc: 0.9861 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0566 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0526 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0500 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0481 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0469 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0458 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0449 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0442 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0437 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0433 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0426 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0421 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0419 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0416 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0409 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0406 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0393 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0393 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0386 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 4.5185e-04 - acc: 0.9998
Epoch 2/40
 - 3s - loss: 2.2387e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 2.4109e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 1.3960e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 8.0042e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 8.5211e-05 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 9.9246e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 6.4273e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 6.4619e-05 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 5.9091e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 4.2923e-05 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 5.3839e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 4.3506e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 4.3566e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 4.1071e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 6.7335e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 3.6264e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 5.6916e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 4.6531e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 3.1883e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 2.4233e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 4.5145e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 3.1369e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.9848e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 2.4288e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.8116e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 5.4889e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 6.1113e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.4913e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 3.2568e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.4060e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 3.8300e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 6.8645e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.6095e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 3.3490e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.0568e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.5328e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 9.3674e-06 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.2675e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.4832e-05 - acc: 1.0000
# Training time = 0:18:00.890438
# F-Score(Ordinary) = 0.633, Recall: 0.539, Precision: 0.765
# F-Score(lvc) = 0.522, Recall: 0.429, Precision: 0.667
# F-Score(ireflv) = 0.771, Recall: 0.648, Precision: 0.951
# F-Score(id) = 0.578, Recall: 0.512, Precision: 0.663
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_20 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_19 (Embedding)        (None, 4, 48)        705264      input_19[0][0]                   
__________________________________________________________________________________________________
embedding_20 (Embedding)        (None, 4, 24)        5640        input_20[0][0]                   
__________________________________________________________________________________________________
flatten_19 (Flatten)            (None, 192)          0           embedding_19[0][0]               
__________________________________________________________________________________________________
flatten_20 (Flatten)            (None, 96)           0           embedding_20[0][0]               
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 288)          0           flatten_19[0][0]                 
                                                                 flatten_20[0][0]                 
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 24)           6936        concatenate_10[0][0]             
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 24)           0           dense_19[0][0]                   
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 8)            200         dropout_10[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.1496 - acc: 0.9605 - val_loss: 3.5763e-06 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0636 - acc: 0.9863 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0569 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0529 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0504 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0488 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0473 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0461 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0453 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0445 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0440 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0435 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0430 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0426 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0420 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0417 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0415 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0412 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0407 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0398 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 5.5847e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 5.6969e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 3.4866e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 1.3943e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 3.6133e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 1.3320e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 2.2175e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 2.4926e-04 - acc: 0.9999
Epoch 9/40
 - 2s - loss: 2.2221e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 1.1868e-04 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 7.3257e-05 - acc: 0.9999
Epoch 12/40
 - 2s - loss: 5.6899e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.1387e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 9.0313e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 5.0754e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.3534e-04 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 2.0074e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 2.8703e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.5025e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 7.8669e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 2.2032e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.4409e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.5334e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.6469e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 6.8725e-05 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.7147e-04 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.7824e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.3180e-04 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.6036e-04 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 7.8514e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.3052e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1266e-04 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.7402e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 6.3617e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.5970e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 6.4996e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1323e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 7.0388e-06 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 2.1843e-04 - acc: 0.9999
Epoch 40/40
 - 2s - loss: 6.0175e-05 - acc: 1.0000
# Training time = 0:17:41.067972
# F-Score(Ordinary) = 0.674, Recall: 0.619, Precision: 0.74
# F-Score(lvc) = 0.549, Recall: 0.513, Precision: 0.591
# F-Score(ireflv) = 0.758, Recall: 0.63, Precision: 0.951
# F-Score(id) = 0.653, Recall: 0.643, Precision: 0.663
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_22 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_21 (Embedding)        (None, 4, 48)        705264      input_21[0][0]                   
__________________________________________________________________________________________________
embedding_22 (Embedding)        (None, 4, 24)        5640        input_22[0][0]                   
__________________________________________________________________________________________________
flatten_21 (Flatten)            (None, 192)          0           embedding_21[0][0]               
__________________________________________________________________________________________________
flatten_22 (Flatten)            (None, 96)           0           embedding_22[0][0]               
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 288)          0           flatten_21[0][0]                 
                                                                 flatten_22[0][0]                 
__________________________________________________________________________________________________
dense_21 (Dense)                (None, 24)           6936        concatenate_11[0][0]             
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 24)           0           dense_21[0][0]                   
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 8)            200         dropout_11[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 23s - loss: 0.1104 - acc: 0.9717 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0547 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0496 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0471 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0457 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0448 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0440 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0433 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0426 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0421 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0416 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0414 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0411 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0382 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.2404e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.6065e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 2.2577e-04 - acc: 0.9999
Epoch 4/40
 - 2s - loss: 7.5884e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 2.3437e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 6.7817e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 9.2933e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.1580e-04 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 6.6244e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.9267e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 2.1169e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.4359e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 4.7616e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 2.2358e-05 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.7273e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 5.6661e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 5.6257e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.5154e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 9.7139e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 5.0654e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 9.3909e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 8.7849e-06 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 9.6300e-06 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 5.0664e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.3089e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 5.1036e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 4.6177e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 8.5949e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 5.0075e-05 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 8.8706e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 9.0117e-06 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 6.5321e-06 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 5.1516e-06 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 4.6220e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 4.5656e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 8.8743e-06 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 5.9382e-06 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 4.4790e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 8.4828e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 4.6683e-05 - acc: 1.0000
# Training time = 0:17:47.050544
# F-Score(Ordinary) = 0.636, Recall: 0.553, Precision: 0.747
# F-Score(lvc) = 0.532, Recall: 0.442, Precision: 0.667
# F-Score(ireflv) = 0.759, Recall: 0.684, Precision: 0.852
# F-Score(id) = 0.598, Recall: 0.528, Precision: 0.689
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_23 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_24 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_23 (Embedding)        (None, 4, 48)        705264      input_23[0][0]                   
__________________________________________________________________________________________________
embedding_24 (Embedding)        (None, 4, 24)        5640        input_24[0][0]                   
__________________________________________________________________________________________________
flatten_23 (Flatten)            (None, 192)          0           embedding_23[0][0]               
__________________________________________________________________________________________________
flatten_24 (Flatten)            (None, 96)           0           embedding_24[0][0]               
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 288)          0           flatten_23[0][0]                 
                                                                 flatten_24[0][0]                 
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 24)           6936        concatenate_12[0][0]             
__________________________________________________________________________________________________
dropout_12 (Dropout)            (None, 24)           0           dense_23[0][0]                   
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 8)            200         dropout_12[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.1109 - acc: 0.9718 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0550 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0498 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0471 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0456 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0443 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0436 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0428 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0424 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0419 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0414 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0411 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0408 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0400 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 4.2962e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 4.2964e-04 - acc: 0.9999
Epoch 3/40
 - 2s - loss: 3.6916e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 2.9833e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 5.4077e-04 - acc: 0.9999
Epoch 6/40
 - 2s - loss: 2.9218e-04 - acc: 0.9999
Epoch 7/40
 - 2s - loss: 3.5585e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 2.5066e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 3.9948e-04 - acc: 0.9999
Epoch 10/40
 - 2s - loss: 2.2536e-04 - acc: 0.9999
Epoch 11/40
 - 2s - loss: 1.6926e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 3.8645e-04 - acc: 0.9999
Epoch 13/40
 - 2s - loss: 1.5432e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 3.0250e-04 - acc: 0.9999
Epoch 15/40
 - 2s - loss: 4.1374e-04 - acc: 0.9999
Epoch 16/40
 - 2s - loss: 1.6228e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.9914e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 1.9878e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 9.9463e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 2.1176e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 1.9982e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 1.0139e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.3495e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.6441e-04 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.8495e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 1.8276e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 2.1147e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 1.6430e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 1.3957e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 2.2124e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 2.0769e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 3.2855e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.8831e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 2.3773e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 3.6762e-04 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 2.1233e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 2.1152e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 1.2717e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 9.3974e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 2.1761e-04 - acc: 0.9999
# Training time = 0:18:28.057977
# F-Score(Ordinary) = 0.593, Recall: 0.481, Precision: 0.774
# F-Score(lvc) = 0.434, Recall: 0.326, Precision: 0.652
# F-Score(ireflv) = 0.761, Recall: 0.634, Precision: 0.951
# F-Score(id) = 0.584, Recall: 0.498, Precision: 0.705
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_25 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_26 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_25 (Embedding)        (None, 4, 48)        705264      input_25[0][0]                   
__________________________________________________________________________________________________
embedding_26 (Embedding)        (None, 4, 24)        5640        input_26[0][0]                   
__________________________________________________________________________________________________
flatten_25 (Flatten)            (None, 192)          0           embedding_25[0][0]               
__________________________________________________________________________________________________
flatten_26 (Flatten)            (None, 96)           0           embedding_26[0][0]               
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 288)          0           flatten_25[0][0]                 
                                                                 flatten_26[0][0]                 
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 24)           6936        concatenate_13[0][0]             
__________________________________________________________________________________________________
dropout_13 (Dropout)            (None, 24)           0           dense_25[0][0]                   
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 8)            200         dropout_13[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.1088 - acc: 0.9720 - val_loss: 6.8546e-06 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0556 - acc: 0.9881 - val_loss: 3.6955e-06 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0505 - acc: 0.9893 - val_loss: 9.0599e-06 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0477 - acc: 0.9899 - val_loss: 6.8546e-06 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0462 - acc: 0.9902 - val_loss: 3.8147e-06 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0452 - acc: 0.9904 - val_loss: 2.2650e-06 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0442 - acc: 0.9907 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0435 - acc: 0.9909 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0429 - acc: 0.9911 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0424 - acc: 0.9912 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0421 - acc: 0.9913 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0416 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0413 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0411 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0408 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0404 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0398 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.5244e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.4416e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 8.5186e-05 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.2101e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 3.5348e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.1719e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 8.3472e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 8.8484e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 5.2787e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 8.6983e-06 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 1.0862e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.4205e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 8.2994e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 8.7660e-06 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 7.2102e-06 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.0027e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 2.1574e-04 - acc: 0.9999
Epoch 18/40
 - 2s - loss: 6.2860e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 3.3298e-05 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 3.6454e-06 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 8.8506e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 5.8353e-05 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 3.9424e-05 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.1842e-05 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 9.1627e-06 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.7262e-04 - acc: 0.9999
Epoch 27/40
 - 2s - loss: 8.1959e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 3.2573e-05 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 3.1737e-06 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 4.1509e-06 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 5.1480e-06 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.1010e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 2.9764e-06 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 7.7641e-06 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 5.8912e-06 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 3.2145e-06 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.8480e-04 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 4.4570e-06 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 2.9503e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 2.7330e-05 - acc: 1.0000
# Training time = 0:18:17.648812
# F-Score(Ordinary) = 0.613, Recall: 0.534, Precision: 0.718
# F-Score(lvc) = 0.537, Recall: 0.482, Precision: 0.606
# F-Score(ireflv) = 0.788, Recall: 0.745, Precision: 0.836
# F-Score(id) = 0.535, Recall: 0.441, Precision: 0.679
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_27 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_28 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_27 (Embedding)        (None, 4, 48)        705264      input_27[0][0]                   
__________________________________________________________________________________________________
embedding_28 (Embedding)        (None, 4, 24)        5640        input_28[0][0]                   
__________________________________________________________________________________________________
flatten_27 (Flatten)            (None, 192)          0           embedding_27[0][0]               
__________________________________________________________________________________________________
flatten_28 (Flatten)            (None, 96)           0           embedding_28[0][0]               
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 288)          0           flatten_27[0][0]                 
                                                                 flatten_28[0][0]                 
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 24)           6936        concatenate_14[0][0]             
__________________________________________________________________________________________________
dropout_14 (Dropout)            (None, 24)           0           dense_27[0][0]                   
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 8)            200         dropout_14[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.1164 - acc: 0.9702 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0566 - acc: 0.9878 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0507 - acc: 0.9891 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0479 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0461 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0449 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0439 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0431 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0424 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0419 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0415 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0400 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0379 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0378 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0377 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0377 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0377 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 2.1568e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 1.1490e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.3963e-04 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 4.6728e-05 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 4.3956e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 4.7742e-05 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 2.5733e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.7986e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.6082e-05 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 3.3914e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.5499e-05 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 2.6963e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1692e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.3910e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 5.0363e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.0395e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.7365e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 2.6769e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 2.7218e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 7.8718e-06 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 4.8493e-06 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 4.5215e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.9947e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 6.7352e-06 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 7.3103e-06 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 5.9990e-06 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.6065e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.3824e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 5.0854e-06 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 5.0352e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.0333e-05 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 5.2037e-06 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.5138e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 8.4273e-06 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.8352e-06 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 7.7028e-06 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 6.7121e-06 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 4.3980e-06 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 4.7458e-06 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.1129e-05 - acc: 1.0000
# Training time = 0:18:13.788980
# F-Score(Ordinary) = 0.616, Recall: 0.514, Precision: 0.767
# F-Score(lvc) = 0.535, Recall: 0.466, Precision: 0.629
# F-Score(ireflv) = 0.784, Recall: 0.675, Precision: 0.934
# F-Score(id) = 0.526, Recall: 0.422, Precision: 0.699
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_29 (Embedding)        (None, 4, 48)        705264      input_29[0][0]                   
__________________________________________________________________________________________________
embedding_30 (Embedding)        (None, 4, 24)        5640        input_30[0][0]                   
__________________________________________________________________________________________________
flatten_29 (Flatten)            (None, 192)          0           embedding_29[0][0]               
__________________________________________________________________________________________________
flatten_30 (Flatten)            (None, 96)           0           embedding_30[0][0]               
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 288)          0           flatten_29[0][0]                 
                                                                 flatten_30[0][0]                 
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 24)           6936        concatenate_15[0][0]             
__________________________________________________________________________________________________
dropout_15 (Dropout)            (None, 24)           0           dense_29[0][0]                   
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 8)            200         dropout_15[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.1095 - acc: 0.9723 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0560 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0508 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0479 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0462 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0451 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0440 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0431 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0425 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0420 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0416 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0407 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0392 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.5230e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.3621e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.5655e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.9803e-04 - acc: 0.9999
Epoch 5/40
 - 2s - loss: 4.3053e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 8.3024e-05 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 1.5010e-04 - acc: 0.9999
Epoch 8/40
 - 2s - loss: 1.5579e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 9.3164e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 9.2492e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 3.8370e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.0843e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.2963e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 3.2410e-06 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 2.7633e-05 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 3.5541e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 7.9353e-06 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 3.8355e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.0103e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.0980e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 9.0655e-05 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 8.0333e-06 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 4.3632e-06 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.0323e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 2.4437e-06 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.6496e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 8.7017e-05 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 7.3741e-06 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 7.2588e-06 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 7.9796e-06 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 3.3795e-06 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 2.6142e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 2.4827e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 2.0946e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 2.4113e-05 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.8120e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 4.4252e-06 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.8199e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 2.4771e-05 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 4.5945e-06 - acc: 1.0000
# Training time = 0:18:07.803074
# F-Score(Ordinary) = 0.636, Recall: 0.545, Precision: 0.763
# F-Score(lvc) = 0.509, Recall: 0.421, Precision: 0.644
# F-Score(ireflv) = 0.753, Recall: 0.624, Precision: 0.951
# F-Score(id) = 0.608, Recall: 0.55, Precision: 0.679
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_31 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_32 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_31 (Embedding)        (None, 4, 48)        705264      input_31[0][0]                   
__________________________________________________________________________________________________
embedding_32 (Embedding)        (None, 4, 24)        5640        input_32[0][0]                   
__________________________________________________________________________________________________
flatten_31 (Flatten)            (None, 192)          0           embedding_31[0][0]               
__________________________________________________________________________________________________
flatten_32 (Flatten)            (None, 96)           0           embedding_32[0][0]               
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 288)          0           flatten_31[0][0]                 
                                                                 flatten_32[0][0]                 
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 24)           6936        concatenate_16[0][0]             
__________________________________________________________________________________________________
dropout_16 (Dropout)            (None, 24)           0           dense_31[0][0]                   
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 8)            200         dropout_16[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0839 - acc: 0.9790 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0505 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0472 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0457 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0446 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0438 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0431 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0423 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0419 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0414 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0412 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0409 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0406 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 2.0748e-04 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.8997e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 3.9870e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 1.2032e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 3.8318e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 3.2196e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 3.6695e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 2.5881e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 1.5480e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 2.0268e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 1.5095e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 2.9453e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 5.0558e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 2.4133e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 9.6757e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.4316e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 2.3366e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 1.8483e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 2.7226e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 1.3499e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 2.6504e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 4.4974e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 8.7872e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 2.1535e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 2.9528e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 3.2964e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 1.2215e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.6074e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 2.7567e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 1.9323e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 1.5243e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 1.5047e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.4848e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 1.8266e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 7.2563e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 2.1369e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 1.0542e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.0688e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 1.6936e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 1.9921e-04 - acc: 0.9999
# Training time = 0:18:19.768966
# F-Score(Ordinary) = 0.649, Recall: 0.56, Precision: 0.772
# F-Score(lvc) = 0.503, Recall: 0.423, Precision: 0.621
# F-Score(ireflv) = 0.775, Recall: 0.679, Precision: 0.902
# F-Score(id) = 0.636, Recall: 0.554, Precision: 0.746
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_33 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_34 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_33 (Embedding)        (None, 4, 48)        705264      input_33[0][0]                   
__________________________________________________________________________________________________
embedding_34 (Embedding)        (None, 4, 24)        5640        input_34[0][0]                   
__________________________________________________________________________________________________
flatten_33 (Flatten)            (None, 192)          0           embedding_33[0][0]               
__________________________________________________________________________________________________
flatten_34 (Flatten)            (None, 96)           0           embedding_34[0][0]               
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 288)          0           flatten_33[0][0]                 
                                                                 flatten_34[0][0]                 
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 24)           6936        concatenate_17[0][0]             
__________________________________________________________________________________________________
dropout_17 (Dropout)            (None, 24)           0           dense_33[0][0]                   
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 8)            200         dropout_17[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0836 - acc: 0.9793 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0506 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0472 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0453 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0442 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0431 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0424 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0418 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0415 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0409 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 6.7617e-05 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 2.6898e-05 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 2.5420e-05 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.2814e-05 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.8136e-04 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 9.0972e-05 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 8.4210e-06 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.7354e-04 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.7250e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 4.2941e-06 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 4.5227e-06 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.7043e-04 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 2.7095e-06 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 3.2003e-06 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.6955e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 8.5211e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 2.6412e-06 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 2.1368e-06 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 3.9641e-06 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.6744e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 8.4166e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 8.4074e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.7578e-06 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 8.4247e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 8.5507e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 2.2585e-06 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 2.1325e-06 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.3626e-06 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 8.3666e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.9380e-06 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 8.1153e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.5828e-04 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 2.3351e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.5526e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 7.8842e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.5395e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.5476e-06 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.2556e-06 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.0677e-06 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.2553e-06 - acc: 1.0000
# Training time = 0:18:19.363962
# F-Score(Ordinary) = 0.579, Recall: 0.465, Precision: 0.765
# F-Score(lvc) = 0.395, Recall: 0.285, Precision: 0.644
# F-Score(ireflv) = 0.776, Recall: 0.663, Precision: 0.934
# F-Score(id) = 0.585, Recall: 0.506, Precision: 0.694
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_35 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_36 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_35 (Embedding)        (None, 4, 48)        705264      input_35[0][0]                   
__________________________________________________________________________________________________
embedding_36 (Embedding)        (None, 4, 24)        5640        input_36[0][0]                   
__________________________________________________________________________________________________
flatten_35 (Flatten)            (None, 192)          0           embedding_35[0][0]               
__________________________________________________________________________________________________
flatten_36 (Flatten)            (None, 96)           0           embedding_36[0][0]               
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 288)          0           flatten_35[0][0]                 
                                                                 flatten_36[0][0]                 
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 24)           6936        concatenate_18[0][0]             
__________________________________________________________________________________________________
dropout_18 (Dropout)            (None, 24)           0           dense_35[0][0]                   
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 8)            200         dropout_18[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0832 - acc: 0.9792 - val_loss: 6.4373e-06 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0508 - acc: 0.9890 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0475 - acc: 0.9898 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0458 - acc: 0.9902 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0446 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0436 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0427 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0421 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0418 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0413 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0406 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0390 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0390 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0386 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0386 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.0370e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 1.5432e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 8.7117e-06 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.5181e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 4.1460e-06 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 1.7817e-04 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 4.9150e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 9.2053e-05 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 3.9758e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 3.9061e-06 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 3.8766e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 4.7439e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 9.0676e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 3.4734e-07 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 7.6808e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 3.2397e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 4.6462e-05 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 2.0417e-06 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 8.6597e-07 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 3.6542e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1685e-06 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 8.3899e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 8.4916e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 2.9467e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 4.4478e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 2.8494e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 6.0383e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 5.6187e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 5.1528e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 2.6497e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 2.2208e-06 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.5098e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 2.2539e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 2.4701e-06 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.8031e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.6123e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 2.2644e-06 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.9270e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 2.9742e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 3.6360e-07 - acc: 1.0000
# Training time = 0:18:03.806880
# F-Score(Ordinary) = 0.605, Recall: 0.501, Precision: 0.763
# F-Score(lvc) = 0.484, Recall: 0.396, Precision: 0.621
# F-Score(ireflv) = 0.801, Recall: 0.697, Precision: 0.943
# F-Score(id) = 0.53, Recall: 0.43, Precision: 0.689
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_37 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_38 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_37 (Embedding)        (None, 4, 48)        705264      input_37[0][0]                   
__________________________________________________________________________________________________
embedding_38 (Embedding)        (None, 4, 24)        5640        input_38[0][0]                   
__________________________________________________________________________________________________
flatten_37 (Flatten)            (None, 192)          0           embedding_37[0][0]               
__________________________________________________________________________________________________
flatten_38 (Flatten)            (None, 96)           0           embedding_38[0][0]               
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 288)          0           flatten_37[0][0]                 
                                                                 flatten_38[0][0]                 
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 24)           6936        concatenate_19[0][0]             
__________________________________________________________________________________________________
dropout_19 (Dropout)            (None, 24)           0           dense_37[0][0]                   
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 8)            200         dropout_19[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0856 - acc: 0.9788 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0502 - acc: 0.9891 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0467 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0450 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0438 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0430 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0421 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0415 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0410 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0406 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0406 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 23s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 23s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 23s - loss: 0.0386 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 23s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 23s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0386 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.5479e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 5.4410e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.2886e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 7.5892e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 2.5059e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 9.7208e-06 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 2.8653e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 2.1977e-06 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 2.8255e-06 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.1829e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 2.3183e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 2.8482e-05 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.3410e-05 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 9.6812e-06 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 4.0923e-06 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 5.4945e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 6.3883e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.4328e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.4785e-06 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 5.7445e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.1511e-06 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 6.4964e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 6.7904e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 6.6868e-06 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 1.3988e-06 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 9.1310e-07 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 2.7637e-06 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 9.3994e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.5172e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.8818e-05 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 5.1828e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 3.8403e-06 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 3.7265e-06 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 7.7143e-06 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 6.2513e-06 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 1.3624e-06 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 5.2040e-06 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 1.4403e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 4.8436e-07 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 5.9470e-07 - acc: 1.0000
# Training time = 0:17:55.131023
# F-Score(Ordinary) = 0.642, Recall: 0.549, Precision: 0.772
# F-Score(lvc) = 0.515, Recall: 0.422, Precision: 0.659
# F-Score(ireflv) = 0.742, Recall: 0.627, Precision: 0.91
# F-Score(id) = 0.621, Recall: 0.555, Precision: 0.705
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_39 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_40 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_39 (Embedding)        (None, 4, 48)        705264      input_39[0][0]                   
__________________________________________________________________________________________________
embedding_40 (Embedding)        (None, 4, 24)        5640        input_40[0][0]                   
__________________________________________________________________________________________________
flatten_39 (Flatten)            (None, 192)          0           embedding_39[0][0]               
__________________________________________________________________________________________________
flatten_40 (Flatten)            (None, 96)           0           embedding_40[0][0]               
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 288)          0           flatten_39[0][0]                 
                                                                 flatten_40[0][0]                 
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 24)           6936        concatenate_20[0][0]             
__________________________________________________________________________________________________
dropout_20 (Dropout)            (None, 24)           0           dense_39[0][0]                   
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 8)            200         dropout_20[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0845 - acc: 0.9792 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0511 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0475 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0456 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0444 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0434 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0428 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0421 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0417 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0414 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0411 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0407 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 23s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0388 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0389 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 23s - loss: 0.0389 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 23s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 23s - loss: 0.0389 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0382 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0382 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 1.8575e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 9.3245e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 3.1431e-05 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.9015e-04 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 7.5369e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 7.3375e-06 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 7.4550e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 3.0642e-06 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 1.3512e-04 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.0853e-04 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 6.2878e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.0875e-04 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.6593e-04 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 1.3068e-06 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 2.9298e-06 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 5.6564e-05 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.9234e-06 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 5.2534e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.0855e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 7.6530e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 1.0636e-04 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.4375e-06 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 9.1599e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.5115e-04 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 5.6036e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 4.2947e-05 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.0530e-04 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.1505e-06 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.2783e-06 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 1.4366e-06 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 7.5731e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 3.8590e-05 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 3.4615e-05 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 2.9969e-05 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 2.6391e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 2.5559e-05 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 5.1607e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.0811e-05 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.7281e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 6.2369e-07 - acc: 1.0000
# Training time = 0:17:55.519482
# F-Score(Ordinary) = 0.602, Recall: 0.498, Precision: 0.763
# F-Score(lvc) = 0.501, Recall: 0.414, Precision: 0.636
# F-Score(ireflv) = 0.735, Recall: 0.606, Precision: 0.934
# F-Score(id) = 0.554, Recall: 0.459, Precision: 0.699
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_41 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_42 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_41 (Embedding)        (None, 4, 48)        705264      input_41[0][0]                   
__________________________________________________________________________________________________
embedding_42 (Embedding)        (None, 4, 24)        5640        input_42[0][0]                   
__________________________________________________________________________________________________
flatten_41 (Flatten)            (None, 192)          0           embedding_41[0][0]               
__________________________________________________________________________________________________
flatten_42 (Flatten)            (None, 96)           0           embedding_42[0][0]               
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 288)          0           flatten_41[0][0]                 
                                                                 flatten_42[0][0]                 
__________________________________________________________________________________________________
dense_41 (Dense)                (None, 24)           6936        concatenate_21[0][0]             
__________________________________________________________________________________________________
dropout_21 (Dropout)            (None, 24)           0           dense_41[0][0]                   
__________________________________________________________________________________________________
dense_42 (Dense)                (None, 8)            200         dropout_21[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0765 - acc: 0.9810 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0512 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0483 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0466 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0453 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0447 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0441 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0433 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0426 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0421 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0418 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0418 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0416 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0414 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0412 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0410 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0406 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0406 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0407 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0394 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.7855e-04 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 2.3674e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 4.8086e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 1.2308e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 2.2937e-04 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 2.1003e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 3.0894e-04 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 2.0404e-04 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.9529e-06 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 3.0208e-04 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1966e-06 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 2.0060e-04 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 5.9674e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.0029e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 8.4034e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 9.9579e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.9749e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 8.4012e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.9638e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 9.7982e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 9.7837e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 5.7991e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 9.7195e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 9.7141e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 7.3318e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 2.8805e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 5.2576e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 9.5730e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 9.5569e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 9.5056e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 5.0137e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 9.4710e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 2.8160e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.8646e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 9.2881e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 4.8356e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 9.2647e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.8415e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 4.2666e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 4.4455e-07 - acc: 1.0000
# Training time = 0:18:46.647252
# F-Score(Ordinary) = 0.554, Recall: 0.432, Precision: 0.774
# F-Score(lvc) = 0.44, Recall: 0.328, Precision: 0.667
# F-Score(ireflv) = 0.707, Recall: 0.578, Precision: 0.91
# F-Score(id) = 0.521, Recall: 0.408, Precision: 0.72
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_43 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_44 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_43 (Embedding)        (None, 4, 48)        705264      input_43[0][0]                   
__________________________________________________________________________________________________
embedding_44 (Embedding)        (None, 4, 24)        5640        input_44[0][0]                   
__________________________________________________________________________________________________
flatten_43 (Flatten)            (None, 192)          0           embedding_43[0][0]               
__________________________________________________________________________________________________
flatten_44 (Flatten)            (None, 96)           0           embedding_44[0][0]               
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 288)          0           flatten_43[0][0]                 
                                                                 flatten_44[0][0]                 
__________________________________________________________________________________________________
dense_43 (Dense)                (None, 24)           6936        concatenate_22[0][0]             
__________________________________________________________________________________________________
dropout_22 (Dropout)            (None, 24)           0           dense_43[0][0]                   
__________________________________________________________________________________________________
dense_44 (Dense)                (None, 8)            200         dropout_22[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0756 - acc: 0.9816 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0509 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0480 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0465 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0453 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0442 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0436 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0428 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0424 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0421 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0419 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0418 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0416 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0409 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0406 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0404 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0404 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 3.0211e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 2.5800e-04 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 4.2424e-06 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 9.9998e-05 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 1.0823e-05 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 9.7399e-06 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 2.0165e-06 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 3.0518e-06 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 2.3588e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 1.4331e-06 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1709e-04 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.4970e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 1.3589e-06 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.2603e-06 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.9913e-06 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 9.4962e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.0974e-06 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 1.6473e-06 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1725e-04 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.0759e-06 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 5.7804e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.0123e-06 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1544e-04 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.2110e-06 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1546e-04 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1532e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 8.4325e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.0531e-06 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 9.4180e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 7.3934e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 3.9437e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 8.9409e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 9.8970e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1451e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 6.8507e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1403e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1373e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 4.5799e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1373e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 6.4980e-07 - acc: 1.0000
# Training time = 0:18:03.586705
# F-Score(Ordinary) = 0.541, Recall: 0.426, Precision: 0.74
# F-Score(lvc) = 0.321, Recall: 0.216, Precision: 0.621
# F-Score(ireflv) = 0.736, Recall: 0.611, Precision: 0.926
# F-Score(id) = 0.621, Recall: 0.592, Precision: 0.653
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_45 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_46 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_45 (Embedding)        (None, 4, 48)        705264      input_45[0][0]                   
__________________________________________________________________________________________________
embedding_46 (Embedding)        (None, 4, 24)        5640        input_46[0][0]                   
__________________________________________________________________________________________________
flatten_45 (Flatten)            (None, 192)          0           embedding_45[0][0]               
__________________________________________________________________________________________________
flatten_46 (Flatten)            (None, 96)           0           embedding_46[0][0]               
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 288)          0           flatten_45[0][0]                 
                                                                 flatten_46[0][0]                 
__________________________________________________________________________________________________
dense_45 (Dense)                (None, 24)           6936        concatenate_23[0][0]             
__________________________________________________________________________________________________
dropout_23 (Dropout)            (None, 24)           0           dense_45[0][0]                   
__________________________________________________________________________________________________
dense_46 (Dense)                (None, 8)            200         dropout_23[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0758 - acc: 0.9813 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0507 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0480 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0467 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0451 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0444 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0436 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0432 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0428 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0423 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0422 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0417 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0416 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0415 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0413 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0411 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0409 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0407 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0403 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0403 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0403 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0400 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0398 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.8109e-04 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 2.4912e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 5.1641e-05 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 4.4553e-05 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 6.3625e-06 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.4949e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 3.7124e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 2.6669e-06 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.0327e-06 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 6.0877e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 2.2997e-06 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 6.7914e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 9.7641e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 6.7872e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 9.2393e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.3260e-06 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.5956e-06 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 7.1138e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 3.6127e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1575e-06 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 4.4012e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 5.5047e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 3.9778e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 4.0591e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 9.6428e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 5.3568e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 4.5350e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 4.6962e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 7.6913e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.4837e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 5.4960e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 3.4525e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 3.3551e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 3.6804e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 2.4751e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.4797e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 4.5036e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.4357e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 3.2563e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 5.0821e-07 - acc: 1.0000
# Training time = 0:18:19.923702
# F-Score(Ordinary) = 0.577, Recall: 0.485, Precision: 0.711
# F-Score(lvc) = 0.473, Recall: 0.381, Precision: 0.621
# F-Score(ireflv) = 0.778, Recall: 0.709, Precision: 0.861
# F-Score(id) = 0.506, Recall: 0.42, Precision: 0.637
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_47 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_48 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_47 (Embedding)        (None, 4, 48)        705264      input_47[0][0]                   
__________________________________________________________________________________________________
embedding_48 (Embedding)        (None, 4, 24)        5640        input_48[0][0]                   
__________________________________________________________________________________________________
flatten_47 (Flatten)            (None, 192)          0           embedding_47[0][0]               
__________________________________________________________________________________________________
flatten_48 (Flatten)            (None, 96)           0           embedding_48[0][0]               
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 288)          0           flatten_47[0][0]                 
                                                                 flatten_48[0][0]                 
__________________________________________________________________________________________________
dense_47 (Dense)                (None, 24)           6936        concatenate_24[0][0]             
__________________________________________________________________________________________________
dropout_24 (Dropout)            (None, 24)           0           dense_47[0][0]                   
__________________________________________________________________________________________________
dense_48 (Dense)                (None, 8)            200         dropout_24[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0765 - acc: 0.9812 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0501 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0474 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0460 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0447 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0439 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0432 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0427 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 23s - loss: 0.0424 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 23s - loss: 0.0421 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0420 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 23s - loss: 0.0416 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 23s - loss: 0.0411 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 23s - loss: 0.0411 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 23s - loss: 0.0414 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 23s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 23s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 23s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0397 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0397 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 23s - loss: 0.0395 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 23s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 23s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 23s - loss: 0.0391 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 23s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.7838e-05 - acc: 1.0000
Epoch 2/40
 - 2s - loss: 1.9410e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 1.4777e-04 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.2768e-06 - acc: 1.0000
Epoch 5/40
 - 2s - loss: 8.9395e-06 - acc: 1.0000
Epoch 6/40
 - 2s - loss: 8.6385e-07 - acc: 1.0000
Epoch 7/40
 - 2s - loss: 9.1931e-06 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 3.5411e-07 - acc: 1.0000
Epoch 9/40
 - 2s - loss: 3.5309e-07 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 2.4815e-05 - acc: 1.0000
Epoch 11/40
 - 2s - loss: 4.5265e-07 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.7497e-06 - acc: 1.0000
Epoch 13/40
 - 2s - loss: 3.8708e-07 - acc: 1.0000
Epoch 14/40
 - 2s - loss: 2.3708e-06 - acc: 1.0000
Epoch 15/40
 - 2s - loss: 1.7195e-07 - acc: 1.0000
Epoch 16/40
 - 2s - loss: 1.3139e-07 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 4.8205e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 2.0318e-05 - acc: 1.0000
Epoch 19/40
 - 2s - loss: 1.1261e-06 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.3292e-07 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 3.2299e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 1.7154e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.2174e-07 - acc: 1.0000
Epoch 24/40
 - 2s - loss: 1.5362e-07 - acc: 1.0000
Epoch 25/40
 - 2s - loss: 2.7031e-06 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.0411e-06 - acc: 1.0000
Epoch 27/40
 - 2s - loss: 1.4159e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 1.2508e-07 - acc: 1.0000
Epoch 29/40
 - 2s - loss: 1.3185e-07 - acc: 1.0000
Epoch 30/40
 - 2s - loss: 2.3456e-06 - acc: 1.0000
Epoch 31/40
 - 2s - loss: 1.8545e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 2.5003e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.5028e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.9808e-07 - acc: 1.0000
Epoch 35/40
 - 2s - loss: 1.3303e-07 - acc: 1.0000
Epoch 36/40
 - 2s - loss: 2.5137e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.2443e-07 - acc: 1.0000
Epoch 38/40
 - 2s - loss: 2.1407e-07 - acc: 1.0000
Epoch 39/40
 - 2s - loss: 1.2730e-06 - acc: 1.0000
Epoch 40/40
 - 2s - loss: 1.9383e-07 - acc: 1.0000
# Training time = 0:18:03.659338
# F-Score(Ordinary) = 0.588, Recall: 0.471, Precision: 0.781
# F-Score(lvc) = 0.372, Recall: 0.254, Precision: 0.697
# F-Score(ireflv) = 0.714, Recall: 0.591, Precision: 0.902
# F-Score(id) = 0.689, Recall: 0.689, Precision: 0.689
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_49 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_50 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_49 (Embedding)        (None, 4, 48)        705264      input_49[0][0]                   
__________________________________________________________________________________________________
embedding_50 (Embedding)        (None, 4, 24)        5640        input_50[0][0]                   
__________________________________________________________________________________________________
flatten_49 (Flatten)            (None, 192)          0           embedding_49[0][0]               
__________________________________________________________________________________________________
flatten_50 (Flatten)            (None, 96)           0           embedding_50[0][0]               
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 288)          0           flatten_49[0][0]                 
                                                                 flatten_50[0][0]                 
__________________________________________________________________________________________________
dense_49 (Dense)                (None, 24)           6936        concatenate_25[0][0]             
__________________________________________________________________________________________________
dropout_25 (Dropout)            (None, 24)           0           dense_49[0][0]                   
__________________________________________________________________________________________________
dense_50 (Dense)                (None, 8)            200         dropout_25[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = sgd, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0760 - acc: 0.9815 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 23s - loss: 0.0513 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 23s - loss: 0.0485 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 23s - loss: 0.0470 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 23s - loss: 0.0458 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 23s - loss: 0.0450 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 23s - loss: 0.0442 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 23s - loss: 0.0438 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0430 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0425 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 23s - loss: 0.0426 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0422 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0418 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0419 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0421 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0415 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0409 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0411 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0405 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0406 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 23s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 23s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 23s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 23s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 23s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 23s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0400 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 23s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 23s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 2s - loss: 2.2704e-04 - acc: 0.9999
Epoch 2/40
 - 2s - loss: 2.7505e-05 - acc: 1.0000
Epoch 3/40
 - 2s - loss: 5.9592e-05 - acc: 1.0000
Epoch 4/40
 - 2s - loss: 1.2406e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 3.3446e-06 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.5343e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 6.0933e-05 - acc: 1.0000
Epoch 8/40
 - 2s - loss: 1.7118e-06 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.5446e-05 - acc: 1.0000
Epoch 10/40
 - 2s - loss: 5.3245e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 2.4518e-05 - acc: 1.0000
Epoch 12/40
 - 2s - loss: 1.3253e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 5.5700e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.2423e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 4.2373e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.0004e-06 - acc: 1.0000
Epoch 17/40
 - 2s - loss: 1.2914e-07 - acc: 1.0000
Epoch 18/40
 - 2s - loss: 6.7374e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.6310e-06 - acc: 1.0000
Epoch 20/40
 - 2s - loss: 1.2841e-05 - acc: 1.0000
Epoch 21/40
 - 2s - loss: 5.7036e-07 - acc: 1.0000
Epoch 22/40
 - 2s - loss: 3.7126e-07 - acc: 1.0000
Epoch 23/40
 - 2s - loss: 1.2240e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.2235e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1946e-07 - acc: 1.0000
Epoch 26/40
 - 2s - loss: 1.1592e-06 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.2367e-07 - acc: 1.0000
Epoch 28/40
 - 2s - loss: 3.4481e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 9.4403e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 3.2104e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1940e-07 - acc: 1.0000
Epoch 32/40
 - 2s - loss: 1.2130e-07 - acc: 1.0000
Epoch 33/40
 - 2s - loss: 1.2263e-07 - acc: 1.0000
Epoch 34/40
 - 2s - loss: 1.2133e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 2.2983e-06 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 8.2238e-07 - acc: 1.0000
Epoch 37/40
 - 2s - loss: 1.1975e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1933e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 4.7154e-06 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 2.9625e-07 - acc: 1.0000
# Training time = 0:18:18.065666
# F-Score(Ordinary) = 0.648, Recall: 0.564, Precision: 0.761
# F-Score(lvc) = 0.497, Recall: 0.414, Precision: 0.621
# F-Score(ireflv) = 0.812, Recall: 0.727, Precision: 0.918
# F-Score(id) = 0.608, Recall: 0.538, Precision: 0.699
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_51 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_52 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_51 (Embedding)        (None, 4, 48)        705264      input_51[0][0]                   
__________________________________________________________________________________________________
embedding_52 (Embedding)        (None, 4, 24)        5640        input_52[0][0]                   
__________________________________________________________________________________________________
flatten_51 (Flatten)            (None, 192)          0           embedding_51[0][0]               
__________________________________________________________________________________________________
flatten_52 (Flatten)            (None, 96)           0           embedding_52[0][0]               
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 288)          0           flatten_51[0][0]                 
                                                                 flatten_52[0][0]                 
__________________________________________________________________________________________________
dense_51 (Dense)                (None, 24)           6936        concatenate_26[0][0]             
__________________________________________________________________________________________________
dropout_26 (Dropout)            (None, 24)           0           dense_51[0][0]                   
__________________________________________________________________________________________________
dense_52 (Dense)                (None, 8)            200         dropout_26[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.0005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.1183 - acc: 0.9731 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0583 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0566 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0576 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0597 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0615 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0634 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0665 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0696 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0727 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0753 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0783 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0830 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0870 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0894 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0969 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1009 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.1137 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.1169 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.1229 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.1273 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.1314 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.1339 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.1348 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.1342 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.1342 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.1344 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.1346 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.1334 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.1345 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.1371 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.1370 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.1372 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.1372 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1364 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1376 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1379 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1369 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1391 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1384 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0080 - acc: 0.9993
Epoch 2/40
 - 3s - loss: 0.0050 - acc: 0.9993
Epoch 3/40
 - 3s - loss: 0.0027 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 0.0023 - acc: 0.9997
Epoch 5/40
 - 3s - loss: 0.0028 - acc: 0.9997
Epoch 6/40
 - 3s - loss: 0.0032 - acc: 0.9996
Epoch 7/40
 - 3s - loss: 0.0023 - acc: 0.9997
Epoch 8/40
 - 3s - loss: 0.0025 - acc: 0.9997
Epoch 9/40
 - 3s - loss: 0.0027 - acc: 0.9997
Epoch 10/40
 - 3s - loss: 0.0021 - acc: 0.9997
Epoch 11/40
 - 3s - loss: 0.0028 - acc: 0.9997
Epoch 12/40
 - 3s - loss: 0.0032 - acc: 0.9996
Epoch 13/40
 - 3s - loss: 0.0021 - acc: 0.9997
Epoch 14/40
 - 3s - loss: 0.0026 - acc: 0.9997
Epoch 15/40
 - 3s - loss: 0.0021 - acc: 0.9997
Epoch 16/40
 - 3s - loss: 0.0022 - acc: 0.9997
Epoch 17/40
 - 3s - loss: 0.0027 - acc: 0.9996
Epoch 18/40
 - 3s - loss: 0.0021 - acc: 0.9997
Epoch 19/40
 - 3s - loss: 0.0023 - acc: 0.9997
Epoch 20/40
 - 3s - loss: 0.0022 - acc: 0.9997
Epoch 21/40
 - 3s - loss: 0.0016 - acc: 0.9998
Epoch 22/40
 - 3s - loss: 0.0022 - acc: 0.9997
Epoch 23/40
 - 3s - loss: 0.0022 - acc: 0.9997
Epoch 24/40
 - 3s - loss: 0.0022 - acc: 0.9997
Epoch 25/40
 - 3s - loss: 0.0020 - acc: 0.9997
Epoch 26/40
 - 3s - loss: 0.0027 - acc: 0.9996
Epoch 27/40
 - 3s - loss: 0.0022 - acc: 0.9997
Epoch 28/40
 - 3s - loss: 0.0021 - acc: 0.9997
Epoch 29/40
 - 3s - loss: 0.0023 - acc: 0.9997
Epoch 30/40
 - 3s - loss: 0.0025 - acc: 0.9996
Epoch 31/40
 - 3s - loss: 0.0025 - acc: 0.9996
Epoch 32/40
 - 3s - loss: 0.0027 - acc: 0.9996
Epoch 33/40
 - 3s - loss: 0.0021 - acc: 0.9997
Epoch 34/40
 - 3s - loss: 0.0022 - acc: 0.9997
Epoch 35/40
 - 3s - loss: 0.0024 - acc: 0.9996
Epoch 36/40
 - 3s - loss: 0.0027 - acc: 0.9996
Epoch 37/40
 - 3s - loss: 0.0023 - acc: 0.9996
Epoch 38/40
 - 3s - loss: 0.0019 - acc: 0.9997
Epoch 39/40
 - 3s - loss: 0.0026 - acc: 0.9996
Epoch 40/40
 - 3s - loss: 0.0028 - acc: 0.9995
# Training time = 0:19:51.483465
# F-Score(Ordinary) = 0.545, Recall: 0.469, Precision: 0.649
# F-Score(lvc) = 0.498, Recall: 0.489, Precision: 0.508
# F-Score(ireflv) = 0.658, Recall: 0.536, Precision: 0.852
# F-Score(id) = 0.483, Recall: 0.404, Precision: 0.601
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_53 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_54 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_53 (Embedding)        (None, 4, 48)        705264      input_53[0][0]                   
__________________________________________________________________________________________________
embedding_54 (Embedding)        (None, 4, 24)        5640        input_54[0][0]                   
__________________________________________________________________________________________________
flatten_53 (Flatten)            (None, 192)          0           embedding_53[0][0]               
__________________________________________________________________________________________________
flatten_54 (Flatten)            (None, 96)           0           embedding_54[0][0]               
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 288)          0           flatten_53[0][0]                 
                                                                 flatten_54[0][0]                 
__________________________________________________________________________________________________
dense_53 (Dense)                (None, 24)           6936        concatenate_27[0][0]             
__________________________________________________________________________________________________
dropout_27 (Dropout)            (None, 24)           0           dense_53[0][0]                   
__________________________________________________________________________________________________
dense_54 (Dense)                (None, 8)            200         dropout_27[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.0005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.1200 - acc: 0.9734 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0576 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0562 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0567 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0584 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0605 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0630 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0657 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0692 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0706 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0732 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0780 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0800 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0831 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0836 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0868 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0886 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0913 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0933 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0966 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0991 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1002 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1028 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1080 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1118 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1173 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1181 - acc: 0.9853 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1237 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1269 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1295 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1297 - acc: 0.9853 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1331 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1345 - acc: 0.9849 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1384 - acc: 0.9855 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1401 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1414 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1416 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1448 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1485 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1501 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.1259 - acc: 0.9916
Epoch 2/40
 - 3s - loss: 0.1186 - acc: 0.9921
Epoch 3/40
 - 3s - loss: 0.1207 - acc: 0.9919
Epoch 4/40
 - 3s - loss: 0.1210 - acc: 0.9919
Epoch 5/40
 - 3s - loss: 0.1180 - acc: 0.9921
Epoch 6/40
 - 3s - loss: 0.1200 - acc: 0.9919
Epoch 7/40
 - 3s - loss: 0.1146 - acc: 0.9921
Epoch 8/40
 - 3s - loss: 0.1244 - acc: 0.9915
Epoch 9/40
 - 3s - loss: 0.1145 - acc: 0.9922
Epoch 10/40
 - 3s - loss: 0.1127 - acc: 0.9922
Epoch 11/40
 - 3s - loss: 0.1115 - acc: 0.9922
Epoch 12/40
 - 3s - loss: 0.1225 - acc: 0.9916
Epoch 13/40
 - 3s - loss: 0.1092 - acc: 0.9924
Epoch 14/40
 - 3s - loss: 0.1146 - acc: 0.9920
Epoch 15/40
 - 3s - loss: 0.1240 - acc: 0.9912
Epoch 16/40
 - 3s - loss: 0.1125 - acc: 0.9919
Epoch 17/40
 - 3s - loss: 0.1149 - acc: 0.9920
Epoch 18/40
 - 3s - loss: 0.1149 - acc: 0.9919
Epoch 19/40
 - 3s - loss: 0.1171 - acc: 0.9916
Epoch 20/40
 - 3s - loss: 0.1090 - acc: 0.9921
Epoch 21/40
 - 3s - loss: 0.1056 - acc: 0.9925
Epoch 22/40
 - 3s - loss: 0.1128 - acc: 0.9922
Epoch 23/40
 - 3s - loss: 0.1207 - acc: 0.9914
Epoch 24/40
 - 3s - loss: 0.1137 - acc: 0.9918
Epoch 25/40
 - 3s - loss: 0.1093 - acc: 0.9918
Epoch 26/40
 - 3s - loss: 0.1116 - acc: 0.9917
Epoch 27/40
 - 3s - loss: 0.1075 - acc: 0.9921
Epoch 28/40
 - 3s - loss: 0.1042 - acc: 0.9923
Epoch 29/40
 - 3s - loss: 0.1085 - acc: 0.9920
Epoch 30/40
 - 3s - loss: 0.1108 - acc: 0.9918
Epoch 31/40
 - 3s - loss: 0.1049 - acc: 0.9921
Epoch 32/40
 - 3s - loss: 0.1060 - acc: 0.9921
Epoch 33/40
 - 3s - loss: 0.1151 - acc: 0.9913
Epoch 34/40
 - 3s - loss: 0.0987 - acc: 0.9924
Epoch 35/40
 - 3s - loss: 0.1033 - acc: 0.9921
Epoch 36/40
 - 3s - loss: 0.1147 - acc: 0.9915
Epoch 37/40
 - 3s - loss: 0.1008 - acc: 0.9920
Epoch 38/40
 - 3s - loss: 0.0997 - acc: 0.9923
Epoch 39/40
 - 3s - loss: 0.1063 - acc: 0.9918
Epoch 40/40
 - 3s - loss: 0.1048 - acc: 0.9922
# Training time = 0:19:11.855519
# F-Score(Ordinary) = 0.32, Recall: 0.215, Precision: 0.624
# F-Score(lvc) = 0.534, Recall: 0.62, Precision: 0.47
# F-Score(ireflv) = 0.678, Recall: 0.574, Precision: 0.828
# F-Score(id) = 0.455, Recall: 0.383, Precision: 0.56
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_55 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_56 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_55 (Embedding)        (None, 4, 48)        705264      input_55[0][0]                   
__________________________________________________________________________________________________
embedding_56 (Embedding)        (None, 4, 24)        5640        input_56[0][0]                   
__________________________________________________________________________________________________
flatten_55 (Flatten)            (None, 192)          0           embedding_55[0][0]               
__________________________________________________________________________________________________
flatten_56 (Flatten)            (None, 96)           0           embedding_56[0][0]               
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 288)          0           flatten_55[0][0]                 
                                                                 flatten_56[0][0]                 
__________________________________________________________________________________________________
dense_55 (Dense)                (None, 24)           6936        concatenate_28[0][0]             
__________________________________________________________________________________________________
dropout_28 (Dropout)            (None, 24)           0           dense_55[0][0]                   
__________________________________________________________________________________________________
dense_56 (Dense)                (None, 8)            200         dropout_28[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.0005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 26s - loss: 0.1215 - acc: 0.9725 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0578 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0567 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0573 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0592 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0614 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0643 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0661 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0683 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0699 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0747 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0803 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0839 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0882 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0919 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0961 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0972 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1071 - acc: 0.9849 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1175 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1205 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1246 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1285 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1282 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1317 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1310 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1319 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1325 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1351 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1363 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1358 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1354 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1349 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1357 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1351 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1360 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1355 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1354 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1364 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1364 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1384 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0126 - acc: 0.9985
Epoch 2/40
 - 3s - loss: 0.0120 - acc: 0.9986
Epoch 3/40
 - 3s - loss: 0.0109 - acc: 0.9987
Epoch 4/40
 - 3s - loss: 0.0119 - acc: 0.9985
Epoch 5/40
 - 3s - loss: 0.0112 - acc: 0.9985
Epoch 6/40
 - 3s - loss: 0.0122 - acc: 0.9984
Epoch 7/40
 - 3s - loss: 0.0124 - acc: 0.9983
Epoch 8/40
 - 3s - loss: 0.0090 - acc: 0.9987
Epoch 9/40
 - 3s - loss: 0.0110 - acc: 0.9984
Epoch 10/40
 - 3s - loss: 0.0119 - acc: 0.9981
Epoch 11/40
 - 3s - loss: 0.0104 - acc: 0.9983
Epoch 12/40
 - 3s - loss: 0.0102 - acc: 0.9982
Epoch 13/40
 - 3s - loss: 0.0080 - acc: 0.9986
Epoch 14/40
 - 3s - loss: 0.0088 - acc: 0.9983
Epoch 15/40
 - 3s - loss: 0.0084 - acc: 0.9983
Epoch 16/40
 - 3s - loss: 0.0073 - acc: 0.9985
Epoch 17/40
 - 3s - loss: 0.0069 - acc: 0.9985
Epoch 18/40
 - 3s - loss: 0.0073 - acc: 0.9983
Epoch 19/40
 - 3s - loss: 0.0065 - acc: 0.9984
Epoch 20/40
 - 3s - loss: 0.0052 - acc: 0.9986
Epoch 21/40
 - 3s - loss: 0.0056 - acc: 0.9984
Epoch 22/40
 - 3s - loss: 0.0052 - acc: 0.9984
Epoch 23/40
 - 3s - loss: 0.0041 - acc: 0.9986
Epoch 24/40
 - 3s - loss: 0.0035 - acc: 0.9987
Epoch 25/40
 - 3s - loss: 0.0047 - acc: 0.9982
Epoch 26/40
 - 3s - loss: 0.0031 - acc: 0.9987
Epoch 27/40
 - 3s - loss: 0.0031 - acc: 0.9985
Epoch 28/40
 - 3s - loss: 0.0028 - acc: 0.9985
Epoch 29/40
 - 3s - loss: 0.0026 - acc: 0.9984
Epoch 30/40
 - 3s - loss: 0.0024 - acc: 0.9984
Epoch 31/40
 - 3s - loss: 0.0021 - acc: 0.9984
Epoch 32/40
 - 3s - loss: 0.0020 - acc: 0.9982
Epoch 33/40
 - 3s - loss: 0.0014 - acc: 0.9985
Epoch 34/40
 - 3s - loss: 0.0013 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 0.0011 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 8.6976e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 6.9468e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 5.2527e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 4.3338e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 3.0263e-04 - acc: 1.0000
# Training time = 0:19:26.410257
# F-Score(Ordinary) = 0.421, Recall: 0.304, Precision: 0.682
# F-Score(lvc) = 0.515, Recall: 0.608, Precision: 0.447
# F-Score(ireflv) = 0.675, Recall: 0.567, Precision: 0.836
# F-Score(id) = 0.446, Recall: 0.336, Precision: 0.663
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_57 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_58 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_57 (Embedding)        (None, 4, 48)        705264      input_57[0][0]                   
__________________________________________________________________________________________________
embedding_58 (Embedding)        (None, 4, 24)        5640        input_58[0][0]                   
__________________________________________________________________________________________________
flatten_57 (Flatten)            (None, 192)          0           embedding_57[0][0]               
__________________________________________________________________________________________________
flatten_58 (Flatten)            (None, 96)           0           embedding_58[0][0]               
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 288)          0           flatten_57[0][0]                 
                                                                 flatten_58[0][0]                 
__________________________________________________________________________________________________
dense_57 (Dense)                (None, 24)           6936        concatenate_29[0][0]             
__________________________________________________________________________________________________
dropout_29 (Dropout)            (None, 24)           0           dense_57[0][0]                   
__________________________________________________________________________________________________
dense_58 (Dense)                (None, 8)            200         dropout_29[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.0005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.1244 - acc: 0.9719 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0588 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0576 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0588 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0597 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0610 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0629 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0655 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0675 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0703 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0717 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0744 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0769 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0804 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0827 - acc: 0.9855 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0866 - acc: 0.9849 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0897 - acc: 0.9853 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0936 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0959 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1014 - acc: 0.9843 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1045 - acc: 0.9843 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1078 - acc: 0.9840 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1124 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1137 - acc: 0.9837 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1208 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1231 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1269 - acc: 0.9842 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1280 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1338 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1459 - acc: 0.9847 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1505 - acc: 0.9846 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1514 - acc: 0.9846 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1529 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1538 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1530 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1526 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1518 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1552 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1554 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1551 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0145 - acc: 0.9991
Epoch 2/40
 - 3s - loss: 0.0127 - acc: 0.9992
Epoch 3/40
 - 3s - loss: 0.0136 - acc: 0.9991
Epoch 4/40
 - 3s - loss: 0.0162 - acc: 0.9990
Epoch 5/40
 - 3s - loss: 0.0145 - acc: 0.9991
Epoch 6/40
 - 3s - loss: 0.0143 - acc: 0.9991
Epoch 7/40
 - 3s - loss: 0.0125 - acc: 0.9992
Epoch 8/40
 - 3s - loss: 0.0112 - acc: 0.9993
Epoch 9/40
 - 3s - loss: 0.0146 - acc: 0.9991
Epoch 10/40
 - 3s - loss: 0.0123 - acc: 0.9992
Epoch 11/40
 - 3s - loss: 0.0117 - acc: 0.9993
Epoch 12/40
 - 3s - loss: 0.0124 - acc: 0.9992
Epoch 13/40
 - 3s - loss: 0.0106 - acc: 0.9993
Epoch 14/40
 - 3s - loss: 0.0135 - acc: 0.9991
Epoch 15/40
 - 3s - loss: 0.0112 - acc: 0.9993
Epoch 16/40
 - 3s - loss: 0.0153 - acc: 0.9990
Epoch 17/40
 - 3s - loss: 0.0137 - acc: 0.9991
Epoch 18/40
 - 3s - loss: 0.0155 - acc: 0.9990
Epoch 19/40
 - 3s - loss: 0.0130 - acc: 0.9992
Epoch 20/40
 - 3s - loss: 0.0154 - acc: 0.9990
Epoch 21/40
 - 3s - loss: 0.0140 - acc: 0.9991
Epoch 22/40
 - 3s - loss: 0.0130 - acc: 0.9992
Epoch 23/40
 - 3s - loss: 0.0134 - acc: 0.9992
Epoch 24/40
 - 3s - loss: 0.0112 - acc: 0.9993
Epoch 25/40
 - 3s - loss: 0.0148 - acc: 0.9991
Epoch 26/40
 - 3s - loss: 0.0138 - acc: 0.9991
Epoch 27/40
 - 3s - loss: 0.0116 - acc: 0.9993
Epoch 28/40
 - 3s - loss: 0.0134 - acc: 0.9991
Epoch 29/40
 - 3s - loss: 0.0140 - acc: 0.9991
Epoch 30/40
 - 3s - loss: 0.0149 - acc: 0.9991
Epoch 31/40
 - 3s - loss: 0.0155 - acc: 0.9990
Epoch 32/40
 - 3s - loss: 0.0110 - acc: 0.9993
Epoch 33/40
 - 3s - loss: 0.0158 - acc: 0.9990
Epoch 34/40
 - 3s - loss: 0.0109 - acc: 0.9993
Epoch 35/40
 - 3s - loss: 0.0131 - acc: 0.9992
Epoch 36/40
 - 3s - loss: 0.0131 - acc: 0.9991
Epoch 37/40
 - 3s - loss: 0.0128 - acc: 0.9992
Epoch 38/40
 - 3s - loss: 0.0141 - acc: 0.9991
Epoch 39/40
 - 3s - loss: 0.0150 - acc: 0.9990
Epoch 40/40
 - 3s - loss: 0.0138 - acc: 0.9991
# Training time = 0:19:34.507340
# F-Score(Ordinary) = 0.621, Recall: 0.628, Precision: 0.615
# F-Score(lvc) = 0.553, Recall: 0.656, Precision: 0.477
# F-Score(ireflv) = 0.69, Recall: 0.6, Precision: 0.811
# F-Score(id) = 0.589, Recall: 0.616, Precision: 0.565
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_59 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_60 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_59 (Embedding)        (None, 4, 48)        705264      input_59[0][0]                   
__________________________________________________________________________________________________
embedding_60 (Embedding)        (None, 4, 24)        5640        input_60[0][0]                   
__________________________________________________________________________________________________
flatten_59 (Flatten)            (None, 192)          0           embedding_59[0][0]               
__________________________________________________________________________________________________
flatten_60 (Flatten)            (None, 96)           0           embedding_60[0][0]               
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 288)          0           flatten_59[0][0]                 
                                                                 flatten_60[0][0]                 
__________________________________________________________________________________________________
dense_59 (Dense)                (None, 24)           6936        concatenate_30[0][0]             
__________________________________________________________________________________________________
dropout_30 (Dropout)            (None, 24)           0           dense_59[0][0]                   
__________________________________________________________________________________________________
dense_60 (Dense)                (None, 8)            200         dropout_30[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.0005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.1215 - acc: 0.9723 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0576 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0559 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0562 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0576 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0601 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0626 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0663 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0693 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0746 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0816 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0911 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0957 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1056 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1144 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1167 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1231 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1259 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1278 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1277 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1291 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1292 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1293 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1294 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1303 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1301 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1306 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1306 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1303 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1302 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1306 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1323 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1314 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1326 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1318 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1327 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1335 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1327 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1327 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1328 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0149 - acc: 0.9984
Epoch 2/40
 - 3s - loss: 0.0153 - acc: 0.9984
Epoch 3/40
 - 3s - loss: 0.0145 - acc: 0.9984
Epoch 4/40
 - 3s - loss: 0.0145 - acc: 0.9983
Epoch 5/40
 - 3s - loss: 0.0120 - acc: 0.9986
Epoch 6/40
 - 3s - loss: 0.0144 - acc: 0.9982
Epoch 7/40
 - 3s - loss: 0.0163 - acc: 0.9979
Epoch 8/40
 - 3s - loss: 0.0130 - acc: 0.9983
Epoch 9/40
 - 3s - loss: 0.0126 - acc: 0.9983
Epoch 10/40
 - 3s - loss: 0.0090 - acc: 0.9987
Epoch 11/40
 - 3s - loss: 0.0107 - acc: 0.9984
Epoch 12/40
 - 3s - loss: 0.0105 - acc: 0.9984
Epoch 13/40
 - 3s - loss: 0.0093 - acc: 0.9985
Epoch 14/40
 - 3s - loss: 0.0095 - acc: 0.9984
Epoch 15/40
 - 3s - loss: 0.0097 - acc: 0.9983
Epoch 16/40
 - 3s - loss: 0.0114 - acc: 0.9980
Epoch 17/40
 - 3s - loss: 0.0087 - acc: 0.9984
Epoch 18/40
 - 3s - loss: 0.0078 - acc: 0.9984
Epoch 19/40
 - 3s - loss: 0.0069 - acc: 0.9986
Epoch 20/40
 - 3s - loss: 0.0081 - acc: 0.9982
Epoch 21/40
 - 3s - loss: 0.0059 - acc: 0.9986
Epoch 22/40
 - 3s - loss: 0.0068 - acc: 0.9983
Epoch 23/40
 - 3s - loss: 0.0064 - acc: 0.9983
Epoch 24/40
 - 3s - loss: 0.0060 - acc: 0.9983
Epoch 25/40
 - 3s - loss: 0.0057 - acc: 0.9982
Epoch 26/40
 - 3s - loss: 0.0059 - acc: 0.9980
Epoch 27/40
 - 3s - loss: 0.0040 - acc: 0.9985
Epoch 28/40
 - 3s - loss: 0.0031 - acc: 0.9987
Epoch 29/40
 - 3s - loss: 0.0032 - acc: 0.9986
Epoch 30/40
 - 3s - loss: 0.0034 - acc: 0.9983
Epoch 31/40
 - 3s - loss: 0.0028 - acc: 0.9984
Epoch 32/40
 - 3s - loss: 0.0026 - acc: 0.9983
Epoch 33/40
 - 3s - loss: 0.0023 - acc: 0.9984
Epoch 34/40
 - 3s - loss: 0.0018 - acc: 0.9984
Epoch 35/40
 - 3s - loss: 0.0014 - acc: 0.9998
Epoch 36/40
 - 3s - loss: 0.0014 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 0.0013 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 9.0721e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 7.5441e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 6.3757e-04 - acc: 1.0000
# Training time = 0:19:03.430386
# F-Score(Ordinary) = 0.423, Recall: 0.306, Precision: 0.689
# F-Score(lvc) = 0.426, Recall: 0.353, Precision: 0.538
# F-Score(ireflv) = 0.711, Recall: 0.618, Precision: 0.836
# F-Score(id) = 0.453, Recall: 0.351, Precision: 0.637
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_61 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_62 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_61 (Embedding)        (None, 4, 48)        705264      input_61[0][0]                   
__________________________________________________________________________________________________
embedding_62 (Embedding)        (None, 4, 24)        5640        input_62[0][0]                   
__________________________________________________________________________________________________
flatten_61 (Flatten)            (None, 192)          0           embedding_61[0][0]               
__________________________________________________________________________________________________
flatten_62 (Flatten)            (None, 96)           0           embedding_62[0][0]               
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 288)          0           flatten_61[0][0]                 
                                                                 flatten_62[0][0]                 
__________________________________________________________________________________________________
dense_61 (Dense)                (None, 24)           6936        concatenate_31[0][0]             
__________________________________________________________________________________________________
dropout_31 (Dropout)            (None, 24)           0           dense_61[0][0]                   
__________________________________________________________________________________________________
dense_62 (Dense)                (None, 8)            200         dropout_31[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0891 - acc: 0.9804 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0591 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0628 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0693 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0822 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0991 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.1162 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.1165 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1255 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1330 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1340 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1341 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1330 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1338 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1337 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1339 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1332 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1329 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1350 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1346 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1363 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1356 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1359 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1361 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1357 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1372 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1362 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1364 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1381 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1380 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1377 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1373 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1374 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1381 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1388 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1382 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1389 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1416 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1416 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1410 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0504 - acc: 0.9968
Epoch 2/40
 - 3s - loss: 0.0516 - acc: 0.9968
Epoch 3/40
 - 3s - loss: 0.0493 - acc: 0.9969
Epoch 4/40
 - 3s - loss: 0.0351 - acc: 0.9977
Epoch 5/40
 - 3s - loss: 0.0282 - acc: 0.9982
Epoch 6/40
 - 3s - loss: 0.0310 - acc: 0.9980
Epoch 7/40
 - 3s - loss: 0.0310 - acc: 0.9981
Epoch 8/40
 - 3s - loss: 0.0277 - acc: 0.9982
Epoch 9/40
 - 3s - loss: 0.0251 - acc: 0.9984
Epoch 10/40
 - 3s - loss: 0.0225 - acc: 0.9985
Epoch 11/40
 - 3s - loss: 0.0105 - acc: 0.9993
Epoch 12/40
 - 3s - loss: 0.0107 - acc: 0.9993
Epoch 13/40
 - 3s - loss: 0.0104 - acc: 0.9993
Epoch 14/40
 - 3s - loss: 0.0098 - acc: 0.9994
Epoch 15/40
 - 3s - loss: 0.0085 - acc: 0.9994
Epoch 16/40
 - 3s - loss: 0.0117 - acc: 0.9993
Epoch 17/40
 - 3s - loss: 0.0107 - acc: 0.9993
Epoch 18/40
 - 3s - loss: 0.0093 - acc: 0.9994
Epoch 19/40
 - 3s - loss: 0.0089 - acc: 0.9995
Epoch 20/40
 - 3s - loss: 0.0097 - acc: 0.9994
Epoch 21/40
 - 3s - loss: 0.0071 - acc: 0.9996
Epoch 22/40
 - 3s - loss: 0.0074 - acc: 0.9995
Epoch 23/40
 - 3s - loss: 0.0090 - acc: 0.9994
Epoch 24/40
 - 3s - loss: 0.0084 - acc: 0.9995
Epoch 25/40
 - 3s - loss: 0.0091 - acc: 0.9994
Epoch 26/40
 - 3s - loss: 0.0114 - acc: 0.9993
Epoch 27/40
 - 3s - loss: 0.0075 - acc: 0.9995
Epoch 28/40
 - 3s - loss: 0.0095 - acc: 0.9994
Epoch 29/40
 - 3s - loss: 0.0093 - acc: 0.9994
Epoch 30/40
 - 3s - loss: 0.0106 - acc: 0.9993
Epoch 31/40
 - 3s - loss: 0.0099 - acc: 0.9994
Epoch 32/40
 - 3s - loss: 0.0099 - acc: 0.9994
Epoch 33/40
 - 3s - loss: 0.0082 - acc: 0.9995
Epoch 34/40
 - 3s - loss: 0.0094 - acc: 0.9994
Epoch 35/40
 - 3s - loss: 0.0104 - acc: 0.9994
Epoch 36/40
 - 3s - loss: 0.0100 - acc: 0.9994
Epoch 37/40
 - 3s - loss: 0.0108 - acc: 0.9993
Epoch 38/40
 - 3s - loss: 0.0087 - acc: 0.9995
Epoch 39/40
 - 3s - loss: 0.0104 - acc: 0.9994
Epoch 40/40
 - 3s - loss: 0.0120 - acc: 0.9992
# Training time = 0:19:09.920355
# F-Score(Ordinary) = 0.036, Recall: 0.018, Precision: 0.492
# F-Score(lvc) = 0.019, Recall: 0.01, Precision: 0.242
# F-Score(ireflv) = 0.562, Recall: 0.45, Precision: 0.746
# F-Score(id) = 0.055, Recall: 0.03, Precision: 0.44
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_63 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_64 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_63 (Embedding)        (None, 4, 48)        705264      input_63[0][0]                   
__________________________________________________________________________________________________
embedding_64 (Embedding)        (None, 4, 24)        5640        input_64[0][0]                   
__________________________________________________________________________________________________
flatten_63 (Flatten)            (None, 192)          0           embedding_63[0][0]               
__________________________________________________________________________________________________
flatten_64 (Flatten)            (None, 96)           0           embedding_64[0][0]               
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 288)          0           flatten_63[0][0]                 
                                                                 flatten_64[0][0]                 
__________________________________________________________________________________________________
dense_63 (Dense)                (None, 24)           6936        concatenate_32[0][0]             
__________________________________________________________________________________________________
dropout_32 (Dropout)            (None, 24)           0           dense_63[0][0]                   
__________________________________________________________________________________________________
dense_64 (Dense)                (None, 8)            200         dropout_32[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0897 - acc: 0.9805 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0584 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0617 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0664 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0728 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0800 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0873 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0988 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1077 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1174 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1219 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1277 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1317 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1361 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1368 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1355 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1350 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1361 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1373 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1374 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1395 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1398 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1397 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1434 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1477 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1519 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1514 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1518 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1526 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1529 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1524 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1528 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1526 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1545 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1564 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1548 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1561 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1590 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1578 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1590 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.2828 - acc: 0.9824
Epoch 2/40
 - 3s - loss: 0.2754 - acc: 0.9828
Epoch 3/40
 - 3s - loss: 0.2636 - acc: 0.9836
Epoch 4/40
 - 3s - loss: 0.2622 - acc: 0.9837
Epoch 5/40
 - 3s - loss: 0.2788 - acc: 0.9827
Epoch 6/40
 - 3s - loss: 0.2772 - acc: 0.9827
Epoch 7/40
 - 3s - loss: 0.2677 - acc: 0.9833
Epoch 8/40
 - 3s - loss: 0.2895 - acc: 0.9819
Epoch 9/40
 - 3s - loss: 0.2688 - acc: 0.9832
Epoch 10/40
 - 3s - loss: 0.2724 - acc: 0.9830
Epoch 11/40
 - 3s - loss: 0.2659 - acc: 0.9834
Epoch 12/40
 - 3s - loss: 0.2794 - acc: 0.9825
Epoch 13/40
 - 3s - loss: 0.2575 - acc: 0.9839
Epoch 14/40
 - 3s - loss: 0.2762 - acc: 0.9828
Epoch 15/40
 - 3s - loss: 0.2817 - acc: 0.9824
Epoch 16/40
 - 3s - loss: 0.2716 - acc: 0.9830
Epoch 17/40
 - 3s - loss: 0.2743 - acc: 0.9829
Epoch 18/40
 - 3s - loss: 0.2871 - acc: 0.9821
Epoch 19/40
 - 3s - loss: 0.2720 - acc: 0.9830
Epoch 20/40
 - 3s - loss: 0.2789 - acc: 0.9826
Epoch 21/40
 - 3s - loss: 0.2666 - acc: 0.9834
Epoch 22/40
 - 3s - loss: 0.2707 - acc: 0.9831
Epoch 23/40
 - 3s - loss: 0.2752 - acc: 0.9828
Epoch 24/40
 - 3s - loss: 0.2769 - acc: 0.9827
Epoch 25/40
 - 3s - loss: 0.2724 - acc: 0.9829
Epoch 26/40
 - 3s - loss: 0.2732 - acc: 0.9829
Epoch 27/40
 - 3s - loss: 0.2715 - acc: 0.9830
Epoch 28/40
 - 3s - loss: 0.2710 - acc: 0.9830
Epoch 29/40
 - 3s - loss: 0.2669 - acc: 0.9833
Epoch 30/40
 - 3s - loss: 0.2793 - acc: 0.9825
Epoch 31/40
 - 3s - loss: 0.2689 - acc: 0.9832
Epoch 32/40
 - 3s - loss: 0.2563 - acc: 0.9839
Epoch 33/40
 - 3s - loss: 0.2929 - acc: 0.9817
Epoch 34/40
 - 3s - loss: 0.2686 - acc: 0.9831
Epoch 35/40
 - 3s - loss: 0.2724 - acc: 0.9829
Epoch 36/40
 - 3s - loss: 0.2850 - acc: 0.9821
Epoch 37/40
 - 3s - loss: 0.2692 - acc: 0.9831
Epoch 38/40
 - 3s - loss: 0.2694 - acc: 0.9832
Epoch 39/40
 - 3s - loss: 0.2805 - acc: 0.9824
Epoch 40/40
 - 3s - loss: 0.2609 - acc: 0.9836
# Training time = 0:19:11.511746
# F-Score(Ordinary) = 0.511, Recall: 0.415, Precision: 0.664
# F-Score(lvc) = 0.373, Recall: 0.303, Precision: 0.485
# F-Score(ireflv) = 0.781, Recall: 0.746, Precision: 0.82
# F-Score(id) = 0.423, Recall: 0.322, Precision: 0.617
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_65 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_66 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_65 (Embedding)        (None, 4, 48)        705264      input_65[0][0]                   
__________________________________________________________________________________________________
embedding_66 (Embedding)        (None, 4, 24)        5640        input_66[0][0]                   
__________________________________________________________________________________________________
flatten_65 (Flatten)            (None, 192)          0           embedding_65[0][0]               
__________________________________________________________________________________________________
flatten_66 (Flatten)            (None, 96)           0           embedding_66[0][0]               
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 288)          0           flatten_65[0][0]                 
                                                                 flatten_66[0][0]                 
__________________________________________________________________________________________________
dense_65 (Dense)                (None, 24)           6936        concatenate_33[0][0]             
__________________________________________________________________________________________________
dropout_33 (Dropout)            (None, 24)           0           dense_65[0][0]                   
__________________________________________________________________________________________________
dense_66 (Dense)                (None, 8)            200         dropout_33[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 26s - loss: 0.0907 - acc: 0.9802 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0595 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0632 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0682 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0759 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0846 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.1097 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.1234 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1286 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1292 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1281 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1286 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1294 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1287 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1291 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1297 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1302 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1330 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1325 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1358 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1377 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1378 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1375 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1391 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1380 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1382 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1392 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1389 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1394 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1404 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1389 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1396 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1396 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1415 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1406 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1428 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1420 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1405 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1404 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1393 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0116 - acc: 0.9992
Epoch 2/40
 - 3s - loss: 0.0166 - acc: 0.9989
Epoch 3/40
 - 3s - loss: 0.0144 - acc: 0.9991
Epoch 4/40
 - 3s - loss: 0.0153 - acc: 0.9979
Epoch 5/40
 - 3s - loss: 0.0029 - acc: 0.9994
Epoch 6/40
 - 3s - loss: 0.0013 - acc: 0.9998
Epoch 7/40
 - 3s - loss: 8.1363e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 8.0989e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 4.8495e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 3.2291e-04 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 0.0013 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 0.0011 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 7.9916e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 4.7850e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 9.5458e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 7.9318e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 0.0013 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 9.4486e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 7.8507e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 6.2652e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 1.5650e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 9.3661e-04 - acc: 0.9999
Epoch 23/40
 - 3s - loss: 7.7818e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 1.5548e-04 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 9.3047e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 6.1862e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 4.6309e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 0.0011 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 4.6054e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 6.1281e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 0.0014 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 6.0838e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 6.0699e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 6.0560e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 0.0012 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 6.0145e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 6.0007e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 4.4917e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 5.9767e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 1.4929e-04 - acc: 1.0000
# Training time = 0:19:10.148637
# F-Score(Ordinary) = 0.075, Recall: 0.04, Precision: 0.577
# F-Score(lvc) = 0.372, Recall: 0.381, Precision: 0.364
# F-Score(ireflv) = 0.513, Recall: 0.375, Precision: 0.811
# F-Score(id) = 0.345, Recall: 0.252, Precision: 0.549
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_67 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_68 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_67 (Embedding)        (None, 4, 48)        705264      input_67[0][0]                   
__________________________________________________________________________________________________
embedding_68 (Embedding)        (None, 4, 24)        5640        input_68[0][0]                   
__________________________________________________________________________________________________
flatten_67 (Flatten)            (None, 192)          0           embedding_67[0][0]               
__________________________________________________________________________________________________
flatten_68 (Flatten)            (None, 96)           0           embedding_68[0][0]               
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 288)          0           flatten_67[0][0]                 
                                                                 flatten_68[0][0]                 
__________________________________________________________________________________________________
dense_67 (Dense)                (None, 24)           6936        concatenate_34[0][0]             
__________________________________________________________________________________________________
dropout_34 (Dropout)            (None, 24)           0           dense_67[0][0]                   
__________________________________________________________________________________________________
dense_68 (Dense)                (None, 8)            200         dropout_34[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0928 - acc: 0.9797 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0594 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0615 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0669 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0719 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0789 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0838 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0909 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1038 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1131 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1259 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1323 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1356 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1385 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1401 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1406 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1390 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1397 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1399 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1392 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1393 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1409 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1387 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1405 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1407 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1408 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1425 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1406 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1408 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1403 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1408 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1403 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1397 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1398 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1396 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1395 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1399 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1393 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1400 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1395 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0617 - acc: 0.9962
Epoch 2/40
 - 3s - loss: 0.0524 - acc: 0.9967
Epoch 3/40
 - 3s - loss: 0.0570 - acc: 0.9965
Epoch 4/40
 - 3s - loss: 0.0568 - acc: 0.9965
Epoch 5/40
 - 3s - loss: 0.0473 - acc: 0.9971
Epoch 6/40
 - 3s - loss: 0.0533 - acc: 0.9967
Epoch 7/40
 - 3s - loss: 0.0506 - acc: 0.9969
Epoch 8/40
 - 3s - loss: 0.0520 - acc: 0.9968
Epoch 9/40
 - 3s - loss: 0.0518 - acc: 0.9968
Epoch 10/40
 - 3s - loss: 0.0557 - acc: 0.9965
Epoch 11/40
 - 3s - loss: 0.0568 - acc: 0.9965
Epoch 12/40
 - 3s - loss: 0.0569 - acc: 0.9965
Epoch 13/40
 - 3s - loss: 0.0532 - acc: 0.9967
Epoch 14/40
 - 3s - loss: 0.0566 - acc: 0.9965
Epoch 15/40
 - 3s - loss: 0.0553 - acc: 0.9966
Epoch 16/40
 - 3s - loss: 0.0537 - acc: 0.9967
Epoch 17/40
 - 3s - loss: 0.0547 - acc: 0.9966
Epoch 18/40
 - 3s - loss: 0.0546 - acc: 0.9966
Epoch 19/40
 - 3s - loss: 0.0535 - acc: 0.9967
Epoch 20/40
 - 3s - loss: 0.0581 - acc: 0.9964
Epoch 21/40
 - 3s - loss: 0.0533 - acc: 0.9967
Epoch 22/40
 - 3s - loss: 0.0501 - acc: 0.9969
Epoch 23/40
 - 3s - loss: 0.0561 - acc: 0.9965
Epoch 24/40
 - 3s - loss: 0.0489 - acc: 0.9970
Epoch 25/40
 - 3s - loss: 0.0548 - acc: 0.9966
Epoch 26/40
 - 3s - loss: 0.0524 - acc: 0.9967
Epoch 27/40
 - 3s - loss: 0.0504 - acc: 0.9969
Epoch 28/40
 - 3s - loss: 0.0553 - acc: 0.9966
Epoch 29/40
 - 3s - loss: 0.0552 - acc: 0.9966
Epoch 30/40
 - 3s - loss: 0.0522 - acc: 0.9968
Epoch 31/40
 - 3s - loss: 0.0480 - acc: 0.9970
Epoch 32/40
 - 3s - loss: 0.0557 - acc: 0.9965
Epoch 33/40
 - 3s - loss: 0.0538 - acc: 0.9967
Epoch 34/40
 - 3s - loss: 0.0573 - acc: 0.9964
Epoch 35/40
 - 3s - loss: 0.0531 - acc: 0.9967
Epoch 36/40
 - 3s - loss: 0.0544 - acc: 0.9966
Epoch 37/40
 - 3s - loss: 0.0590 - acc: 0.9963
Epoch 38/40
 - 3s - loss: 0.0490 - acc: 0.9970
Epoch 39/40
 - 3s - loss: 0.0527 - acc: 0.9967
Epoch 40/40
 - 3s - loss: 0.0527 - acc: 0.9967
# Training time = 0:18:52.907099
# F-Score(Ordinary) = 0.542, Recall: 0.442, Precision: 0.702
# F-Score(lvc) = 0.524, Recall: 0.56, Precision: 0.492
# F-Score(ireflv) = 0.682, Recall: 0.557, Precision: 0.877
# F-Score(id) = 0.413, Recall: 0.305, Precision: 0.637
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_69 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_70 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_69 (Embedding)        (None, 4, 48)        705264      input_69[0][0]                   
__________________________________________________________________________________________________
embedding_70 (Embedding)        (None, 4, 24)        5640        input_70[0][0]                   
__________________________________________________________________________________________________
flatten_69 (Flatten)            (None, 192)          0           embedding_69[0][0]               
__________________________________________________________________________________________________
flatten_70 (Flatten)            (None, 96)           0           embedding_70[0][0]               
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 288)          0           flatten_69[0][0]                 
                                                                 flatten_70[0][0]                 
__________________________________________________________________________________________________
dense_69 (Dense)                (None, 24)           6936        concatenate_35[0][0]             
__________________________________________________________________________________________________
dropout_35 (Dropout)            (None, 24)           0           dense_69[0][0]                   
__________________________________________________________________________________________________
dense_70 (Dense)                (None, 8)            200         dropout_35[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0907 - acc: 0.9801 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0582 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0612 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0662 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0735 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0851 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.1027 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.1129 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1249 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1290 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1327 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1321 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1323 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1318 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1332 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1332 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1359 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1364 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1369 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1387 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1396 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1394 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1410 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1395 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1417 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1402 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1401 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1400 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1382 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1398 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1402 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1401 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1398 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1403 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1411 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1404 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1402 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1426 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1414 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1421 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0691 - acc: 0.9956
Epoch 2/40
 - 3s - loss: 0.0718 - acc: 0.9955
Epoch 3/40
 - 3s - loss: 0.0628 - acc: 0.9960
Epoch 4/40
 - 3s - loss: 0.0663 - acc: 0.9957
Epoch 5/40
 - 3s - loss: 0.0647 - acc: 0.9959
Epoch 6/40
 - 3s - loss: 0.0696 - acc: 0.9956
Epoch 7/40
 - 3s - loss: 0.0742 - acc: 0.9953
Epoch 8/40
 - 3s - loss: 0.0654 - acc: 0.9958
Epoch 9/40
 - 3s - loss: 0.0721 - acc: 0.9954
Epoch 10/40
 - 3s - loss: 0.0632 - acc: 0.9960
Epoch 11/40
 - 3s - loss: 0.0638 - acc: 0.9959
Epoch 12/40
 - 3s - loss: 0.0581 - acc: 0.9963
Epoch 13/40
 - 3s - loss: 0.0631 - acc: 0.9959
Epoch 14/40
 - 3s - loss: 0.0662 - acc: 0.9957
Epoch 15/40
 - 3s - loss: 0.0709 - acc: 0.9955
Epoch 16/40
 - 3s - loss: 0.0737 - acc: 0.9952
Epoch 17/40
 - 3s - loss: 0.0654 - acc: 0.9958
Epoch 18/40
 - 3s - loss: 0.0599 - acc: 0.9961
Epoch 19/40
 - 3s - loss: 0.0634 - acc: 0.9960
Epoch 20/40
 - 3s - loss: 0.0633 - acc: 0.9959
Epoch 21/40
 - 3s - loss: 0.0613 - acc: 0.9960
Epoch 22/40
 - 3s - loss: 0.0605 - acc: 0.9960
Epoch 23/40
 - 3s - loss: 0.0617 - acc: 0.9960
Epoch 24/40
 - 3s - loss: 0.0682 - acc: 0.9956
Epoch 25/40
 - 3s - loss: 0.0607 - acc: 0.9961
Epoch 26/40
 - 3s - loss: 0.0662 - acc: 0.9956
Epoch 27/40
 - 3s - loss: 0.0636 - acc: 0.9959
Epoch 28/40
 - 3s - loss: 0.0601 - acc: 0.9961
Epoch 29/40
 - 3s - loss: 0.0605 - acc: 0.9961
Epoch 30/40
 - 3s - loss: 0.0693 - acc: 0.9955
Epoch 31/40
 - 3s - loss: 0.0617 - acc: 0.9960
Epoch 32/40
 - 3s - loss: 0.0639 - acc: 0.9958
Epoch 33/40
 - 3s - loss: 0.0659 - acc: 0.9957
Epoch 34/40
 - 3s - loss: 0.0697 - acc: 0.9954
Epoch 35/40
 - 3s - loss: 0.0624 - acc: 0.9959
Epoch 36/40
 - 3s - loss: 0.0708 - acc: 0.9955
Epoch 37/40
 - 3s - loss: 0.0620 - acc: 0.9959
Epoch 38/40
 - 3s - loss: 0.0652 - acc: 0.9958
Epoch 39/40
 - 3s - loss: 0.0616 - acc: 0.9960
Epoch 40/40
 - 3s - loss: 0.0674 - acc: 0.9956
# Training time = 0:19:15.355036
# F-Score(Ordinary) = 0.421, Recall: 0.294, Precision: 0.745
# F-Score(lvc) = 0.354, Recall: 0.246, Precision: 0.636
# F-Score(ireflv) = 0.627, Recall: 0.478, Precision: 0.91
# F-Score(id) = 0.335, Recall: 0.225, Precision: 0.653
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_71 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_72 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_71 (Embedding)        (None, 4, 48)        705264      input_71[0][0]                   
__________________________________________________________________________________________________
embedding_72 (Embedding)        (None, 4, 24)        5640        input_72[0][0]                   
__________________________________________________________________________________________________
flatten_71 (Flatten)            (None, 192)          0           embedding_71[0][0]               
__________________________________________________________________________________________________
flatten_72 (Flatten)            (None, 96)           0           embedding_72[0][0]               
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 288)          0           flatten_71[0][0]                 
                                                                 flatten_72[0][0]                 
__________________________________________________________________________________________________
dense_71 (Dense)                (None, 24)           6936        concatenate_36[0][0]             
__________________________________________________________________________________________________
dropout_36 (Dropout)            (None, 24)           0           dense_71[0][0]                   
__________________________________________________________________________________________________
dense_72 (Dense)                (None, 8)            200         dropout_36[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 26s - loss: 0.0768 - acc: 0.9837 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 26s - loss: 0.0675 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 26s - loss: 0.0841 - acc: 0.9853 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 26s - loss: 0.1248 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 26s - loss: 0.1327 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 26s - loss: 0.1335 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 26s - loss: 0.1353 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 26s - loss: 0.1351 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 26s - loss: 0.1370 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 26s - loss: 0.1369 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 26s - loss: 0.1385 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 26s - loss: 0.1408 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 26s - loss: 0.1383 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 26s - loss: 0.1413 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 26s - loss: 0.1427 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 26s - loss: 0.1413 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 26s - loss: 0.1435 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 26s - loss: 0.1445 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 26s - loss: 0.1442 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 26s - loss: 0.1466 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 26s - loss: 0.1519 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 26s - loss: 0.1555 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 26s - loss: 0.1638 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 26s - loss: 0.1672 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 26s - loss: 0.1685 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 26s - loss: 0.1656 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 26s - loss: 0.1656 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 26s - loss: 0.1615 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 26s - loss: 0.1616 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 26s - loss: 0.1587 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 26s - loss: 0.1574 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 26s - loss: 0.1565 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 26s - loss: 0.1553 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 26s - loss: 0.1561 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 26s - loss: 0.1542 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 26s - loss: 0.1573 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 26s - loss: 0.1592 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1599 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 26s - loss: 0.1645 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 26s - loss: 0.1640 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.1633 - acc: 0.9899
Epoch 2/40
 - 3s - loss: 0.1706 - acc: 0.9894
Epoch 3/40
 - 3s - loss: 0.1675 - acc: 0.9896
Epoch 4/40
 - 3s - loss: 0.1692 - acc: 0.9895
Epoch 5/40
 - 3s - loss: 0.1712 - acc: 0.9894
Epoch 6/40
 - 3s - loss: 0.1646 - acc: 0.9898
Epoch 7/40
 - 3s - loss: 0.1630 - acc: 0.9899
Epoch 8/40
 - 3s - loss: 0.1759 - acc: 0.9891
Epoch 9/40
 - 3s - loss: 0.1734 - acc: 0.9892
Epoch 10/40
 - 3s - loss: 0.1712 - acc: 0.9894
Epoch 11/40
 - 3s - loss: 0.1688 - acc: 0.9895
Epoch 12/40
 - 3s - loss: 0.1715 - acc: 0.9894
Epoch 13/40
 - 3s - loss: 0.1686 - acc: 0.9895
Epoch 14/40
 - 3s - loss: 0.1566 - acc: 0.9903
Epoch 15/40
 - 3s - loss: 0.1617 - acc: 0.9900
Epoch 16/40
 - 3s - loss: 0.1708 - acc: 0.9894
Epoch 17/40
 - 3s - loss: 0.1843 - acc: 0.9886
Epoch 18/40
 - 3s - loss: 0.1717 - acc: 0.9893
Epoch 19/40
 - 3s - loss: 0.1611 - acc: 0.9900
Epoch 20/40
 - 3s - loss: 0.1622 - acc: 0.9899
Epoch 21/40
 - 3s - loss: 0.1639 - acc: 0.9898
Epoch 22/40
 - 3s - loss: 0.1628 - acc: 0.9899
Epoch 23/40
 - 3s - loss: 0.1726 - acc: 0.9893
Epoch 24/40
 - 3s - loss: 0.1695 - acc: 0.9895
Epoch 25/40
 - 3s - loss: 0.1681 - acc: 0.9896
Epoch 26/40
 - 3s - loss: 0.1710 - acc: 0.9894
Epoch 27/40
 - 3s - loss: 0.1703 - acc: 0.9894
Epoch 28/40
 - 3s - loss: 0.1785 - acc: 0.9889
Epoch 29/40
 - 3s - loss: 0.1675 - acc: 0.9896
Epoch 30/40
 - 3s - loss: 0.1701 - acc: 0.9894
Epoch 31/40
 - 3s - loss: 0.1803 - acc: 0.9888
Epoch 32/40
 - 3s - loss: 0.1659 - acc: 0.9897
Epoch 33/40
 - 3s - loss: 0.1611 - acc: 0.9900
Epoch 34/40
 - 3s - loss: 0.1708 - acc: 0.9894
Epoch 35/40
 - 3s - loss: 0.1675 - acc: 0.9896
Epoch 36/40
 - 3s - loss: 0.1768 - acc: 0.9890
Epoch 37/40
 - 3s - loss: 0.1788 - acc: 0.9889
Epoch 38/40
 - 3s - loss: 0.1708 - acc: 0.9894
Epoch 39/40
 - 3s - loss: 0.1644 - acc: 0.9898
Epoch 40/40
 - 3s - loss: 0.1641 - acc: 0.9898
# Training time = 0:19:35.492926
# F-Score(Ordinary) = 0.524, Recall: 0.412, Precision: 0.72
# F-Score(lvc) = 0.613, Recall: 0.813, Precision: 0.492
# F-Score(ireflv) = 0.772, Recall: 0.667, Precision: 0.918
# F-Score(id) = 0.348, Recall: 0.237, Precision: 0.653
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_73 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_74 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_73 (Embedding)        (None, 4, 48)        705264      input_73[0][0]                   
__________________________________________________________________________________________________
embedding_74 (Embedding)        (None, 4, 24)        5640        input_74[0][0]                   
__________________________________________________________________________________________________
flatten_73 (Flatten)            (None, 192)          0           embedding_73[0][0]               
__________________________________________________________________________________________________
flatten_74 (Flatten)            (None, 96)           0           embedding_74[0][0]               
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 288)          0           flatten_73[0][0]                 
                                                                 flatten_74[0][0]                 
__________________________________________________________________________________________________
dense_73 (Dense)                (None, 24)           6936        concatenate_37[0][0]             
__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 24)           0           dense_73[0][0]                   
__________________________________________________________________________________________________
dense_74 (Dense)                (None, 8)            200         dropout_37[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0773 - acc: 0.9837 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0671 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0820 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.1104 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.1315 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.1371 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.1376 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.1381 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1389 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1407 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1425 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1435 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1443 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1458 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1470 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1472 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1486 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1489 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1488 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1494 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1499 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1504 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1498 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1493 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1489 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1493 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1501 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1497 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1494 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1487 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1473 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1485 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1489 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1499 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1494 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1497 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1485 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1475 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1472 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1482 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0978 - acc: 0.9939
Epoch 2/40
 - 3s - loss: 0.0944 - acc: 0.9941
Epoch 3/40
 - 3s - loss: 0.0918 - acc: 0.9943
Epoch 4/40
 - 3s - loss: 0.0812 - acc: 0.9949
Epoch 5/40
 - 3s - loss: 0.0959 - acc: 0.9940
Epoch 6/40
 - 3s - loss: 0.1011 - acc: 0.9937
Epoch 7/40
 - 3s - loss: 0.0908 - acc: 0.9944
Epoch 8/40
 - 3s - loss: 0.0945 - acc: 0.9941
Epoch 9/40
 - 3s - loss: 0.0923 - acc: 0.9943
Epoch 10/40
 - 3s - loss: 0.0967 - acc: 0.9940
Epoch 11/40
 - 3s - loss: 0.0945 - acc: 0.9941
Epoch 12/40
 - 3s - loss: 0.0906 - acc: 0.9944
Epoch 13/40
 - 3s - loss: 0.0879 - acc: 0.9945
Epoch 14/40
 - 3s - loss: 0.0883 - acc: 0.9945
Epoch 15/40
 - 3s - loss: 0.0897 - acc: 0.9944
Epoch 16/40
 - 3s - loss: 0.0921 - acc: 0.9943
Epoch 17/40
 - 3s - loss: 0.0894 - acc: 0.9944
Epoch 18/40
 - 3s - loss: 0.0955 - acc: 0.9941
Epoch 19/40
 - 3s - loss: 0.0918 - acc: 0.9943
Epoch 20/40
 - 3s - loss: 0.0896 - acc: 0.9944
Epoch 21/40
 - 3s - loss: 0.0917 - acc: 0.9943
Epoch 22/40
 - 3s - loss: 0.0869 - acc: 0.9946
Epoch 23/40
 - 3s - loss: 0.0983 - acc: 0.9939
Epoch 24/40
 - 3s - loss: 0.0864 - acc: 0.9946
Epoch 25/40
 - 3s - loss: 0.1008 - acc: 0.9937
Epoch 26/40
 - 3s - loss: 0.0954 - acc: 0.9941
Epoch 27/40
 - 3s - loss: 0.0944 - acc: 0.9941
Epoch 28/40
 - 3s - loss: 0.0909 - acc: 0.9943
Epoch 29/40
 - 3s - loss: 0.0909 - acc: 0.9943
Epoch 30/40
 - 3s - loss: 0.1000 - acc: 0.9938
Epoch 31/40
 - 3s - loss: 0.0942 - acc: 0.9941
Epoch 32/40
 - 3s - loss: 0.0895 - acc: 0.9944
Epoch 33/40
 - 3s - loss: 0.0939 - acc: 0.9942
Epoch 34/40
 - 3s - loss: 0.0920 - acc: 0.9943
Epoch 35/40
 - 3s - loss: 0.0869 - acc: 0.9946
Epoch 36/40
 - 3s - loss: 0.0877 - acc: 0.9945
Epoch 37/40
 - 3s - loss: 0.0903 - acc: 0.9944
Epoch 38/40
 - 3s - loss: 0.0887 - acc: 0.9945
Epoch 39/40
 - 3s - loss: 0.0795 - acc: 0.9951
Epoch 40/40
 - 3s - loss: 0.0903 - acc: 0.9944
# Training time = 0:19:17.759727
# F-Score(Ordinary) = 0.325, Recall: 0.207, Precision: 0.752
# F-Score(lvc) = 0.178, Recall: 0.106, Precision: 0.545
# F-Score(ireflv) = 0.641, Recall: 0.483, Precision: 0.951
# F-Score(id) = 0.304, Recall: 0.195, Precision: 0.694
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_75 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_76 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_75 (Embedding)        (None, 4, 48)        705264      input_75[0][0]                   
__________________________________________________________________________________________________
embedding_76 (Embedding)        (None, 4, 24)        5640        input_76[0][0]                   
__________________________________________________________________________________________________
flatten_75 (Flatten)            (None, 192)          0           embedding_75[0][0]               
__________________________________________________________________________________________________
flatten_76 (Flatten)            (None, 96)           0           embedding_76[0][0]               
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 288)          0           flatten_75[0][0]                 
                                                                 flatten_76[0][0]                 
__________________________________________________________________________________________________
dense_75 (Dense)                (None, 24)           6936        concatenate_38[0][0]             
__________________________________________________________________________________________________
dropout_38 (Dropout)            (None, 24)           0           dense_75[0][0]                   
__________________________________________________________________________________________________
dense_76 (Dense)                (None, 8)            200         dropout_38[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 26s - loss: 0.0778 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 26s - loss: 0.0701 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 26s - loss: 0.0889 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 26s - loss: 0.1115 - acc: 0.9847 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 26s - loss: 0.1286 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 26s - loss: 0.1366 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 26s - loss: 0.1371 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 26s - loss: 0.1374 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 26s - loss: 0.1387 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 26s - loss: 0.1405 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.1431 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.1443 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.1449 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.1420 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.1425 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.1446 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 26s - loss: 0.1447 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 26s - loss: 0.1430 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 26s - loss: 0.1426 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 26s - loss: 0.1450 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 26s - loss: 0.1438 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 26s - loss: 0.1430 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 26s - loss: 0.1439 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 26s - loss: 0.1441 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 26s - loss: 0.1436 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 26s - loss: 0.1432 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 26s - loss: 0.1446 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 26s - loss: 0.1438 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 26s - loss: 0.1445 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 26s - loss: 0.1435 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 26s - loss: 0.1429 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 26s - loss: 0.1444 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 26s - loss: 0.1445 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 26s - loss: 0.1442 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 26s - loss: 0.1424 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 26s - loss: 0.1417 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 26s - loss: 0.1402 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 26s - loss: 0.1404 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 26s - loss: 0.1401 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 26s - loss: 0.1398 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0495 - acc: 0.9967
Epoch 2/40
 - 3s - loss: 0.0568 - acc: 0.9965
Epoch 3/40
 - 3s - loss: 0.0541 - acc: 0.9966
Epoch 4/40
 - 3s - loss: 0.0532 - acc: 0.9967
Epoch 5/40
 - 3s - loss: 0.0510 - acc: 0.9968
Epoch 6/40
 - 3s - loss: 0.0503 - acc: 0.9969
Epoch 7/40
 - 3s - loss: 0.0514 - acc: 0.9968
Epoch 8/40
 - 3s - loss: 0.0543 - acc: 0.9966
Epoch 9/40
 - 3s - loss: 0.0526 - acc: 0.9967
Epoch 10/40
 - 3s - loss: 0.0497 - acc: 0.9969
Epoch 11/40
 - 3s - loss: 0.0563 - acc: 0.9965
Epoch 12/40
 - 3s - loss: 0.0557 - acc: 0.9965
Epoch 13/40
 - 3s - loss: 0.0562 - acc: 0.9965
Epoch 14/40
 - 3s - loss: 0.0538 - acc: 0.9967
Epoch 15/40
 - 3s - loss: 0.0531 - acc: 0.9967
Epoch 16/40
 - 3s - loss: 0.0547 - acc: 0.9966
Epoch 17/40
 - 3s - loss: 0.0543 - acc: 0.9966
Epoch 18/40
 - 3s - loss: 0.0574 - acc: 0.9964
Epoch 19/40
 - 3s - loss: 0.0468 - acc: 0.9971
Epoch 20/40
 - 3s - loss: 0.0526 - acc: 0.9967
Epoch 21/40
 - 3s - loss: 0.0529 - acc: 0.9967
Epoch 22/40
 - 3s - loss: 0.0537 - acc: 0.9966
Epoch 23/40
 - 3s - loss: 0.0490 - acc: 0.9970
Epoch 24/40
 - 3s - loss: 0.0482 - acc: 0.9970
Epoch 25/40
 - 3s - loss: 0.0468 - acc: 0.9971
Epoch 26/40
 - 3s - loss: 0.0512 - acc: 0.9968
Epoch 27/40
 - 3s - loss: 0.0522 - acc: 0.9967
Epoch 28/40
 - 3s - loss: 0.0511 - acc: 0.9968
Epoch 29/40
 - 3s - loss: 0.0536 - acc: 0.9967
Epoch 30/40
 - 3s - loss: 0.0559 - acc: 0.9965
Epoch 31/40
 - 3s - loss: 0.0499 - acc: 0.9969
Epoch 32/40
 - 3s - loss: 0.0538 - acc: 0.9966
Epoch 33/40
 - 3s - loss: 0.0466 - acc: 0.9971
Epoch 34/40
 - 3s - loss: 0.0461 - acc: 0.9971
Epoch 35/40
 - 3s - loss: 0.0503 - acc: 0.9969
Epoch 36/40
 - 3s - loss: 0.0512 - acc: 0.9968
Epoch 37/40
 - 3s - loss: 0.0547 - acc: 0.9966
Epoch 38/40
 - 3s - loss: 0.0531 - acc: 0.9967
Epoch 39/40
 - 3s - loss: 0.0431 - acc: 0.9973
Epoch 40/40
 - 3s - loss: 0.0473 - acc: 0.9970
# Training time = 0:19:38.420429
# F-Score(Ordinary) = 0.614, Recall: 0.579, Precision: 0.653
# F-Score(lvc) = 0.517, Recall: 0.481, Precision: 0.561
# F-Score(ireflv) = 0.815, Recall: 0.856, Precision: 0.779
# F-Score(id) = 0.543, Recall: 0.496, Precision: 0.601
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_77 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_78 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_77 (Embedding)        (None, 4, 48)        705264      input_77[0][0]                   
__________________________________________________________________________________________________
embedding_78 (Embedding)        (None, 4, 24)        5640        input_78[0][0]                   
__________________________________________________________________________________________________
flatten_77 (Flatten)            (None, 192)          0           embedding_77[0][0]               
__________________________________________________________________________________________________
flatten_78 (Flatten)            (None, 96)           0           embedding_78[0][0]               
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 288)          0           flatten_77[0][0]                 
                                                                 flatten_78[0][0]                 
__________________________________________________________________________________________________
dense_77 (Dense)                (None, 24)           6936        concatenate_39[0][0]             
__________________________________________________________________________________________________
dropout_39 (Dropout)            (None, 24)           0           dense_77[0][0]                   
__________________________________________________________________________________________________
dense_78 (Dense)                (None, 8)            200         dropout_39[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0798 - acc: 0.9833 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0687 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0790 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.1103 - acc: 0.9853 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.1292 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.1370 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.1447 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.1429 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1455 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1454 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1461 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1495 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1508 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1528 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1487 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1490 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1493 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1483 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1483 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1496 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1517 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1511 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1510 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1486 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1469 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1458 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1461 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1491 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1469 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1492 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1509 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1508 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1489 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1489 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1492 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1586 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1613 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1577 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1578 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1555 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0155 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 5.3722e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.3389e-05 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 5.7084e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.8127e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.4293e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.3005e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.2654e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.2346e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.2278e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:19:30.889278
# F-Score(Ordinary) = 0.203, Recall: 0.119, Precision: 0.698
# F-Score(lvc) = 0.346, Recall: 0.255, Precision: 0.538
# F-Score(ireflv) = 0.653, Recall: 0.517, Precision: 0.885
# F-Score(id) = 0.133, Recall: 0.074, Precision: 0.611
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_79 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_80 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_79 (Embedding)        (None, 4, 48)        705264      input_79[0][0]                   
__________________________________________________________________________________________________
embedding_80 (Embedding)        (None, 4, 24)        5640        input_80[0][0]                   
__________________________________________________________________________________________________
flatten_79 (Flatten)            (None, 192)          0           embedding_79[0][0]               
__________________________________________________________________________________________________
flatten_80 (Flatten)            (None, 96)           0           embedding_80[0][0]               
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 288)          0           flatten_79[0][0]                 
                                                                 flatten_80[0][0]                 
__________________________________________________________________________________________________
dense_79 (Dense)                (None, 24)           6936        concatenate_40[0][0]             
__________________________________________________________________________________________________
dropout_40 (Dropout)            (None, 24)           0           dense_79[0][0]                   
__________________________________________________________________________________________________
dense_80 (Dense)                (None, 8)            200         dropout_40[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 26s - loss: 0.0774 - acc: 0.9837 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 26s - loss: 0.0657 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0781 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0981 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.1240 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.1385 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.1370 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.1379 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.1381 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.1398 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.1412 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.1506 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.1704 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.1717 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.1708 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 26s - loss: 0.1696 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 26s - loss: 0.1713 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 26s - loss: 0.1683 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 26s - loss: 0.1733 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 26s - loss: 0.1698 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 26s - loss: 0.1708 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 26s - loss: 0.1727 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 26s - loss: 0.1736 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 26s - loss: 0.1743 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 26s - loss: 0.1768 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 26s - loss: 0.1771 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 26s - loss: 0.1720 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 26s - loss: 0.1710 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 26s - loss: 0.1708 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 26s - loss: 0.1692 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 26s - loss: 0.1706 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 26s - loss: 0.1705 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 26s - loss: 0.1671 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 26s - loss: 0.1695 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 26s - loss: 0.1678 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 26s - loss: 0.1692 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 26s - loss: 0.1696 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 26s - loss: 0.1677 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 26s - loss: 0.1667 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 26s - loss: 0.1704 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.6369 - acc: 0.9605
Epoch 2/40
 - 3s - loss: 0.6577 - acc: 0.9592
Epoch 3/40
 - 3s - loss: 0.6433 - acc: 0.9601
Epoch 4/40
 - 3s - loss: 0.6492 - acc: 0.9597
Epoch 5/40
 - 3s - loss: 0.6354 - acc: 0.9606
Epoch 6/40
 - 3s - loss: 0.6588 - acc: 0.9591
Epoch 7/40
 - 3s - loss: 0.6658 - acc: 0.9587
Epoch 8/40
 - 3s - loss: 0.6552 - acc: 0.9593
Epoch 9/40
 - 3s - loss: 0.6500 - acc: 0.9597
Epoch 10/40
 - 3s - loss: 0.6504 - acc: 0.9596
Epoch 11/40
 - 3s - loss: 0.6468 - acc: 0.9599
Epoch 12/40
 - 3s - loss: 0.6451 - acc: 0.9600
Epoch 13/40
 - 3s - loss: 0.6406 - acc: 0.9603
Epoch 14/40
 - 3s - loss: 0.6426 - acc: 0.9601
Epoch 15/40
 - 3s - loss: 0.6417 - acc: 0.9602
Epoch 16/40
 - 3s - loss: 0.6493 - acc: 0.9597
Epoch 17/40
 - 3s - loss: 0.6490 - acc: 0.9597
Epoch 18/40
 - 3s - loss: 0.6508 - acc: 0.9596
Epoch 19/40
 - 3s - loss: 0.6440 - acc: 0.9600
Epoch 20/40
 - 3s - loss: 0.6416 - acc: 0.9602
Epoch 21/40
 - 3s - loss: 0.6586 - acc: 0.9591
Epoch 22/40
 - 3s - loss: 0.6442 - acc: 0.9600
Epoch 23/40
 - 3s - loss: 0.6471 - acc: 0.9599
Epoch 24/40
 - 3s - loss: 0.6424 - acc: 0.9601
Epoch 25/40
 - 3s - loss: 0.6513 - acc: 0.9596
Epoch 26/40
 - 3s - loss: 0.6752 - acc: 0.9581
Epoch 27/40
 - 3s - loss: 0.6311 - acc: 0.9608
Epoch 28/40
 - 3s - loss: 0.6566 - acc: 0.9593
Epoch 29/40
 - 3s - loss: 0.6329 - acc: 0.9607
Epoch 30/40
 - 3s - loss: 0.6584 - acc: 0.9592
Epoch 31/40
 - 3s - loss: 0.6548 - acc: 0.9594
Epoch 32/40
 - 3s - loss: 0.6646 - acc: 0.9588
Epoch 33/40
 - 3s - loss: 0.6444 - acc: 0.9600
Epoch 34/40
 - 3s - loss: 0.6604 - acc: 0.9590
Epoch 35/40
 - 3s - loss: 0.6537 - acc: 0.9594
Epoch 36/40
 - 3s - loss: 0.6446 - acc: 0.9600
Epoch 37/40
 - 3s - loss: 0.6555 - acc: 0.9593
Epoch 38/40
 - 3s - loss: 0.6369 - acc: 0.9605
Epoch 39/40
 - 3s - loss: 0.6318 - acc: 0.9608
Epoch 40/40
 - 3s - loss: 0.6511 - acc: 0.9596
# Training time = 0:19:55.589396
# F-Score(Ordinary) = 0.459, Recall: 0.333, Precision: 0.736
# F-Score(lvc) = 0.429, Recall: 0.351, Precision: 0.553
# F-Score(ireflv) = 0.623, Recall: 0.467, Precision: 0.934
# F-Score(id) = 0.368, Recall: 0.256, Precision: 0.658
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_81 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_82 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_81 (Embedding)        (None, 4, 48)        705264      input_81[0][0]                   
__________________________________________________________________________________________________
embedding_82 (Embedding)        (None, 4, 24)        5640        input_82[0][0]                   
__________________________________________________________________________________________________
flatten_81 (Flatten)            (None, 192)          0           embedding_81[0][0]               
__________________________________________________________________________________________________
flatten_82 (Flatten)            (None, 96)           0           embedding_82[0][0]               
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 288)          0           flatten_81[0][0]                 
                                                                 flatten_82[0][0]                 
__________________________________________________________________________________________________
dense_81 (Dense)                (None, 24)           6936        concatenate_41[0][0]             
__________________________________________________________________________________________________
dropout_41 (Dropout)            (None, 24)           0           dense_81[0][0]                   
__________________________________________________________________________________________________
dense_82 (Dense)                (None, 8)            200         dropout_41[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0873 - acc: 0.9833 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.1333 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.1427 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.1517 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.1544 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.1560 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.1563 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.1621 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1565 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1607 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1578 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1590 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1593 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1599 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1614 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1661 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1714 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1709 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1687 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1678 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1676 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1635 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1657 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1649 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1617 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1600 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1592 - acc: 0.9855 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1592 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1589 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1563 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1556 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1561 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1538 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1528 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1533 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1521 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1540 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 26s - loss: 0.1533 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 26s - loss: 0.1508 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1520 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0102 - acc: 0.9972
Epoch 2/40
 - 3s - loss: 1.1875e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 5.2049e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.4514e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.2851e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.2460e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.2284e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.2189e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.2103e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.2061e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.2023e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.2016e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.2016e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.2009e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.2016e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1956e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:19:21.357085
# F-Score(Ordinary) = 0.061, Recall: 0.032, Precision: 0.642
# F-Score(lvc) = 0.347, Recall: 0.28, Precision: 0.455
# F-Score(ireflv) = 0.611, Recall: 0.5, Precision: 0.787
# F-Score(id) = 0.37, Recall: 0.266, Precision: 0.606
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_83 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_84 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_83 (Embedding)        (None, 4, 48)        705264      input_83[0][0]                   
__________________________________________________________________________________________________
embedding_84 (Embedding)        (None, 4, 24)        5640        input_84[0][0]                   
__________________________________________________________________________________________________
flatten_83 (Flatten)            (None, 192)          0           embedding_83[0][0]               
__________________________________________________________________________________________________
flatten_84 (Flatten)            (None, 96)           0           embedding_84[0][0]               
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 288)          0           flatten_83[0][0]                 
                                                                 flatten_84[0][0]                 
__________________________________________________________________________________________________
dense_83 (Dense)                (None, 24)           6936        concatenate_42[0][0]             
__________________________________________________________________________________________________
dropout_42 (Dropout)            (None, 24)           0           dense_83[0][0]                   
__________________________________________________________________________________________________
dense_84 (Dense)                (None, 8)            200         dropout_42[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 26s - loss: 0.0815 - acc: 0.9832 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 26s - loss: 0.1204 - acc: 0.9843 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 26s - loss: 0.1502 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 26s - loss: 0.1514 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 26s - loss: 0.1525 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 26s - loss: 0.1561 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 26s - loss: 0.1569 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 26s - loss: 0.1600 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 26s - loss: 0.1669 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 26s - loss: 0.1793 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 26s - loss: 0.1807 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 26s - loss: 0.1784 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 26s - loss: 0.1811 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 26s - loss: 0.1804 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 26s - loss: 0.1805 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 26s - loss: 0.1789 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 26s - loss: 0.1819 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 26s - loss: 0.1817 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 26s - loss: 0.1806 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 26s - loss: 0.1805 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 26s - loss: 0.1784 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 26s - loss: 0.1741 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 26s - loss: 0.1720 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 26s - loss: 0.1707 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 26s - loss: 0.1736 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 26s - loss: 0.1714 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 26s - loss: 0.1673 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 26s - loss: 0.1672 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 26s - loss: 0.1581 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 28s - loss: 0.1551 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 28s - loss: 0.1565 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 28s - loss: 0.1554 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 28s - loss: 0.1544 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 28s - loss: 0.1578 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 28s - loss: 0.1588 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.1604 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 28s - loss: 0.1570 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 28s - loss: 0.1563 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 28s - loss: 0.1568 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 28s - loss: 0.1558 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.1827 - acc: 0.9886
Epoch 2/40
 - 3s - loss: 0.1856 - acc: 0.9884
Epoch 3/40
 - 3s - loss: 0.1899 - acc: 0.9881
Epoch 4/40
 - 3s - loss: 0.1889 - acc: 0.9882
Epoch 5/40
 - 3s - loss: 0.1927 - acc: 0.9879
Epoch 6/40
 - 3s - loss: 0.1993 - acc: 0.9875
Epoch 7/40
 - 3s - loss: 0.1838 - acc: 0.9885
Epoch 8/40
 - 3s - loss: 0.1830 - acc: 0.9885
Epoch 9/40
 - 3s - loss: 0.1990 - acc: 0.9875
Epoch 10/40
 - 3s - loss: 0.1878 - acc: 0.9881
Epoch 11/40
 - 3s - loss: 0.1893 - acc: 0.9880
Epoch 12/40
 - 3s - loss: 0.1834 - acc: 0.9884
Epoch 13/40
 - 3s - loss: 0.1813 - acc: 0.9886
Epoch 14/40
 - 3s - loss: 0.1854 - acc: 0.9882
Epoch 15/40
 - 3s - loss: 0.1965 - acc: 0.9875
Epoch 16/40
 - 3s - loss: 0.1949 - acc: 0.9876
Epoch 17/40
 - 3s - loss: 0.1765 - acc: 0.9889
Epoch 18/40
 - 3s - loss: 0.1748 - acc: 0.9892
Epoch 19/40
 - 3s - loss: 0.1897 - acc: 0.9882
Epoch 20/40
 - 3s - loss: 0.1877 - acc: 0.9884
Epoch 21/40
 - 3s - loss: 0.1810 - acc: 0.9888
Epoch 22/40
 - 3s - loss: 0.1810 - acc: 0.9888
Epoch 23/40
 - 3s - loss: 0.1810 - acc: 0.9888
Epoch 24/40
 - 3s - loss: 0.1783 - acc: 0.9889
Epoch 25/40
 - 3s - loss: 0.1989 - acc: 0.9877
Epoch 26/40
 - 3s - loss: 0.1854 - acc: 0.9885
Epoch 27/40
 - 3s - loss: 0.1918 - acc: 0.9881
Epoch 28/40
 - 3s - loss: 0.1925 - acc: 0.9881
Epoch 29/40
 - 3s - loss: 0.1891 - acc: 0.9883
Epoch 30/40
 - 3s - loss: 0.1993 - acc: 0.9876
Epoch 31/40
 - 3s - loss: 0.1836 - acc: 0.9886
Epoch 32/40
 - 3s - loss: 0.1885 - acc: 0.9883
Epoch 33/40
 - 3s - loss: 0.1885 - acc: 0.9883
Epoch 34/40
 - 3s - loss: 0.1909 - acc: 0.9882
Epoch 35/40
 - 3s - loss: 0.1896 - acc: 0.9882
Epoch 36/40
 - 3s - loss: 0.1794 - acc: 0.9889
Epoch 37/40
 - 3s - loss: 0.1783 - acc: 0.9889
Epoch 38/40
 - 3s - loss: 0.1818 - acc: 0.9887
Epoch 39/40
 - 3s - loss: 0.1816 - acc: 0.9887
Epoch 40/40
 - 3s - loss: 0.1799 - acc: 0.9888
# Training time = 0:20:06.108538
# F-Score(Ordinary) = 0.076, Recall: 0.04, Precision: 0.664
# F-Score(lvc) = 0.469, Recall: 0.514, Precision: 0.432
# F-Score(ireflv) = 0.606, Recall: 0.458, Precision: 0.893
# F-Score(id) = 0.315, Recall: 0.212, Precision: 0.606
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_85 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_86 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_85 (Embedding)        (None, 4, 48)        705264      input_85[0][0]                   
__________________________________________________________________________________________________
embedding_86 (Embedding)        (None, 4, 24)        5640        input_86[0][0]                   
__________________________________________________________________________________________________
flatten_85 (Flatten)            (None, 192)          0           embedding_85[0][0]               
__________________________________________________________________________________________________
flatten_86 (Flatten)            (None, 96)           0           embedding_86[0][0]               
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 288)          0           flatten_85[0][0]                 
                                                                 flatten_86[0][0]                 
__________________________________________________________________________________________________
dense_85 (Dense)                (None, 24)           6936        concatenate_43[0][0]             
__________________________________________________________________________________________________
dropout_43 (Dropout)            (None, 24)           0           dense_85[0][0]                   
__________________________________________________________________________________________________
dense_86 (Dense)                (None, 8)            200         dropout_43[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0828 - acc: 0.9833 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.1185 - acc: 0.9841 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.1532 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.1591 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.1706 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.1760 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.1760 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.1766 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1759 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1774 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1731 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1775 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1749 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1726 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1734 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1708 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1715 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1696 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1695 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1669 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1682 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1669 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1679 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1663 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1676 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1691 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1786 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1846 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1669 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1657 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1621 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1619 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1604 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1591 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 26s - loss: 0.1583 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1596 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1584 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1575 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1587 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1580 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.1926 - acc: 0.9880
Epoch 2/40
 - 3s - loss: 0.2228 - acc: 0.9862
Epoch 3/40
 - 3s - loss: 0.2154 - acc: 0.9866
Epoch 4/40
 - 3s - loss: 0.2141 - acc: 0.9867
Epoch 5/40
 - 3s - loss: 0.2232 - acc: 0.9862
Epoch 6/40
 - 3s - loss: 0.2201 - acc: 0.9863
Epoch 7/40
 - 3s - loss: 0.2141 - acc: 0.9867
Epoch 8/40
 - 3s - loss: 0.2195 - acc: 0.9864
Epoch 9/40
 - 3s - loss: 0.2139 - acc: 0.9867
Epoch 10/40
 - 3s - loss: 0.2217 - acc: 0.9862
Epoch 11/40
 - 3s - loss: 0.2197 - acc: 0.9864
Epoch 12/40
 - 3s - loss: 0.2199 - acc: 0.9864
Epoch 13/40
 - 3s - loss: 0.2263 - acc: 0.9860
Epoch 14/40
 - 3s - loss: 0.2263 - acc: 0.9860
Epoch 15/40
 - 3s - loss: 0.2358 - acc: 0.9854
Epoch 16/40
 - 3s - loss: 0.2018 - acc: 0.9875
Epoch 17/40
 - 3s - loss: 0.2157 - acc: 0.9866
Epoch 18/40
 - 3s - loss: 0.2206 - acc: 0.9863
Epoch 19/40
 - 3s - loss: 0.2190 - acc: 0.9864
Epoch 20/40
 - 3s - loss: 0.2190 - acc: 0.9864
Epoch 21/40
 - 3s - loss: 0.2029 - acc: 0.9874
Epoch 22/40
 - 3s - loss: 0.2141 - acc: 0.9867
Epoch 23/40
 - 3s - loss: 0.2117 - acc: 0.9869
Epoch 24/40
 - 3s - loss: 0.2155 - acc: 0.9866
Epoch 25/40
 - 3s - loss: 0.2150 - acc: 0.9867
Epoch 26/40
 - 3s - loss: 0.2192 - acc: 0.9864
Epoch 27/40
 - 3s - loss: 0.2033 - acc: 0.9874
Epoch 28/40
 - 3s - loss: 0.2285 - acc: 0.9858
Epoch 29/40
 - 3s - loss: 0.2217 - acc: 0.9862
Epoch 30/40
 - 3s - loss: 0.2093 - acc: 0.9870
Epoch 31/40
 - 3s - loss: 0.2175 - acc: 0.9865
Epoch 32/40
 - 3s - loss: 0.2117 - acc: 0.9869
Epoch 33/40
 - 3s - loss: 0.2305 - acc: 0.9857
Epoch 34/40
 - 3s - loss: 0.2271 - acc: 0.9859
Epoch 35/40
 - 3s - loss: 0.2354 - acc: 0.9854
Epoch 36/40
 - 3s - loss: 0.2186 - acc: 0.9864
Epoch 37/40
 - 3s - loss: 0.2217 - acc: 0.9862
Epoch 38/40
 - 3s - loss: 0.2234 - acc: 0.9861
Epoch 39/40
 - 3s - loss: 0.2114 - acc: 0.9869
Epoch 40/40
 - 3s - loss: 0.2181 - acc: 0.9865
# Training time = 0:19:16.906096
# F-Score(Ordinary) = 0.651, Recall: 0.632, Precision: 0.671
# F-Score(lvc) = 0.534, Recall: 0.606, Precision: 0.477
# F-Score(ireflv) = 0.815, Recall: 0.755, Precision: 0.885
# F-Score(id) = 0.565, Recall: 0.522, Precision: 0.617
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_87 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_88 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_87 (Embedding)        (None, 4, 48)        705264      input_87[0][0]                   
__________________________________________________________________________________________________
embedding_88 (Embedding)        (None, 4, 24)        5640        input_88[0][0]                   
__________________________________________________________________________________________________
flatten_87 (Flatten)            (None, 192)          0           embedding_87[0][0]               
__________________________________________________________________________________________________
flatten_88 (Flatten)            (None, 96)           0           embedding_88[0][0]               
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 288)          0           flatten_87[0][0]                 
                                                                 flatten_88[0][0]                 
__________________________________________________________________________________________________
dense_87 (Dense)                (None, 24)           6936        concatenate_44[0][0]             
__________________________________________________________________________________________________
dropout_44 (Dropout)            (None, 24)           0           dense_87[0][0]                   
__________________________________________________________________________________________________
dense_88 (Dense)                (None, 8)            200         dropout_44[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0852 - acc: 0.9828 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.1309 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.1467 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.1546 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.1596 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.1597 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.1572 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.1588 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1543 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1525 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1557 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1512 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1547 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1567 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1587 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1631 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1572 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1534 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1552 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1549 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1584 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1572 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1515 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1521 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1499 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1484 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1456 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1463 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1440 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1453 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1462 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1452 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1441 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1435 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1445 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1444 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1444 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1454 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1452 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1463 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0095 - acc: 0.9993
Epoch 2/40
 - 3s - loss: 0.0030 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 0.0046 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 0.0031 - acc: 0.9998
Epoch 5/40
 - 3s - loss: 0.0012 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 0.0016 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 0.0017 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 0.0029 - acc: 0.9998
Epoch 9/40
 - 3s - loss: 0.0031 - acc: 0.9998
Epoch 10/40
 - 3s - loss: 0.0046 - acc: 0.9997
Epoch 11/40
 - 3s - loss: 0.0032 - acc: 0.9998
Epoch 12/40
 - 3s - loss: 0.0045 - acc: 0.9997
Epoch 13/40
 - 3s - loss: 0.0020 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 0.0013 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 0.0053 - acc: 0.9997
Epoch 16/40
 - 3s - loss: 0.0048 - acc: 0.9997
Epoch 17/40
 - 3s - loss: 0.0011 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 0.0020 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 0.0015 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 0.0018 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 0.0015 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 0.0020 - acc: 0.9999
Epoch 23/40
 - 3s - loss: 0.0013 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 0.0018 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 0.0013 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 0.0022 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 0.0020 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 0.0022 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 0.0024 - acc: 0.9998
Epoch 30/40
 - 3s - loss: 0.0011 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 0.0027 - acc: 0.9998
Epoch 32/40
 - 3s - loss: 0.0015 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 0.0020 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 0.0013 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 0.0022 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 0.0025 - acc: 0.9998
Epoch 37/40
 - 3s - loss: 8.8502e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 0.0038 - acc: 0.9998
Epoch 39/40
 - 3s - loss: 0.0018 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 0.0022 - acc: 0.9999
# Training time = 0:19:00.407218
# F-Score(Ordinary) = 0.198, Recall: 0.117, Precision: 0.647
# F-Score(lvc) = 0.324, Recall: 0.252, Precision: 0.455
# F-Score(ireflv) = 0.102, Recall: 0.054, Precision: 0.844
# F-Score(id) = 0.485, Recall: 0.407, Precision: 0.601
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_89 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_90 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_89 (Embedding)        (None, 4, 48)        705264      input_89[0][0]                   
__________________________________________________________________________________________________
embedding_90 (Embedding)        (None, 4, 24)        5640        input_90[0][0]                   
__________________________________________________________________________________________________
flatten_89 (Flatten)            (None, 192)          0           embedding_89[0][0]               
__________________________________________________________________________________________________
flatten_90 (Flatten)            (None, 96)           0           embedding_90[0][0]               
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 288)          0           flatten_89[0][0]                 
                                                                 flatten_90[0][0]                 
__________________________________________________________________________________________________
dense_89 (Dense)                (None, 24)           6936        concatenate_45[0][0]             
__________________________________________________________________________________________________
dropout_45 (Dropout)            (None, 24)           0           dense_89[0][0]                   
__________________________________________________________________________________________________
dense_90 (Dense)                (None, 8)            200         dropout_45[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0830 - acc: 0.9832 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.1278 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.1447 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.1473 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.1521 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.1544 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.1570 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.1549 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1583 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1597 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1610 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1598 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1651 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1674 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1687 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1698 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1693 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1665 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1724 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1686 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1729 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1691 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1667 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1675 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1702 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1693 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1655 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1654 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1643 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1597 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1776 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1806 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1801 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1776 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1751 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1785 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1768 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1802 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1930 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1900 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.8358 - acc: 0.9476
Epoch 2/40
 - 3s - loss: 0.8675 - acc: 0.9453
Epoch 3/40
 - 3s - loss: 0.8248 - acc: 0.9477
Epoch 4/40
 - 3s - loss: 0.8180 - acc: 0.9478
Epoch 5/40
 - 3s - loss: 0.8484 - acc: 0.9469
Epoch 6/40
 - 3s - loss: 0.8255 - acc: 0.9488
Epoch 7/40
 - 3s - loss: 0.8221 - acc: 0.9490
Epoch 8/40
 - 3s - loss: 0.8210 - acc: 0.9491
Epoch 9/40
 - 3s - loss: 0.8320 - acc: 0.9484
Epoch 10/40
 - 3s - loss: 0.8438 - acc: 0.9477
Epoch 11/40
 - 3s - loss: 0.8289 - acc: 0.9486
Epoch 12/40
 - 3s - loss: 0.8265 - acc: 0.9487
Epoch 13/40
 - 3s - loss: 0.8265 - acc: 0.9487
Epoch 14/40
 - 3s - loss: 0.8214 - acc: 0.9490
Epoch 15/40
 - 3s - loss: 0.8327 - acc: 0.9483
Epoch 16/40
 - 3s - loss: 0.7942 - acc: 0.9507
Epoch 17/40
 - 3s - loss: 0.8358 - acc: 0.9481
Epoch 18/40
 - 3s - loss: 0.8181 - acc: 0.9492
Epoch 19/40
 - 3s - loss: 0.8070 - acc: 0.9499
Epoch 20/40
 - 3s - loss: 0.8354 - acc: 0.9482
Epoch 21/40
 - 3s - loss: 0.8225 - acc: 0.9490
Epoch 22/40
 - 3s - loss: 0.8163 - acc: 0.9494
Epoch 23/40
 - 3s - loss: 0.8073 - acc: 0.9499
Epoch 24/40
 - 3s - loss: 0.8334 - acc: 0.9483
Epoch 25/40
 - 3s - loss: 0.8338 - acc: 0.9483
Epoch 26/40
 - 3s - loss: 0.8575 - acc: 0.9468
Epoch 27/40
 - 3s - loss: 0.8139 - acc: 0.9495
Epoch 28/40
 - 3s - loss: 0.8480 - acc: 0.9474
Epoch 29/40
 - 3s - loss: 0.8272 - acc: 0.9487
Epoch 30/40
 - 3s - loss: 0.8265 - acc: 0.9487
Epoch 31/40
 - 3s - loss: 0.8117 - acc: 0.9496
Epoch 32/40
 - 3s - loss: 0.8132 - acc: 0.9495
Epoch 33/40
 - 3s - loss: 0.8079 - acc: 0.9499
Epoch 34/40
 - 3s - loss: 0.8212 - acc: 0.9491
Epoch 35/40
 - 3s - loss: 0.8042 - acc: 0.9501
Epoch 36/40
 - 3s - loss: 0.8234 - acc: 0.9489
Epoch 37/40
 - 3s - loss: 0.8391 - acc: 0.9479
Epoch 38/40
 - 3s - loss: 0.8296 - acc: 0.9485
Epoch 39/40
 - 3s - loss: 0.8039 - acc: 0.9501
Epoch 40/40
 - 3s - loss: 0.8358 - acc: 0.9481
# Training time = 0:19:11.909255
# F-Score(Ordinary) = 0.103, Recall: 0.056, Precision: 0.676
# F-Score(lvc) = 0.404, Recall: 0.329, Precision: 0.523
# F-Score(ireflv) = 0.837, Recall: 0.814, Precision: 0.861
# F-Score(id) = 0.158, Recall: 0.091, Precision: 0.601
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_91 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_92 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_91 (Embedding)        (None, 4, 48)        705264      input_91[0][0]                   
__________________________________________________________________________________________________
embedding_92 (Embedding)        (None, 4, 24)        5640        input_92[0][0]                   
__________________________________________________________________________________________________
flatten_91 (Flatten)            (None, 192)          0           embedding_91[0][0]               
__________________________________________________________________________________________________
flatten_92 (Flatten)            (None, 96)           0           embedding_92[0][0]               
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 288)          0           flatten_91[0][0]                 
                                                                 flatten_92[0][0]                 
__________________________________________________________________________________________________
dense_91 (Dense)                (None, 24)           6936        concatenate_46[0][0]             
__________________________________________________________________________________________________
dropout_46 (Dropout)            (None, 24)           0           dense_91[0][0]                   
__________________________________________________________________________________________________
dense_92 (Dense)                (None, 8)            200         dropout_46[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 26s - loss: 0.1188 - acc: 0.9807 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.1794 - acc: 0.9830 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.2072 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.2105 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.2054 - acc: 0.9846 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.1997 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.2074 - acc: 0.9849 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.2041 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.2204 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.2244 - acc: 0.9841 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.2167 - acc: 0.9845 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.2224 - acc: 0.9846 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.2261 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.2180 - acc: 0.9829 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.2111 - acc: 0.9830 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.2157 - acc: 0.9833 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.2113 - acc: 0.9831 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 26s - loss: 0.2149 - acc: 0.9832 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 26s - loss: 0.2099 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 26s - loss: 0.2033 - acc: 0.9840 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 26s - loss: 0.1907 - acc: 0.9843 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 26s - loss: 0.1884 - acc: 0.9847 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 26s - loss: 0.1880 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 26s - loss: 0.1902 - acc: 0.9847 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 26s - loss: 0.1865 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 26s - loss: 0.1861 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 26s - loss: 0.1840 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 26s - loss: 0.1844 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 26s - loss: 0.1860 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 26s - loss: 0.1827 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 26s - loss: 0.1870 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 26s - loss: 0.1853 - acc: 0.9853 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 26s - loss: 0.1891 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 26s - loss: 0.2171 - acc: 0.9829 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 26s - loss: 0.2167 - acc: 0.9827 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 26s - loss: 0.2225 - acc: 0.9828 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 26s - loss: 0.2211 - acc: 0.9830 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 26s - loss: 0.2959 - acc: 0.9782 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 26s - loss: 0.3076 - acc: 0.9775 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 26s - loss: 0.2198 - acc: 0.9828 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.0527 - acc: 0.9333
Epoch 2/40
 - 3s - loss: 1.0406 - acc: 0.9354
Epoch 3/40
 - 3s - loss: 1.0309 - acc: 0.9360
Epoch 4/40
 - 3s - loss: 1.0294 - acc: 0.9361
Epoch 5/40
 - 3s - loss: 1.0347 - acc: 0.9358
Epoch 6/40
 - 3s - loss: 1.0212 - acc: 0.9366
Epoch 7/40
 - 3s - loss: 1.0280 - acc: 0.9362
Epoch 8/40
 - 3s - loss: 1.0369 - acc: 0.9357
Epoch 9/40
 - 3s - loss: 1.0267 - acc: 0.9363
Epoch 10/40
 - 3s - loss: 1.0367 - acc: 0.9357
Epoch 11/40
 - 3s - loss: 1.0528 - acc: 0.9347
Epoch 12/40
 - 3s - loss: 1.0373 - acc: 0.9356
Epoch 13/40
 - 3s - loss: 1.0373 - acc: 0.9356
Epoch 14/40
 - 3s - loss: 1.0265 - acc: 0.9363
Epoch 15/40
 - 3s - loss: 1.0333 - acc: 0.9359
Epoch 16/40
 - 3s - loss: 1.0309 - acc: 0.9360
Epoch 17/40
 - 3s - loss: 1.0415 - acc: 0.9354
Epoch 18/40
 - 3s - loss: 1.0303 - acc: 0.9361
Epoch 19/40
 - 3s - loss: 1.0444 - acc: 0.9352
Epoch 20/40
 - 3s - loss: 1.0391 - acc: 0.9355
Epoch 21/40
 - 3s - loss: 1.0205 - acc: 0.9367
Epoch 22/40
 - 3s - loss: 1.0561 - acc: 0.9345
Epoch 23/40
 - 3s - loss: 1.0409 - acc: 0.9354
Epoch 24/40
 - 3s - loss: 1.0081 - acc: 0.9375
Epoch 25/40
 - 3s - loss: 1.0194 - acc: 0.9368
Epoch 26/40
 - 3s - loss: 1.0296 - acc: 0.9361
Epoch 27/40
 - 3s - loss: 1.0395 - acc: 0.9355
Epoch 28/40
 - 3s - loss: 1.0376 - acc: 0.9356
Epoch 29/40
 - 3s - loss: 1.0457 - acc: 0.9351
Epoch 30/40
 - 3s - loss: 1.0493 - acc: 0.9349
Epoch 31/40
 - 3s - loss: 1.0420 - acc: 0.9354
Epoch 32/40
 - 3s - loss: 1.0238 - acc: 0.9365
Epoch 33/40
 - 3s - loss: 1.0258 - acc: 0.9364
Epoch 34/40
 - 3s - loss: 1.0320 - acc: 0.9360
Epoch 35/40
 - 3s - loss: 1.0303 - acc: 0.9361
Epoch 36/40
 - 3s - loss: 0.9957 - acc: 0.9382
Epoch 37/40
 - 3s - loss: 1.0298 - acc: 0.9361
Epoch 38/40
 - 3s - loss: 1.0236 - acc: 0.9365
Epoch 39/40
 - 3s - loss: 1.0557 - acc: 0.9345
Epoch 40/40
 - 3s - loss: 1.0663 - acc: 0.9338
# Training time = 0:20:04.341795
# F-Score(Ordinary) = 0.221, Recall: 0.131, Precision: 0.707
# F-Score(lvc) = 0.387, Recall: 0.302, Precision: 0.538
# F-Score(ireflv) = 0.663, Recall: 0.511, Precision: 0.943
# F-Score(id) = 0.374, Recall: 0.274, Precision: 0.591
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_93 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_94 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_93 (Embedding)        (None, 4, 48)        705264      input_93[0][0]                   
__________________________________________________________________________________________________
embedding_94 (Embedding)        (None, 4, 24)        5640        input_94[0][0]                   
__________________________________________________________________________________________________
flatten_93 (Flatten)            (None, 192)          0           embedding_93[0][0]               
__________________________________________________________________________________________________
flatten_94 (Flatten)            (None, 96)           0           embedding_94[0][0]               
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 288)          0           flatten_93[0][0]                 
                                                                 flatten_94[0][0]                 
__________________________________________________________________________________________________
dense_93 (Dense)                (None, 24)           6936        concatenate_47[0][0]             
__________________________________________________________________________________________________
dropout_47 (Dropout)            (None, 24)           0           dense_93[0][0]                   
__________________________________________________________________________________________________
dense_94 (Dense)                (None, 8)            200         dropout_47[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.1273 - acc: 0.9824 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.1655 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.1710 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.1718 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.1682 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.1685 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.1718 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.1712 - acc: 0.9880 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1700 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1761 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1674 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1669 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1650 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1618 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1633 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1607 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1634 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1730 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1646 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1752 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1607 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1695 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1855 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1840 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1835 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1805 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1835 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 26s - loss: 0.1789 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1779 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1777 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1789 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1902 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.2036 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 26s - loss: 0.2063 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1907 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1898 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1862 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1854 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1936 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1976 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.5373 - acc: 0.9644
Epoch 2/40
 - 3s - loss: 0.5088 - acc: 0.9684
Epoch 3/40
 - 3s - loss: 0.5048 - acc: 0.9687
Epoch 4/40
 - 3s - loss: 0.5349 - acc: 0.9668
Epoch 5/40
 - 3s - loss: 0.5121 - acc: 0.9682
Epoch 6/40
 - 3s - loss: 0.5285 - acc: 0.9672
Epoch 7/40
 - 3s - loss: 0.4898 - acc: 0.9696
Epoch 8/40
 - 3s - loss: 0.5132 - acc: 0.9682
Epoch 9/40
 - 3s - loss: 0.4938 - acc: 0.9694
Epoch 10/40
 - 3s - loss: 0.5108 - acc: 0.9683
Epoch 11/40
 - 3s - loss: 0.5190 - acc: 0.9678
Epoch 12/40
 - 3s - loss: 0.5183 - acc: 0.9678
Epoch 13/40
 - 3s - loss: 0.4929 - acc: 0.9694
Epoch 14/40
 - 3s - loss: 0.5303 - acc: 0.9671
Epoch 15/40
 - 3s - loss: 0.5144 - acc: 0.9681
Epoch 16/40
 - 3s - loss: 0.5093 - acc: 0.9684
Epoch 17/40
 - 3s - loss: 0.5221 - acc: 0.9676
Epoch 18/40
 - 3s - loss: 0.5232 - acc: 0.9675
Epoch 19/40
 - 3s - loss: 0.5075 - acc: 0.9685
Epoch 20/40
 - 3s - loss: 0.5402 - acc: 0.9665
Epoch 21/40
 - 3s - loss: 0.5002 - acc: 0.9690
Epoch 22/40
 - 3s - loss: 0.5250 - acc: 0.9674
Epoch 23/40
 - 3s - loss: 0.5075 - acc: 0.9685
Epoch 24/40
 - 3s - loss: 0.5281 - acc: 0.9672
Epoch 25/40
 - 3s - loss: 0.5108 - acc: 0.9683
Epoch 26/40
 - 3s - loss: 0.5320 - acc: 0.9670
Epoch 27/40
 - 3s - loss: 0.5360 - acc: 0.9667
Epoch 28/40
 - 3s - loss: 0.5093 - acc: 0.9684
Epoch 29/40
 - 3s - loss: 0.5194 - acc: 0.9678
Epoch 30/40
 - 3s - loss: 0.5194 - acc: 0.9678
Epoch 31/40
 - 3s - loss: 0.5128 - acc: 0.9682
Epoch 32/40
 - 3s - loss: 0.5290 - acc: 0.9672
Epoch 33/40
 - 3s - loss: 0.5152 - acc: 0.9680
Epoch 34/40
 - 3s - loss: 0.5168 - acc: 0.9679
Epoch 35/40
 - 3s - loss: 0.5117 - acc: 0.9683
Epoch 36/40
 - 3s - loss: 0.5075 - acc: 0.9685
Epoch 37/40
 - 3s - loss: 0.5139 - acc: 0.9681
Epoch 38/40
 - 3s - loss: 0.5325 - acc: 0.9670
Epoch 39/40
 - 3s - loss: 0.5243 - acc: 0.9675
Epoch 40/40
 - 3s - loss: 0.5174 - acc: 0.9679
# Training time = 0:19:22.845946
# F-Score(Ordinary) = 0.589, Recall: 0.529, Precision: 0.664
# F-Score(lvc) = 0.452, Recall: 0.399, Precision: 0.523
# F-Score(ireflv) = 0.792, Recall: 0.773, Precision: 0.811
# F-Score(id) = 0.539, Recall: 0.491, Precision: 0.596
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_95 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_96 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_95 (Embedding)        (None, 4, 48)        705264      input_95[0][0]                   
__________________________________________________________________________________________________
embedding_96 (Embedding)        (None, 4, 24)        5640        input_96[0][0]                   
__________________________________________________________________________________________________
flatten_95 (Flatten)            (None, 192)          0           embedding_95[0][0]               
__________________________________________________________________________________________________
flatten_96 (Flatten)            (None, 96)           0           embedding_96[0][0]               
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 288)          0           flatten_95[0][0]                 
                                                                 flatten_96[0][0]                 
__________________________________________________________________________________________________
dense_95 (Dense)                (None, 24)           6936        concatenate_48[0][0]             
__________________________________________________________________________________________________
dropout_48 (Dropout)            (None, 24)           0           dense_95[0][0]                   
__________________________________________________________________________________________________
dense_96 (Dense)                (None, 8)            200         dropout_48[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.1188 - acc: 0.9806 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.1767 - acc: 0.9824 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.1978 - acc: 0.9827 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.1992 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.2162 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.2143 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.2071 - acc: 0.9842 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.1939 - acc: 0.9843 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1989 - acc: 0.9839 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.2219 - acc: 0.9827 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.2173 - acc: 0.9829 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 26s - loss: 0.2159 - acc: 0.9827 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 26s - loss: 0.2148 - acc: 0.9828 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.2115 - acc: 0.9832 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.2151 - acc: 0.9829 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.2154 - acc: 0.9828 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.2126 - acc: 0.9832 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.2201 - acc: 0.9830 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.2168 - acc: 0.9833 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.2176 - acc: 0.9832 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.2102 - acc: 0.9837 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.2126 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.2071 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 26s - loss: 0.2109 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 26s - loss: 0.2112 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 26s - loss: 0.2107 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 26s - loss: 0.2086 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 26s - loss: 0.2095 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 26s - loss: 0.2095 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 26s - loss: 0.2075 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.2057 - acc: 0.9839 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.2116 - acc: 0.9839 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.2071 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.2040 - acc: 0.9841 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.2062 - acc: 0.9840 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.2078 - acc: 0.9839 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 26s - loss: 0.2096 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.2071 - acc: 0.9838 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.2072 - acc: 0.9839 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.2041 - acc: 0.9843 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.7316 - acc: 0.9536
Epoch 2/40
 - 3s - loss: 0.7161 - acc: 0.9546
Epoch 3/40
 - 3s - loss: 0.7246 - acc: 0.9550
Epoch 4/40
 - 3s - loss: 0.7170 - acc: 0.9555
Epoch 5/40
 - 3s - loss: 0.7278 - acc: 0.9548
Epoch 6/40
 - 3s - loss: 0.7210 - acc: 0.9553
Epoch 7/40
 - 3s - loss: 0.7267 - acc: 0.9549
Epoch 8/40
 - 3s - loss: 0.7099 - acc: 0.9560
Epoch 9/40
 - 3s - loss: 0.7199 - acc: 0.9553
Epoch 10/40
 - 3s - loss: 0.7232 - acc: 0.9551
Epoch 11/40
 - 3s - loss: 0.7385 - acc: 0.9542
Epoch 12/40
 - 3s - loss: 0.7351 - acc: 0.9544
Epoch 13/40
 - 3s - loss: 0.7283 - acc: 0.9548
Epoch 14/40
 - 3s - loss: 0.7245 - acc: 0.9550
Epoch 15/40
 - 3s - loss: 0.7035 - acc: 0.9564
Epoch 16/40
 - 3s - loss: 0.7393 - acc: 0.9541
Epoch 17/40
 - 3s - loss: 0.7177 - acc: 0.9555
Epoch 18/40
 - 3s - loss: 0.7024 - acc: 0.9564
Epoch 19/40
 - 3s - loss: 0.7247 - acc: 0.9550
Epoch 20/40
 - 3s - loss: 0.7232 - acc: 0.9551
Epoch 21/40
 - 3s - loss: 0.7177 - acc: 0.9555
Epoch 22/40
 - 3s - loss: 0.7342 - acc: 0.9544
Epoch 23/40
 - 3s - loss: 0.7126 - acc: 0.9558
Epoch 24/40
 - 3s - loss: 0.7376 - acc: 0.9542
Epoch 25/40
 - 3s - loss: 0.7575 - acc: 0.9530
Epoch 26/40
 - 3s - loss: 0.7104 - acc: 0.9559
Epoch 27/40
 - 3s - loss: 0.7139 - acc: 0.9557
Epoch 28/40
 - 3s - loss: 0.7294 - acc: 0.9547
Epoch 29/40
 - 3s - loss: 0.7208 - acc: 0.9553
Epoch 30/40
 - 3s - loss: 0.7121 - acc: 0.9558
Epoch 31/40
 - 3s - loss: 0.7199 - acc: 0.9553
Epoch 32/40
 - 3s - loss: 0.7026 - acc: 0.9564
Epoch 33/40
 - 3s - loss: 0.7473 - acc: 0.9536
Epoch 34/40
 - 3s - loss: 0.7128 - acc: 0.9558
Epoch 35/40
 - 3s - loss: 0.7427 - acc: 0.9539
Epoch 36/40
 - 3s - loss: 0.7117 - acc: 0.9558
Epoch 37/40
 - 3s - loss: 0.7212 - acc: 0.9553
Epoch 38/40
 - 3s - loss: 0.7294 - acc: 0.9547
Epoch 39/40
 - 3s - loss: 0.7159 - acc: 0.9556
Epoch 40/40
 - 3s - loss: 0.7203 - acc: 0.9553
# Training time = 0:19:20.757490
# F-Score(Ordinary) = 0.104, Recall: 0.056, Precision: 0.642
# F-Score(lvc) = 0.391, Recall: 0.328, Precision: 0.485
# F-Score(ireflv) = 0.344, Recall: 0.215, Precision: 0.861
# F-Score(id) = 0.287, Recall: 0.195, Precision: 0.544
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_97 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_98 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_97 (Embedding)        (None, 4, 48)        705264      input_97[0][0]                   
__________________________________________________________________________________________________
embedding_98 (Embedding)        (None, 4, 24)        5640        input_98[0][0]                   
__________________________________________________________________________________________________
flatten_97 (Flatten)            (None, 192)          0           embedding_97[0][0]               
__________________________________________________________________________________________________
flatten_98 (Flatten)            (None, 96)           0           embedding_98[0][0]               
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 288)          0           flatten_97[0][0]                 
                                                                 flatten_98[0][0]                 
__________________________________________________________________________________________________
dense_97 (Dense)                (None, 24)           6936        concatenate_49[0][0]             
__________________________________________________________________________________________________
dropout_49 (Dropout)            (None, 24)           0           dense_97[0][0]                   
__________________________________________________________________________________________________
dense_98 (Dense)                (None, 8)            200         dropout_49[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.1268 - acc: 0.9823 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.1600 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.1652 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.1668 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.1704 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.1706 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.1735 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.1721 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1750 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1696 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1713 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1726 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1710 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1691 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1691 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1663 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1683 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1679 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1697 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 26s - loss: 0.1689 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1662 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1630 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1651 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 26s - loss: 0.1644 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1658 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1626 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1653 - acc: 0.9874 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1651 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1601 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1647 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1661 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1634 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1630 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.1609 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1623 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1659 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.1677 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.1672 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.1625 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.1616 - acc: 0.9877 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0302 - acc: 0.9981
Epoch 2/40
 - 3s - loss: 0.0280 - acc: 0.9983
Epoch 3/40
 - 3s - loss: 0.0324 - acc: 0.9980
Epoch 4/40
 - 3s - loss: 0.0247 - acc: 0.9985
Epoch 5/40
 - 3s - loss: 0.0314 - acc: 0.9981
Epoch 6/40
 - 3s - loss: 0.0279 - acc: 0.9983
Epoch 7/40
 - 3s - loss: 0.0289 - acc: 0.9982
Epoch 8/40
 - 3s - loss: 0.0301 - acc: 0.9981
Epoch 9/40
 - 3s - loss: 0.0309 - acc: 0.9981
Epoch 10/40
 - 3s - loss: 0.0308 - acc: 0.9981
Epoch 11/40
 - 3s - loss: 0.0285 - acc: 0.9982
Epoch 12/40
 - 3s - loss: 0.0284 - acc: 0.9982
Epoch 13/40
 - 3s - loss: 0.0285 - acc: 0.9982
Epoch 14/40
 - 3s - loss: 0.0323 - acc: 0.9980
Epoch 15/40
 - 3s - loss: 0.0338 - acc: 0.9979
Epoch 16/40
 - 3s - loss: 0.0269 - acc: 0.9983
Epoch 17/40
 - 3s - loss: 0.0300 - acc: 0.9981
Epoch 18/40
 - 3s - loss: 0.0254 - acc: 0.9984
Epoch 19/40
 - 3s - loss: 0.0296 - acc: 0.9982
Epoch 20/40
 - 3s - loss: 0.0265 - acc: 0.9984
Epoch 21/40
 - 3s - loss: 0.0261 - acc: 0.9984
Epoch 22/40
 - 3s - loss: 0.0282 - acc: 0.9982
Epoch 23/40
 - 3s - loss: 0.0272 - acc: 0.9983
Epoch 24/40
 - 3s - loss: 0.0294 - acc: 0.9982
Epoch 25/40
 - 3s - loss: 0.0299 - acc: 0.9981
Epoch 26/40
 - 3s - loss: 0.0337 - acc: 0.9979
Epoch 27/40
 - 3s - loss: 0.0310 - acc: 0.9981
Epoch 28/40
 - 3s - loss: 0.0281 - acc: 0.9983
Epoch 29/40
 - 3s - loss: 0.0313 - acc: 0.9981
Epoch 30/40
 - 3s - loss: 0.0320 - acc: 0.9980
Epoch 31/40
 - 3s - loss: 0.0285 - acc: 0.9982
Epoch 32/40
 - 3s - loss: 0.0311 - acc: 0.9981
Epoch 33/40
 - 3s - loss: 0.0302 - acc: 0.9981
Epoch 34/40
 - 3s - loss: 0.0313 - acc: 0.9981
Epoch 35/40
 - 3s - loss: 0.0320 - acc: 0.9980
Epoch 36/40
 - 3s - loss: 0.0310 - acc: 0.9981
Epoch 37/40
 - 3s - loss: 0.0293 - acc: 0.9982
Epoch 38/40
 - 3s - loss: 0.0285 - acc: 0.9982
Epoch 39/40
 - 3s - loss: 0.0256 - acc: 0.9984
Epoch 40/40
 - 3s - loss: 0.0281 - acc: 0.9983
# Training time = 0:19:18.574787
# F-Score(Ordinary) = 0.141, Recall: 0.078, Precision: 0.694
# F-Score(lvc) = 0.042, Recall: 0.022, Precision: 0.545
# F-Score(ireflv) = 0.663, Recall: 0.532, Precision: 0.877
# F-Score(id) = 0.369, Recall: 0.262, Precision: 0.622
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_99 (InputLayer)           (None, 4)            0                                            
__________________________________________________________________________________________________
input_100 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_99 (Embedding)        (None, 4, 48)        705264      input_99[0][0]                   
__________________________________________________________________________________________________
embedding_100 (Embedding)       (None, 4, 24)        5640        input_100[0][0]                  
__________________________________________________________________________________________________
flatten_99 (Flatten)            (None, 192)          0           embedding_99[0][0]               
__________________________________________________________________________________________________
flatten_100 (Flatten)           (None, 96)           0           embedding_100[0][0]              
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 288)          0           flatten_99[0][0]                 
                                                                 flatten_100[0][0]                
__________________________________________________________________________________________________
dense_99 (Dense)                (None, 24)           6936        concatenate_50[0][0]             
__________________________________________________________________________________________________
dropout_50 (Dropout)            (None, 24)           0           dense_99[0][0]                   
__________________________________________________________________________________________________
dense_100 (Dense)               (None, 8)            200         dropout_50[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = rmsprop, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.1253 - acc: 0.9819 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.1711 - acc: 0.9855 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.1875 - acc: 0.9856 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.1808 - acc: 0.9849 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.1788 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.1785 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.1760 - acc: 0.9849 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.1744 - acc: 0.9847 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.1734 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.1702 - acc: 0.9849 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.1717 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.1692 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.1699 - acc: 0.9855 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.1704 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.1734 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.1694 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.1673 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.1680 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.1676 - acc: 0.9871 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.1691 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.1658 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.1681 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.1657 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.1621 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.1621 - acc: 0.9868 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.1663 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.1740 - acc: 0.9857 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.1778 - acc: 0.9857 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.1730 - acc: 0.9858 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.1715 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.1723 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.1685 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.1715 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 26s - loss: 0.1718 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.1704 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.1698 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 26s - loss: 0.1684 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.1711 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.1722 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.1697 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0021 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:19:31.054065
# F-Score(Ordinary) = 0.173, Recall: 0.099, Precision: 0.694
# F-Score(lvc) = 0.532, Recall: 0.614, Precision: 0.47
# F-Score(ireflv) = 0.708, Recall: 0.59, Precision: 0.885
# F-Score(id) = 0.183, Recall: 0.107, Precision: 0.627
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_101 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_102 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_101 (Embedding)       (None, 4, 48)        705264      input_101[0][0]                  
__________________________________________________________________________________________________
embedding_102 (Embedding)       (None, 4, 24)        5640        input_102[0][0]                  
__________________________________________________________________________________________________
flatten_101 (Flatten)           (None, 192)          0           embedding_101[0][0]              
__________________________________________________________________________________________________
flatten_102 (Flatten)           (None, 96)           0           embedding_102[0][0]              
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 288)          0           flatten_101[0][0]                
                                                                 flatten_102[0][0]                
__________________________________________________________________________________________________
dense_101 (Dense)               (None, 24)           6936        concatenate_51[0][0]             
__________________________________________________________________________________________________
dropout_51 (Dropout)            (None, 24)           0           dense_101[0][0]                  
__________________________________________________________________________________________________
dense_102 (Dense)               (None, 8)            200         dropout_51[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.0005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 28s - loss: 0.1022 - acc: 0.9769 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 28s - loss: 0.0462 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0432 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0417 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0371 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.4352e-04 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.6200e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 4.2393e-06 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.5465e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 5.1445e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.5052e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 9.9068e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 2.6485e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 9.7888e-05 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 4.8537e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.4467e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.3807e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 4.8203e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 4.7792e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 4.7428e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.2129e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 4.7057e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1981e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 4.6658e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 4.6276e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1937e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 9.1304e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 9.0015e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 4.4608e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1926e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 4.4273e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.3034e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.2766e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 4.2098e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 4.1811e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 8.2382e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 4.0794e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 4.0467e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:21:05.886752
# F-Score(Ordinary) = 0.664, Recall: 0.66, Precision: 0.669
# F-Score(lvc) = 0.552, Recall: 0.685, Precision: 0.462
# F-Score(ireflv) = 0.816, Recall: 0.813, Precision: 0.82
# F-Score(id) = 0.663, Recall: 0.679, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_103 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_104 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_103 (Embedding)       (None, 4, 48)        705264      input_103[0][0]                  
__________________________________________________________________________________________________
embedding_104 (Embedding)       (None, 4, 24)        5640        input_104[0][0]                  
__________________________________________________________________________________________________
flatten_103 (Flatten)           (None, 192)          0           embedding_103[0][0]              
__________________________________________________________________________________________________
flatten_104 (Flatten)           (None, 96)           0           embedding_104[0][0]              
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 288)          0           flatten_103[0][0]                
                                                                 flatten_104[0][0]                
__________________________________________________________________________________________________
dense_103 (Dense)               (None, 24)           6936        concatenate_52[0][0]             
__________________________________________________________________________________________________
dropout_52 (Dropout)            (None, 24)           0           dense_103[0][0]                  
__________________________________________________________________________________________________
dense_104 (Dense)               (None, 8)            200         dropout_52[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.0005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.1039 - acc: 0.9773 - val_loss: 7.7486e-07 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0463 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0432 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0418 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0409 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0381 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0368 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0368 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 2.6003e-04 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 8.5222e-05 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 8.3292e-06 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 5.5389e-06 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 6.2436e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 6.2762e-05 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 6.0258e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.1847e-04 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1696e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 2.9449e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 5.8012e-05 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 5.7626e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 5.7218e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.3276e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 5.6876e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 5.6464e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1175e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1050e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.0944e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.0824e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.0709e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1952e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.0590e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 5.2532e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 5.2189e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.0332e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1923e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1923e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.0194e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 5.0497e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 5.0205e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 4.9843e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 4.9522e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 4.9101e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 4.8694e-05 - acc: 1.0000
# Training time = 0:21:16.302923
# F-Score(Ordinary) = 0.669, Recall: 0.69, Precision: 0.649
# F-Score(lvc) = 0.458, Recall: 0.412, Precision: 0.515
# F-Score(ireflv) = 0.819, Recall: 0.864, Precision: 0.779
# F-Score(id) = 0.698, Recall: 0.814, Precision: 0.611
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_105 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_106 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_105 (Embedding)       (None, 4, 48)        705264      input_105[0][0]                  
__________________________________________________________________________________________________
embedding_106 (Embedding)       (None, 4, 24)        5640        input_106[0][0]                  
__________________________________________________________________________________________________
flatten_105 (Flatten)           (None, 192)          0           embedding_105[0][0]              
__________________________________________________________________________________________________
flatten_106 (Flatten)           (None, 96)           0           embedding_106[0][0]              
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 288)          0           flatten_105[0][0]                
                                                                 flatten_106[0][0]                
__________________________________________________________________________________________________
dense_105 (Dense)               (None, 24)           6936        concatenate_53[0][0]             
__________________________________________________________________________________________________
dropout_53 (Dropout)            (None, 24)           0           dense_105[0][0]                  
__________________________________________________________________________________________________
dense_106 (Dense)               (None, 8)            200         dropout_53[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.0005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.1050 - acc: 0.9769 - val_loss: 7.7486e-06 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0465 - acc: 0.9905 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0433 - acc: 0.9912 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0418 - acc: 0.9915 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0409 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0386 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0381 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0374 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.6064e-04 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 6.4728e-05 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 3.9158e-06 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 4.7516e-05 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.9277e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 2.8528e-05 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 2.9019e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 2.8610e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.9966e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 4.3111e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 3.1219e-06 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.6726e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.5808e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.2648e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 5.4060e-06 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.2268e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 7.6430e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1993e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.5450e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.2692e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 7.8553e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1940e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 2.4427e-06 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 5.7457e-06 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.8201e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 3.6972e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.2458e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1948e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.7898e-06 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.8173e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.2247e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1924e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 7.6984e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.9454e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.3069e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.2051e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1923e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.2508e-07 - acc: 1.0000
# Training time = 0:21:15.226872
# F-Score(Ordinary) = 0.543, Recall: 0.782, Precision: 0.416
# F-Score(lvc) = 0.529, Recall: 0.877, Precision: 0.379
# F-Score(ireflv) = 0.714, Recall: 0.66, Precision: 0.779
# F-Score(id) = 0.304, Recall: 0.946, Precision: 0.181
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_107 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_108 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_107 (Embedding)       (None, 4, 48)        705264      input_107[0][0]                  
__________________________________________________________________________________________________
embedding_108 (Embedding)       (None, 4, 24)        5640        input_108[0][0]                  
__________________________________________________________________________________________________
flatten_107 (Flatten)           (None, 192)          0           embedding_107[0][0]              
__________________________________________________________________________________________________
flatten_108 (Flatten)           (None, 96)           0           embedding_108[0][0]              
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 288)          0           flatten_107[0][0]                
                                                                 flatten_108[0][0]                
__________________________________________________________________________________________________
dense_107 (Dense)               (None, 24)           6936        concatenate_54[0][0]             
__________________________________________________________________________________________________
dropout_54 (Dropout)            (None, 24)           0           dense_107[0][0]                  
__________________________________________________________________________________________________
dense_108 (Dense)               (None, 8)            200         dropout_54[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.0005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.1056 - acc: 0.9767 - val_loss: 3.6955e-06 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0462 - acc: 0.9904 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0431 - acc: 0.9911 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0419 - acc: 0.9914 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 5/40
 - 26s - loss: 0.0410 - acc: 0.9916 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0402 - acc: 0.9918 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0381 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.2449e-04 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 7.8189e-05 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 5.4891e-05 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 2.6073e-05 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 4.4271e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 7.0415e-06 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 3.0498e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 2.1992e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.6346e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 2.4799e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 5.8841e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.3384e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 6.1904e-06 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 2.1348e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.5658e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1999e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.2585e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 2.3167e-06 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.3123e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.3127e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.8855e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1971e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1936e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1937e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1954e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1974e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.3594e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.2005e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1959e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.2011e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1933e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1962e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1926e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 8.1977e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1929e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.2008e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 7.1923e-06 - acc: 1.0000
# Training time = 0:20:15.471658
# F-Score(Ordinary) = 0.707, Recall: 0.891, Precision: 0.586
# F-Score(lvc) = 0.549, Recall: 0.869, Precision: 0.402
# F-Score(ireflv) = 0.82, Recall: 0.91, Precision: 0.746
# F-Score(id) = 0.699, Recall: 0.857, Precision: 0.591
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_109 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_110 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_109 (Embedding)       (None, 4, 48)        705264      input_109[0][0]                  
__________________________________________________________________________________________________
embedding_110 (Embedding)       (None, 4, 24)        5640        input_110[0][0]                  
__________________________________________________________________________________________________
flatten_109 (Flatten)           (None, 192)          0           embedding_109[0][0]              
__________________________________________________________________________________________________
flatten_110 (Flatten)           (None, 96)           0           embedding_110[0][0]              
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 288)          0           flatten_109[0][0]                
                                                                 flatten_110[0][0]                
__________________________________________________________________________________________________
dense_109 (Dense)               (None, 24)           6936        concatenate_55[0][0]             
__________________________________________________________________________________________________
dropout_55 (Dropout)            (None, 24)           0           dense_109[0][0]                  
__________________________________________________________________________________________________
dense_110 (Dense)               (None, 8)            200         dropout_55[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.0005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.1057 - acc: 0.9767 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0468 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0436 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 26s - loss: 0.0421 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 26s - loss: 0.0410 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 26s - loss: 0.0406 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 26s - loss: 0.0401 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 26s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 26s - loss: 0.0393 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 26s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 26s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 26s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 26s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 26s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 26s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 26s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0377 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 26s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 26s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 26s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 26s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 26s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 26s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 26s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 4.7129e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 4.0266e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 2.8419e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 3.3656e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 2.2153e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 2.1890e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 2.1572e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 2.6499e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 3.6314e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 2.0377e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 2.0070e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 2.4629e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 1.4550e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 2.3826e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 2.7939e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 4.0736e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 1.3305e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 8.7784e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.2984e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 2.1176e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 1.2460e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 8.2009e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.6108e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 3.9794e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.5630e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 3.0245e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 1.4723e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 1.4436e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 1.7619e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 2.7277e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 9.9712e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.6261e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.5804e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 6.2027e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 9.1513e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 2.0620e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 1.7013e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 1.1015e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 1.3404e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 1.8080e-04 - acc: 0.9999
# Training time = 0:20:23.418158
# F-Score(Ordinary) = 0.748, Recall: 0.874, Precision: 0.653
# F-Score(lvc) = 0.6, Recall: 0.882, Precision: 0.455
# F-Score(ireflv) = 0.833, Recall: 0.874, Precision: 0.795
# F-Score(id) = 0.747, Recall: 0.839, Precision: 0.674
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_111 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_112 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_111 (Embedding)       (None, 4, 48)        705264      input_111[0][0]                  
__________________________________________________________________________________________________
embedding_112 (Embedding)       (None, 4, 24)        5640        input_112[0][0]                  
__________________________________________________________________________________________________
flatten_111 (Flatten)           (None, 192)          0           embedding_111[0][0]              
__________________________________________________________________________________________________
flatten_112 (Flatten)           (None, 96)           0           embedding_112[0][0]              
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 288)          0           flatten_111[0][0]                
                                                                 flatten_112[0][0]                
__________________________________________________________________________________________________
dense_111 (Dense)               (None, 24)           6936        concatenate_56[0][0]             
__________________________________________________________________________________________________
dropout_56 (Dropout)            (None, 24)           0           dense_111[0][0]                  
__________________________________________________________________________________________________
dense_112 (Dense)               (None, 8)            200         dropout_56[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 28s - loss: 0.0793 - acc: 0.9821 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0448 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0423 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0409 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0403 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 28s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 28s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 28s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 28s - loss: 0.0386 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 28s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 28s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 28s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 28s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 28s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 28s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 28s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 28s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 28s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 28s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 28s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 28s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 28s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 28s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 28s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 28s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 28s - loss: 0.0376 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0378 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 28s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0382 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 2.3900e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 2.3427e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.6948e-06 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 2.2841e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 7.5502e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 2.2247e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.4601e-04 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.6727e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.4387e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 7.1022e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.2699e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.2501e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 7.0443e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 6.9607e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 6.8865e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1969e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 6.8105e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1933e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 6.7289e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 6.6504e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1924e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.3057e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.2791e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 6.3071e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 6.2381e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.8229e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.7674e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 5.7890e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 5.7295e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1215e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 5.5191e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 5.4513e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:20:56.062965
# F-Score(Ordinary) = 0.652, Recall: 0.625, Precision: 0.682
# F-Score(lvc) = 0.552, Recall: 0.685, Precision: 0.462
# F-Score(ireflv) = 0.815, Recall: 0.818, Precision: 0.811
# F-Score(id) = 0.574, Recall: 0.489, Precision: 0.694
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_113 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_114 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_113 (Embedding)       (None, 4, 48)        705264      input_113[0][0]                  
__________________________________________________________________________________________________
embedding_114 (Embedding)       (None, 4, 24)        5640        input_114[0][0]                  
__________________________________________________________________________________________________
flatten_113 (Flatten)           (None, 192)          0           embedding_113[0][0]              
__________________________________________________________________________________________________
flatten_114 (Flatten)           (None, 96)           0           embedding_114[0][0]              
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 288)          0           flatten_113[0][0]                
                                                                 flatten_114[0][0]                
__________________________________________________________________________________________________
dense_113 (Dense)               (None, 24)           6936        concatenate_57[0][0]             
__________________________________________________________________________________________________
dropout_57 (Dropout)            (None, 24)           0           dense_113[0][0]                  
__________________________________________________________________________________________________
dense_114 (Dense)               (None, 8)            200         dropout_57[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0806 - acc: 0.9824 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0450 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0423 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0411 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0404 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0388 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0388 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0382 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0381 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 8.7009e-05 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.0415e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 3.3394e-06 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 2.3530e-05 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 4.2960e-06 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 8.3173e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 3.7148e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 6.1193e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 3.0984e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.5660e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.6200e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.8999e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.2295e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.2232e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.2499e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.2589e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.2077e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.2020e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1993e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1941e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1933e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1940e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.8570e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1924e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 9.1844e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 9.1081e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 9.0354e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 8.9486e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 8.8684e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 8.7816e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:20:46.850579
# F-Score(Ordinary) = 0.694, Recall: 0.769, Precision: 0.633
# F-Score(lvc) = 0.564, Recall: 0.814, Precision: 0.432
# F-Score(ireflv) = 0.786, Recall: 0.748, Precision: 0.828
# F-Score(id) = 0.685, Recall: 0.748, Precision: 0.632
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_115 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_116 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_115 (Embedding)       (None, 4, 48)        705264      input_115[0][0]                  
__________________________________________________________________________________________________
embedding_116 (Embedding)       (None, 4, 24)        5640        input_116[0][0]                  
__________________________________________________________________________________________________
flatten_115 (Flatten)           (None, 192)          0           embedding_115[0][0]              
__________________________________________________________________________________________________
flatten_116 (Flatten)           (None, 96)           0           embedding_116[0][0]              
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 288)          0           flatten_115[0][0]                
                                                                 flatten_116[0][0]                
__________________________________________________________________________________________________
dense_115 (Dense)               (None, 24)           6936        concatenate_58[0][0]             
__________________________________________________________________________________________________
dropout_58 (Dropout)            (None, 24)           0           dense_115[0][0]                  
__________________________________________________________________________________________________
dense_116 (Dense)               (None, 8)            200         dropout_58[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0811 - acc: 0.9820 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0451 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0423 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0374 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 7.0072e-05 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.3094e-06 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 4.7401e-05 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 7.1986e-06 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 6.5958e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.0184e-06 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 2.8104e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.5750e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.4704e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.4787e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 2.7441e-07 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.2309e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.2291e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.2599e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.2342e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.2241e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.6730e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.1944e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1945e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1950e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1944e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1930e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.2154e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.2067e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1924e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1924e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 2.3863e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1830e-04 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.3389e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1590e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:20:59.476356
# F-Score(Ordinary) = 0.718, Recall: 0.88, Precision: 0.606
# F-Score(lvc) = 0.585, Recall: 0.822, Precision: 0.455
# F-Score(ireflv) = 0.802, Recall: 0.845, Precision: 0.762
# F-Score(id) = 0.736, Recall: 0.936, Precision: 0.606
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_117 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_118 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_117 (Embedding)       (None, 4, 48)        705264      input_117[0][0]                  
__________________________________________________________________________________________________
embedding_118 (Embedding)       (None, 4, 24)        5640        input_118[0][0]                  
__________________________________________________________________________________________________
flatten_117 (Flatten)           (None, 192)          0           embedding_117[0][0]              
__________________________________________________________________________________________________
flatten_118 (Flatten)           (None, 96)           0           embedding_118[0][0]              
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 288)          0           flatten_117[0][0]                
                                                                 flatten_118[0][0]                
__________________________________________________________________________________________________
dense_117 (Dense)               (None, 24)           6936        concatenate_59[0][0]             
__________________________________________________________________________________________________
dropout_59 (Dropout)            (None, 24)           0           dense_117[0][0]                  
__________________________________________________________________________________________________
dense_118 (Dense)               (None, 8)            200         dropout_59[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 28s - loss: 0.0811 - acc: 0.9821 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0448 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0423 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0411 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0403 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0397 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0392 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0391 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0388 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0385 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0382 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0374 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0376 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0384 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.2285e-05 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.9877e-05 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 7.8466e-05 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 5.4987e-05 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 6.9917e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.5958e-06 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 6.3207e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.2166e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 3.1042e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.2240e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 4.5985e-06 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.3450e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.6216e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.7861e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.5840e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.4913e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.5040e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.2459e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1982e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1957e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 2.1685e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1938e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 4.0679e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1939e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.8942e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 3.7137e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1965e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1960e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 3.0459e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 9.4000e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.2105e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.3442e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1931e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1923e-07 - acc: 1.0000
# Training time = 0:21:05.900934
# F-Score(Ordinary) = 0.642, Recall: 0.775, Precision: 0.548
# F-Score(lvc) = 0.533, Recall: 0.825, Precision: 0.394
# F-Score(ireflv) = 0.817, Recall: 0.87, Precision: 0.77
# F-Score(id) = 0.58, Recall: 0.676, Precision: 0.508
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_119 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_120 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_119 (Embedding)       (None, 4, 48)        705264      input_119[0][0]                  
__________________________________________________________________________________________________
embedding_120 (Embedding)       (None, 4, 24)        5640        input_120[0][0]                  
__________________________________________________________________________________________________
flatten_119 (Flatten)           (None, 192)          0           embedding_119[0][0]              
__________________________________________________________________________________________________
flatten_120 (Flatten)           (None, 96)           0           embedding_120[0][0]              
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 288)          0           flatten_119[0][0]                
                                                                 flatten_120[0][0]                
__________________________________________________________________________________________________
dense_119 (Dense)               (None, 24)           6936        concatenate_60[0][0]             
__________________________________________________________________________________________________
dropout_60 (Dropout)            (None, 24)           0           dense_119[0][0]                  
__________________________________________________________________________________________________
dense_120 (Dense)               (None, 8)            200         dropout_60[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.001
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 28s - loss: 0.0818 - acc: 0.9820 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0452 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0427 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0413 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0394 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0373 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0374 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 5.5780e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 4.7486e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 3.3304e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 3.9165e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 2.5560e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 2.5040e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 2.4403e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 2.9571e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 3.9828e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 2.2002e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 2.1385e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 2.5814e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 1.5028e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 2.4205e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 2.7745e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 3.9291e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 1.2546e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 8.1795e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1914e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.8953e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 1.0887e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 7.0492e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.3543e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 3.2859e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.2627e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 2.3354e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 1.0931e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 1.0408e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 1.2249e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 1.8000e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 6.3038e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 9.9147e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 9.1701e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 3.4748e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 4.9786e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.0522e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 8.0728e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 4.9366e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 5.7010e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 7.1453e-05 - acc: 1.0000
# Training time = 0:20:57.864102
# F-Score(Ordinary) = 0.7, Recall: 0.777, Precision: 0.638
# F-Score(lvc) = 0.565, Recall: 0.726, Precision: 0.462
# F-Score(ireflv) = 0.82, Recall: 0.838, Precision: 0.803
# F-Score(id) = 0.696, Recall: 0.753, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_121 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_122 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_121 (Embedding)       (None, 4, 48)        705264      input_121[0][0]                  
__________________________________________________________________________________________________
embedding_122 (Embedding)       (None, 4, 24)        5640        input_122[0][0]                  
__________________________________________________________________________________________________
flatten_121 (Flatten)           (None, 192)          0           embedding_121[0][0]              
__________________________________________________________________________________________________
flatten_122 (Flatten)           (None, 96)           0           embedding_122[0][0]              
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 288)          0           flatten_121[0][0]                
                                                                 flatten_122[0][0]                
__________________________________________________________________________________________________
dense_121 (Dense)               (None, 24)           6936        concatenate_61[0][0]             
__________________________________________________________________________________________________
dropout_61 (Dropout)            (None, 24)           0           dense_121[0][0]                  
__________________________________________________________________________________________________
dense_122 (Dense)               (None, 8)            200         dropout_61[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 28s - loss: 0.0679 - acc: 0.9847 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0447 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0424 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0414 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0408 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0405 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0400 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0400 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0401 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0398 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0394 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0407 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0408 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0407 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0419 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0407 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0408 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0418 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0406 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0012 - acc: 0.9998
Epoch 2/40
 - 3s - loss: 9.4102e-04 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 0.0012 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 0.0015 - acc: 0.9996
Epoch 5/40
 - 3s - loss: 9.0881e-04 - acc: 0.9997
Epoch 6/40
 - 3s - loss: 8.4361e-04 - acc: 0.9997
Epoch 7/40
 - 3s - loss: 7.3336e-04 - acc: 0.9996
Epoch 8/40
 - 3s - loss: 4.6649e-04 - acc: 0.9998
Epoch 9/40
 - 3s - loss: 3.0041e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 2.3869e-04 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.3227e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1323e-04 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 8.1276e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 5.3333e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 3.6984e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 2.5610e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.7692e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.0300e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 6.4425e-06 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 4.4289e-06 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 4.4962e-06 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 2.4020e-06 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.8283e-06 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1687e-06 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 8.0428e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 7.5630e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 5.0074e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 4.0323e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 3.1428e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 2.5110e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 2.0727e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.9105e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.6415e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.6453e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.4537e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.4935e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.3393e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.3041e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.3218e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.2988e-07 - acc: 1.0000
# Training time = 0:20:37.486544
# F-Score(Ordinary) = 0.557, Recall: 0.458, Precision: 0.711
# F-Score(lvc) = 0.433, Recall: 0.362, Precision: 0.538
# F-Score(ireflv) = 0.766, Recall: 0.675, Precision: 0.885
# F-Score(id) = 0.496, Recall: 0.393, Precision: 0.674
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_123 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_124 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_123 (Embedding)       (None, 4, 48)        705264      input_123[0][0]                  
__________________________________________________________________________________________________
embedding_124 (Embedding)       (None, 4, 24)        5640        input_124[0][0]                  
__________________________________________________________________________________________________
flatten_123 (Flatten)           (None, 192)          0           embedding_123[0][0]              
__________________________________________________________________________________________________
flatten_124 (Flatten)           (None, 96)           0           embedding_124[0][0]              
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 288)          0           flatten_123[0][0]                
                                                                 flatten_124[0][0]                
__________________________________________________________________________________________________
dense_123 (Dense)               (None, 24)           6936        concatenate_62[0][0]             
__________________________________________________________________________________________________
dropout_62 (Dropout)            (None, 24)           0           dense_123[0][0]                  
__________________________________________________________________________________________________
dense_124 (Dense)               (None, 8)            200         dropout_62[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0689 - acc: 0.9847 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0449 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0425 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0417 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0410 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0402 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0399 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0401 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0399 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0404 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0404 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0403 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0408 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0410 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0407 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0412 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0409 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0408 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0408 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0407 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.7895e-04 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 2.6892e-07 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 2.9525e-04 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.4647e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 2.9007e-04 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 2.8624e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.3696e-07 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.3305e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 2.8166e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.2660e-07 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.3888e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 4.0767e-04 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.2028e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 2.6610e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1969e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.3111e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.2957e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.2812e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1923e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.2657e-04 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.2483e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.2312e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 2.4143e-04 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1868e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1708e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1560e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1407e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:20:20.967343
# F-Score(Ordinary) = 0.654, Recall: 0.626, Precision: 0.685
# F-Score(lvc) = 0.618, Recall: 0.875, Precision: 0.477
# F-Score(ireflv) = 0.795, Recall: 0.752, Precision: 0.844
# F-Score(id) = 0.541, Recall: 0.457, Precision: 0.663
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_125 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_126 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_125 (Embedding)       (None, 4, 48)        705264      input_125[0][0]                  
__________________________________________________________________________________________________
embedding_126 (Embedding)       (None, 4, 24)        5640        input_126[0][0]                  
__________________________________________________________________________________________________
flatten_125 (Flatten)           (None, 192)          0           embedding_125[0][0]              
__________________________________________________________________________________________________
flatten_126 (Flatten)           (None, 96)           0           embedding_126[0][0]              
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 288)          0           flatten_125[0][0]                
                                                                 flatten_126[0][0]                
__________________________________________________________________________________________________
dense_125 (Dense)               (None, 24)           6936        concatenate_63[0][0]             
__________________________________________________________________________________________________
dropout_63 (Dropout)            (None, 24)           0           dense_125[0][0]                  
__________________________________________________________________________________________________
dense_126 (Dense)               (None, 8)            200         dropout_63[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0692 - acc: 0.9845 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0448 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0427 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0415 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0412 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0404 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0400 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0400 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0404 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0394 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0393 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0394 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0404 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0397 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0408 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0394 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0397 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0400 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0406 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0419 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0414 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0405 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0405 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0401 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0406 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0403 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 7.3981e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 9.2020e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 7.7112e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 6.2923e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 1.2427e-04 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 2.4631e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 7.1668e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 8.0092e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 2.2245e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 8.5682e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 2.0690e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 3.0344e-04 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 6.7780e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 3.7002e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 5.3260e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 5.8879e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 1.6240e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 3.1363e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 5.1473e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 2.7787e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 3.3018e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 3.0766e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 3.3938e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 2.1061e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 1.0066e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 9.7166e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 9.3386e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.3206e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 2.7646e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 1.7566e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 2.4624e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.5957e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 7.2237e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 8.6921e-05 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 7.6565e-05 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 1.1184e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.4201e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 7.4099e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 6.8805e-05 - acc: 1.0000
# Training time = 0:20:24.561449
# F-Score(Ordinary) = 0.656, Recall: 0.64, Precision: 0.673
# F-Score(lvc) = 0.644, Recall: 0.929, Precision: 0.492
# F-Score(ireflv) = 0.845, Recall: 0.863, Precision: 0.828
# F-Score(id) = 0.536, Recall: 0.459, Precision: 0.642
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_127 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_128 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_127 (Embedding)       (None, 4, 48)        705264      input_127[0][0]                  
__________________________________________________________________________________________________
embedding_128 (Embedding)       (None, 4, 24)        5640        input_128[0][0]                  
__________________________________________________________________________________________________
flatten_127 (Flatten)           (None, 192)          0           embedding_127[0][0]              
__________________________________________________________________________________________________
flatten_128 (Flatten)           (None, 96)           0           embedding_128[0][0]              
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 288)          0           flatten_127[0][0]                
                                                                 flatten_128[0][0]                
__________________________________________________________________________________________________
dense_127 (Dense)               (None, 24)           6936        concatenate_64[0][0]             
__________________________________________________________________________________________________
dropout_64 (Dropout)            (None, 24)           0           dense_127[0][0]                  
__________________________________________________________________________________________________
dense_128 (Dense)               (None, 8)            200         dropout_64[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0689 - acc: 0.9845 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0448 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0427 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0416 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0411 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0410 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0403 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0399 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0392 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0403 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0405 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0399 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0397 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0401 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0402 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0424 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0427 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0421 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0423 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0441 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0425 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0423 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0425 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0425 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0426 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0425 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0436 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 6.0448e-06 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 1.4700e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 9.6079e-05 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.7133e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.7090e-04 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.5912e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.6986e-04 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.4110e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.3804e-07 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 8.3869e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 5.2858e-06 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.2715e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.2020e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.6503e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1923e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1927e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 3.7641e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1960e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1960e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.2280e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1928e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1927e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1959e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1936e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1923e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:20:18.438392
# F-Score(Ordinary) = 0.695, Recall: 0.788, Precision: 0.622
# F-Score(lvc) = 0.495, Recall: 0.646, Precision: 0.402
# F-Score(ireflv) = 0.828, Recall: 0.846, Precision: 0.811
# F-Score(id) = 0.711, Recall: 0.804, Precision: 0.637
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_129 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_130 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_129 (Embedding)       (None, 4, 48)        705264      input_129[0][0]                  
__________________________________________________________________________________________________
embedding_130 (Embedding)       (None, 4, 24)        5640        input_130[0][0]                  
__________________________________________________________________________________________________
flatten_129 (Flatten)           (None, 192)          0           embedding_129[0][0]              
__________________________________________________________________________________________________
flatten_130 (Flatten)           (None, 96)           0           embedding_130[0][0]              
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 288)          0           flatten_129[0][0]                
                                                                 flatten_130[0][0]                
__________________________________________________________________________________________________
dense_129 (Dense)               (None, 24)           6936        concatenate_65[0][0]             
__________________________________________________________________________________________________
dropout_65 (Dropout)            (None, 24)           0           dense_129[0][0]                  
__________________________________________________________________________________________________
dense_130 (Dense)               (None, 8)            200         dropout_65[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.002
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0690 - acc: 0.9848 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 26s - loss: 0.0448 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 26s - loss: 0.0428 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0417 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0413 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0408 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0402 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0399 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0398 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0395 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0394 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0395 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0394 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0398 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0395 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0411 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0399 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0403 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0399 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0397 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0405 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0406 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0411 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0403 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0404 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 26s - loss: 0.0400 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0406 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0406 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0407 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0408 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 26s - loss: 0.0407 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 7.8539e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 6.6315e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 4.6162e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 5.3830e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 3.4796e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 3.3760e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 3.2480e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 3.8721e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 5.1039e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 2.7629e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 2.6383e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 3.1125e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 1.7738e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 2.7865e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 3.0810e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 4.1531e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 1.2732e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 8.1159e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1474e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.7357e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 9.4706e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 5.9115e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.0792e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 2.5068e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 9.1277e-05 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 1.5026e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 6.3294e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 5.5650e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 5.9212e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 7.5363e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 2.3469e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 3.3455e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 2.7127e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 9.4060e-06 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.2490e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 2.2421e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.4490e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 7.8263e-06 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 8.1040e-06 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 8.7664e-06 - acc: 1.0000
# Training time = 0:20:17.543741
# F-Score(Ordinary) = 0.674, Recall: 0.706, Precision: 0.644
# F-Score(lvc) = 0.583, Recall: 0.75, Precision: 0.477
# F-Score(ireflv) = 0.803, Recall: 0.821, Precision: 0.787
# F-Score(id) = 0.64, Recall: 0.618, Precision: 0.663
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_131 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_132 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_131 (Embedding)       (None, 4, 48)        705264      input_131[0][0]                  
__________________________________________________________________________________________________
embedding_132 (Embedding)       (None, 4, 24)        5640        input_132[0][0]                  
__________________________________________________________________________________________________
flatten_131 (Flatten)           (None, 192)          0           embedding_131[0][0]              
__________________________________________________________________________________________________
flatten_132 (Flatten)           (None, 96)           0           embedding_132[0][0]              
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 288)          0           flatten_131[0][0]                
                                                                 flatten_132[0][0]                
__________________________________________________________________________________________________
dense_131 (Dense)               (None, 24)           6936        concatenate_66[0][0]             
__________________________________________________________________________________________________
dropout_66 (Dropout)            (None, 24)           0           dense_131[0][0]                  
__________________________________________________________________________________________________
dense_132 (Dense)               (None, 8)            200         dropout_66[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0638 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0492 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0485 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0473 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0473 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0468 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0480 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0477 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0477 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0472 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0485 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0505 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0500 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0505 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0500 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0522 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0516 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0518 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0519 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0541 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0523 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0530 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0559 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0544 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0548 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0547 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0544 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0534 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0548 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0554 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0543 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0561 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0565 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0570 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0559 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0550 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0574 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0551 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0567 - acc: 0.9883 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0567 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0045 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 5.1673e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 2.2179e-04 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.0423e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 5.5235e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 3.0980e-05 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.7397e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 9.9288e-06 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 5.5872e-06 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 3.1699e-06 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.8857e-06 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.0992e-06 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 6.9303e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 4.3674e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 2.9835e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 2.2312e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.7770e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.5110e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.3820e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.2787e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.2391e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.2082e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:20:53.036223
# F-Score(Ordinary) = 0.47, Recall: 0.362, Precision: 0.669
# F-Score(lvc) = 0.523, Recall: 0.683, Precision: 0.424
# F-Score(ireflv) = 0.765, Recall: 0.669, Precision: 0.893
# F-Score(id) = 0.336, Recall: 0.224, Precision: 0.668
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_133 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_134 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_133 (Embedding)       (None, 4, 48)        705264      input_133[0][0]                  
__________________________________________________________________________________________________
embedding_134 (Embedding)       (None, 4, 24)        5640        input_134[0][0]                  
__________________________________________________________________________________________________
flatten_133 (Flatten)           (None, 192)          0           embedding_133[0][0]              
__________________________________________________________________________________________________
flatten_134 (Flatten)           (None, 96)           0           embedding_134[0][0]              
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 288)          0           flatten_133[0][0]                
                                                                 flatten_134[0][0]                
__________________________________________________________________________________________________
dense_133 (Dense)               (None, 24)           6936        concatenate_67[0][0]             
__________________________________________________________________________________________________
dropout_67 (Dropout)            (None, 24)           0           dense_133[0][0]                  
__________________________________________________________________________________________________
dense_134 (Dense)               (None, 8)            200         dropout_67[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0640 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0486 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0476 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0475 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0473 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0473 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0476 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0474 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0475 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0487 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0481 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0492 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 28s - loss: 0.0482 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0509 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0498 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0488 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0489 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0484 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0496 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0492 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0500 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0491 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0499 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0501 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0520 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0507 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0515 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0505 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0521 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0535 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0526 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0538 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0534 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0534 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0528 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0532 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0543 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0533 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0544 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0545 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0053 - acc: 0.9997
Epoch 2/40
 - 3s - loss: 0.0042 - acc: 0.9997
Epoch 3/40
 - 3s - loss: 0.0044 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 0.0069 - acc: 0.9996
Epoch 5/40
 - 3s - loss: 0.0051 - acc: 0.9997
Epoch 6/40
 - 3s - loss: 0.0058 - acc: 0.9996
Epoch 7/40
 - 3s - loss: 0.0033 - acc: 0.9998
Epoch 8/40
 - 3s - loss: 0.0042 - acc: 0.9997
Epoch 9/40
 - 3s - loss: 0.0058 - acc: 0.9996
Epoch 10/40
 - 3s - loss: 0.0046 - acc: 0.9997
Epoch 11/40
 - 3s - loss: 0.0051 - acc: 0.9997
Epoch 12/40
 - 3s - loss: 0.0040 - acc: 0.9998
Epoch 13/40
 - 3s - loss: 0.0046 - acc: 0.9997
Epoch 14/40
 - 3s - loss: 0.0046 - acc: 0.9997
Epoch 15/40
 - 3s - loss: 0.0040 - acc: 0.9998
Epoch 16/40
 - 3s - loss: 0.0040 - acc: 0.9998
Epoch 17/40
 - 3s - loss: 0.0053 - acc: 0.9997
Epoch 18/40
 - 3s - loss: 0.0077 - acc: 0.9995
Epoch 19/40
 - 3s - loss: 0.0038 - acc: 0.9998
Epoch 20/40
 - 3s - loss: 0.0071 - acc: 0.9996
Epoch 21/40
 - 3s - loss: 0.0042 - acc: 0.9997
Epoch 22/40
 - 3s - loss: 0.0046 - acc: 0.9997
Epoch 23/40
 - 3s - loss: 0.0055 - acc: 0.9997
Epoch 24/40
 - 3s - loss: 0.0046 - acc: 0.9997
Epoch 25/40
 - 3s - loss: 0.0046 - acc: 0.9997
Epoch 26/40
 - 3s - loss: 0.0069 - acc: 0.9996
Epoch 27/40
 - 3s - loss: 0.0055 - acc: 0.9997
Epoch 28/40
 - 3s - loss: 0.0051 - acc: 0.9997
Epoch 29/40
 - 3s - loss: 0.0038 - acc: 0.9998
Epoch 30/40
 - 3s - loss: 0.0029 - acc: 0.9998
Epoch 31/40
 - 3s - loss: 0.0060 - acc: 0.9996
Epoch 32/40
 - 3s - loss: 0.0049 - acc: 0.9997
Epoch 33/40
 - 3s - loss: 0.0064 - acc: 0.9996
Epoch 34/40
 - 3s - loss: 0.0066 - acc: 0.9996
Epoch 35/40
 - 3s - loss: 0.0042 - acc: 0.9997
Epoch 36/40
 - 3s - loss: 0.0044 - acc: 0.9997
Epoch 37/40
 - 3s - loss: 0.0062 - acc: 0.9996
Epoch 38/40
 - 3s - loss: 0.0058 - acc: 0.9996
Epoch 39/40
 - 3s - loss: 0.0058 - acc: 0.9996
Epoch 40/40
 - 3s - loss: 0.0049 - acc: 0.9997
# Training time = 0:20:46.615225
# F-Score(Ordinary) = 0.507, Recall: 0.407, Precision: 0.671
# F-Score(lvc) = 0.587, Recall: 0.803, Precision: 0.462
# F-Score(ireflv) = 0.763, Recall: 0.748, Precision: 0.779
# F-Score(id) = 0.382, Recall: 0.26, Precision: 0.72
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_135 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_136 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_135 (Embedding)       (None, 4, 48)        705264      input_135[0][0]                  
__________________________________________________________________________________________________
embedding_136 (Embedding)       (None, 4, 24)        5640        input_136[0][0]                  
__________________________________________________________________________________________________
flatten_135 (Flatten)           (None, 192)          0           embedding_135[0][0]              
__________________________________________________________________________________________________
flatten_136 (Flatten)           (None, 96)           0           embedding_136[0][0]              
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 288)          0           flatten_135[0][0]                
                                                                 flatten_136[0][0]                
__________________________________________________________________________________________________
dense_135 (Dense)               (None, 24)           6936        concatenate_68[0][0]             
__________________________________________________________________________________________________
dropout_68 (Dropout)            (None, 24)           0           dense_135[0][0]                  
__________________________________________________________________________________________________
dense_136 (Dense)               (None, 8)            200         dropout_68[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.0647 - acc: 0.9852 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0494 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0483 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0471 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0467 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0479 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0473 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0485 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0472 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0468 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0469 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0481 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0487 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0477 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0475 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0484 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0494 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0490 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0505 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0489 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0502 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0507 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0497 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0496 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0507 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0494 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0494 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0500 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0515 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0502 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0527 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0500 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0509 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0515 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0515 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0509 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0518 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0517 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0533 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0529 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0056 - acc: 0.9996
Epoch 2/40
 - 3s - loss: 0.0051 - acc: 0.9997
Epoch 3/40
 - 3s - loss: 0.0055 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 0.0066 - acc: 0.9996
Epoch 5/40
 - 3s - loss: 0.0051 - acc: 0.9997
Epoch 6/40
 - 3s - loss: 0.0062 - acc: 0.9996
Epoch 7/40
 - 3s - loss: 0.0049 - acc: 0.9997
Epoch 8/40
 - 3s - loss: 0.0049 - acc: 0.9997
Epoch 9/40
 - 3s - loss: 0.0049 - acc: 0.9997
Epoch 10/40
 - 3s - loss: 0.0066 - acc: 0.9996
Epoch 11/40
 - 3s - loss: 0.0062 - acc: 0.9996
Epoch 12/40
 - 3s - loss: 0.0051 - acc: 0.9997
Epoch 13/40
 - 3s - loss: 0.0055 - acc: 0.9997
Epoch 14/40
 - 3s - loss: 0.0062 - acc: 0.9996
Epoch 15/40
 - 3s - loss: 0.0073 - acc: 0.9995
Epoch 16/40
 - 3s - loss: 0.0044 - acc: 0.9997
Epoch 17/40
 - 3s - loss: 0.0038 - acc: 0.9998
Epoch 18/40
 - 3s - loss: 0.0040 - acc: 0.9998
Epoch 19/40
 - 3s - loss: 0.0051 - acc: 0.9997
Epoch 20/40
 - 3s - loss: 0.0055 - acc: 0.9997
Epoch 21/40
 - 3s - loss: 0.0051 - acc: 0.9997
Epoch 22/40
 - 3s - loss: 0.0033 - acc: 0.9998
Epoch 23/40
 - 3s - loss: 0.0053 - acc: 0.9997
Epoch 24/40
 - 3s - loss: 0.0064 - acc: 0.9996
Epoch 25/40
 - 3s - loss: 0.0040 - acc: 0.9998
Epoch 26/40
 - 3s - loss: 0.0049 - acc: 0.9997
Epoch 27/40
 - 3s - loss: 0.0044 - acc: 0.9997
Epoch 28/40
 - 3s - loss: 0.0058 - acc: 0.9996
Epoch 29/40
 - 3s - loss: 0.0064 - acc: 0.9996
Epoch 30/40
 - 3s - loss: 0.0060 - acc: 0.9996
Epoch 31/40
 - 3s - loss: 0.0060 - acc: 0.9996
Epoch 32/40
 - 3s - loss: 0.0062 - acc: 0.9996
Epoch 33/40
 - 3s - loss: 0.0044 - acc: 0.9997
Epoch 34/40
 - 3s - loss: 0.0049 - acc: 0.9997
Epoch 35/40
 - 3s - loss: 0.0038 - acc: 0.9998
Epoch 36/40
 - 3s - loss: 0.0055 - acc: 0.9997
Epoch 37/40
 - 3s - loss: 0.0044 - acc: 0.9997
Epoch 38/40
 - 3s - loss: 0.0038 - acc: 0.9998
Epoch 39/40
 - 3s - loss: 0.0064 - acc: 0.9996
Epoch 40/40
 - 3s - loss: 0.0053 - acc: 0.9997
# Training time = 0:21:03.173399
# F-Score(Ordinary) = 0.352, Recall: 0.237, Precision: 0.678
# F-Score(lvc) = 0.58, Recall: 0.8, Precision: 0.455
# F-Score(ireflv) = 0.8, Recall: 0.781, Precision: 0.82
# F-Score(id) = 0.225, Recall: 0.135, Precision: 0.679
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_137 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_138 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_137 (Embedding)       (None, 4, 48)        705264      input_137[0][0]                  
__________________________________________________________________________________________________
embedding_138 (Embedding)       (None, 4, 24)        5640        input_138[0][0]                  
__________________________________________________________________________________________________
flatten_137 (Flatten)           (None, 192)          0           embedding_137[0][0]              
__________________________________________________________________________________________________
flatten_138 (Flatten)           (None, 96)           0           embedding_138[0][0]              
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 288)          0           flatten_137[0][0]                
                                                                 flatten_138[0][0]                
__________________________________________________________________________________________________
dense_137 (Dense)               (None, 24)           6936        concatenate_69[0][0]             
__________________________________________________________________________________________________
dropout_69 (Dropout)            (None, 24)           0           dense_137[0][0]                  
__________________________________________________________________________________________________
dense_138 (Dense)               (None, 8)            200         dropout_69[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0645 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0490 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0473 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0470 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0487 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0469 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0484 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0481 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0490 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0479 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0493 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0486 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0476 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0481 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 28s - loss: 0.0510 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0502 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0517 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0513 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0520 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0513 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0515 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0511 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0516 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0504 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0509 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0525 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0532 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0544 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0529 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0530 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0540 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0525 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0533 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0526 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0528 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0534 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0546 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0532 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0543 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0540 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 2.1797e-04 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 2.2136e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 2.2138e-04 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.4124e-07 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.2685e-07 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 2.2135e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 2.2135e-04 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.2126e-07 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 2.2135e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 2.2135e-04 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 6.6380e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.1930e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1935e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 2.2135e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1922e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 2.2135e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 2.2135e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 6.6380e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 4.4257e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 4.4257e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 4.4257e-04 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 4.4257e-04 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 8.8502e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 4.4257e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 4.4257e-04 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 6.6380e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 2.2135e-04 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 2.2135e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 2.2135e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 2.2135e-04 - acc: 1.0000
# Training time = 0:21:29.146279
# F-Score(Ordinary) = 0.315, Recall: 0.204, Precision: 0.691
# F-Score(lvc) = 0.575, Recall: 0.762, Precision: 0.462
# F-Score(ireflv) = 0.625, Recall: 0.491, Precision: 0.861
# F-Score(id) = 0.177, Recall: 0.103, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_139 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_140 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_139 (Embedding)       (None, 4, 48)        705264      input_139[0][0]                  
__________________________________________________________________________________________________
embedding_140 (Embedding)       (None, 4, 24)        5640        input_140[0][0]                  
__________________________________________________________________________________________________
flatten_139 (Flatten)           (None, 192)          0           embedding_139[0][0]              
__________________________________________________________________________________________________
flatten_140 (Flatten)           (None, 96)           0           embedding_140[0][0]              
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 288)          0           flatten_139[0][0]                
                                                                 flatten_140[0][0]                
__________________________________________________________________________________________________
dense_139 (Dense)               (None, 24)           6936        concatenate_70[0][0]             
__________________________________________________________________________________________________
dropout_70 (Dropout)            (None, 24)           0           dense_139[0][0]                  
__________________________________________________________________________________________________
dense_140 (Dense)               (None, 8)            200         dropout_70[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0651 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0492 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0480 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0475 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0475 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0481 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0484 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0476 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0475 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0482 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0483 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0476 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0480 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0477 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0490 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0496 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0527 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0501 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0500 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.0490 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.0494 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.0502 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.0505 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.0515 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.0536 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.0507 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.0516 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.0508 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.0517 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0531 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0538 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0551 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0561 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0546 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0545 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0551 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0548 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0529 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0557 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0541 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0043 - acc: 0.9996
Epoch 2/40
 - 3s - loss: 0.0036 - acc: 0.9996
Epoch 3/40
 - 3s - loss: 0.0027 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 0.0020 - acc: 0.9997
Epoch 5/40
 - 3s - loss: 0.0015 - acc: 0.9998
Epoch 6/40
 - 3s - loss: 0.0012 - acc: 0.9998
Epoch 7/40
 - 3s - loss: 0.0013 - acc: 0.9996
Epoch 8/40
 - 3s - loss: 7.5577e-04 - acc: 0.9996
Epoch 9/40
 - 3s - loss: 3.9291e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.1381e-04 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 4.8548e-05 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 4.6637e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.9634e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1864e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 7.2470e-06 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 4.5456e-06 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 2.3474e-06 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.3162e-06 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.0531e-06 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 7.1648e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 3.5538e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 2.3962e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 2.2353e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 2.0267e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.6168e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.5571e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.3457e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.3416e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.2694e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.2776e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.2354e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.2484e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.2275e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.2192e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.2194e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.2146e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.2140e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.2040e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.2105e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.2053e-07 - acc: 1.0000
# Training time = 0:20:40.945787
# F-Score(Ordinary) = 0.424, Recall: 0.414, Precision: 0.434
# F-Score(lvc) = 0.487, Recall: 0.43, Precision: 0.561
# F-Score(ireflv) = 0.586, Recall: 0.451, Precision: 0.836
# F-Score(id) = 0.081, Recall: 0.189, Precision: 0.052
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_141 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_142 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_141 (Embedding)       (None, 4, 48)        705264      input_141[0][0]                  
__________________________________________________________________________________________________
embedding_142 (Embedding)       (None, 4, 24)        5640        input_142[0][0]                  
__________________________________________________________________________________________________
flatten_141 (Flatten)           (None, 192)          0           embedding_141[0][0]              
__________________________________________________________________________________________________
flatten_142 (Flatten)           (None, 96)           0           embedding_142[0][0]              
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 288)          0           flatten_141[0][0]                
                                                                 flatten_142[0][0]                
__________________________________________________________________________________________________
dense_141 (Dense)               (None, 24)           6936        concatenate_71[0][0]             
__________________________________________________________________________________________________
dropout_71 (Dropout)            (None, 24)           0           dense_141[0][0]                  
__________________________________________________________________________________________________
dense_142 (Dense)               (None, 8)            200         dropout_71[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0742 - acc: 0.9832 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0656 - acc: 0.9858 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0677 - acc: 0.9860 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0727 - acc: 0.9853 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0786 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0822 - acc: 0.9843 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0897 - acc: 0.9825 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0980 - acc: 0.9774 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.1015 - acc: 0.9776 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.1042 - acc: 0.9771 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.1020 - acc: 0.9773 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.1060 - acc: 0.9772 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.1126 - acc: 0.9765 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.1103 - acc: 0.9766 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.1181 - acc: 0.9764 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.1133 - acc: 0.9771 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.1259 - acc: 0.9750 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.1214 - acc: 0.9757 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.1191 - acc: 0.9760 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.1202 - acc: 0.9755 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.1319 - acc: 0.9740 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.1346 - acc: 0.9755 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.1367 - acc: 0.9760 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.1401 - acc: 0.9754 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.1419 - acc: 0.9735 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.1375 - acc: 0.9758 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.1404 - acc: 0.9755 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.1469 - acc: 0.9745 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.1442 - acc: 0.9757 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.1557 - acc: 0.9746 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.1589 - acc: 0.9745 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.1602 - acc: 0.9737 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.2146 - acc: 0.9707 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.1439 - acc: 0.9748 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.1545 - acc: 0.9744 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.1556 - acc: 0.9744 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.1715 - acc: 0.9737 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.1614 - acc: 0.9746 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.1648 - acc: 0.9743 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.1677 - acc: 0.9745 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0094 - acc: 0.9972
Epoch 2/40
 - 3s - loss: 6.4470e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 2.7362e-04 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.3687e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 7.2309e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 4.0078e-05 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 2.2082e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.2567e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 7.1393e-06 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 4.0237e-06 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 2.3588e-06 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.3675e-06 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 8.1584e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 5.0149e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 3.3025e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 2.2545e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.7035e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.3516e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.2072e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:20:24.215911
# F-Score(Ordinary) = 0.395, Recall: 0.283, Precision: 0.653
# F-Score(lvc) = 0.444, Recall: 0.456, Precision: 0.432
# F-Score(ireflv) = 0.546, Recall: 0.404, Precision: 0.844
# F-Score(id) = 0.285, Recall: 0.188, Precision: 0.591
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_143 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_144 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_143 (Embedding)       (None, 4, 48)        705264      input_143[0][0]                  
__________________________________________________________________________________________________
embedding_144 (Embedding)       (None, 4, 24)        5640        input_144[0][0]                  
__________________________________________________________________________________________________
flatten_143 (Flatten)           (None, 192)          0           embedding_143[0][0]              
__________________________________________________________________________________________________
flatten_144 (Flatten)           (None, 96)           0           embedding_144[0][0]              
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 288)          0           flatten_143[0][0]                
                                                                 flatten_144[0][0]                
__________________________________________________________________________________________________
dense_143 (Dense)               (None, 24)           6936        concatenate_72[0][0]             
__________________________________________________________________________________________________
dropout_72 (Dropout)            (None, 24)           0           dense_143[0][0]                  
__________________________________________________________________________________________________
dense_144 (Dense)               (None, 8)            200         dropout_72[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0739 - acc: 0.9832 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0658 - acc: 0.9863 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0666 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0716 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0757 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.5702 - acc: 0.9546 - val_loss: 16.1181 - val_acc: 0.0000e+00
Epoch 7/40
 - 27s - loss: 0.1557 - acc: 0.9804 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0834 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0839 - acc: 0.9846 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0861 - acc: 0.9847 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0977 - acc: 0.9824 - val_loss: 1.3709e-06 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0982 - acc: 0.9823 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0936 - acc: 0.9826 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.1027 - acc: 0.9822 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0971 - acc: 0.9823 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0965 - acc: 0.9820 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.1008 - acc: 0.9816 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.1014 - acc: 0.9816 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.1047 - acc: 0.9810 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.1156 - acc: 0.9801 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.1057 - acc: 0.9806 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.1097 - acc: 0.9804 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.1509 - acc: 0.9773 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.1181 - acc: 0.9803 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.1292 - acc: 0.9803 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.1261 - acc: 0.9806 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.1370 - acc: 0.9797 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.1497 - acc: 0.9775 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.1354 - acc: 0.9791 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.1330 - acc: 0.9786 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.1418 - acc: 0.9783 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.1383 - acc: 0.9787 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.1398 - acc: 0.9783 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.1469 - acc: 0.9784 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.1635 - acc: 0.9764 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.1635 - acc: 0.9736 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.1758 - acc: 0.9725 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.1916 - acc: 0.9754 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.2001 - acc: 0.9756 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.2091 - acc: 0.9759 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0101 - acc: 0.9971
Epoch 2/40
 - 3s - loss: 4.1831e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.7101e-04 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 8.3797e-05 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 4.4879e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 2.4616e-05 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.3318e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 7.6993e-06 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 4.5035e-06 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 2.5564e-06 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.4713e-06 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 8.7519e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 5.5836e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 3.6636e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 2.5803e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.9636e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.6127e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.4176e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.3002e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.2387e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.2048e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:20:43.081323
# F-Score(Ordinary) = 0.432, Recall: 0.324, Precision: 0.649
# F-Score(lvc) = 0.366, Recall: 0.279, Precision: 0.53
# F-Score(id) = 0.369, Recall: 0.271, Precision: 0.575
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_145 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_146 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_145 (Embedding)       (None, 4, 48)        705264      input_145[0][0]                  
__________________________________________________________________________________________________
embedding_146 (Embedding)       (None, 4, 24)        5640        input_146[0][0]                  
__________________________________________________________________________________________________
flatten_145 (Flatten)           (None, 192)          0           embedding_145[0][0]              
__________________________________________________________________________________________________
flatten_146 (Flatten)           (None, 96)           0           embedding_146[0][0]              
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 288)          0           flatten_145[0][0]                
                                                                 flatten_146[0][0]                
__________________________________________________________________________________________________
dense_145 (Dense)               (None, 24)           6936        concatenate_73[0][0]             
__________________________________________________________________________________________________
dropout_73 (Dropout)            (None, 24)           0           dense_145[0][0]                  
__________________________________________________________________________________________________
dense_146 (Dense)               (None, 8)            200         dropout_73[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0732 - acc: 0.9833 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0654 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0669 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0694 - acc: 0.9866 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0707 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0718 - acc: 0.9869 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0754 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0791 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0829 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0842 - acc: 0.9867 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0922 - acc: 0.9855 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0936 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0939 - acc: 0.9850 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0988 - acc: 0.9852 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.0938 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0957 - acc: 0.9851 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.0989 - acc: 0.9845 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.0996 - acc: 0.9845 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.0995 - acc: 0.9842 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.1028 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.1056 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.1045 - acc: 0.9831 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.1062 - acc: 0.9829 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.1052 - acc: 0.9829 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.1112 - acc: 0.9825 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.1085 - acc: 0.9833 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.1134 - acc: 0.9826 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.1099 - acc: 0.9823 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.1137 - acc: 0.9820 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.1149 - acc: 0.9820 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.1227 - acc: 0.9767 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.1169 - acc: 0.9774 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.1229 - acc: 0.9783 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.1320 - acc: 0.9778 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.1289 - acc: 0.9784 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.1283 - acc: 0.9788 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.1339 - acc: 0.9784 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.1344 - acc: 0.9783 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.1359 - acc: 0.9783 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.1439 - acc: 0.9778 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.2453 - acc: 0.9848
Epoch 2/40
 - 3s - loss: 0.2270 - acc: 0.9859
Epoch 3/40
 - 3s - loss: 0.2303 - acc: 0.9857
Epoch 4/40
 - 3s - loss: 0.2352 - acc: 0.9854
Epoch 5/40
 - 3s - loss: 0.2356 - acc: 0.9854
Epoch 6/40
 - 3s - loss: 0.2372 - acc: 0.9853
Epoch 7/40
 - 3s - loss: 0.2228 - acc: 0.9862
Epoch 8/40
 - 3s - loss: 0.2323 - acc: 0.9856
Epoch 9/40
 - 3s - loss: 0.2318 - acc: 0.9856
Epoch 10/40
 - 3s - loss: 0.2248 - acc: 0.9861
Epoch 11/40
 - 3s - loss: 0.2400 - acc: 0.9851
Epoch 12/40
 - 3s - loss: 0.2349 - acc: 0.9854
Epoch 13/40
 - 3s - loss: 0.2316 - acc: 0.9856
Epoch 14/40
 - 3s - loss: 0.2380 - acc: 0.9852
Epoch 15/40
 - 3s - loss: 0.2248 - acc: 0.9861
Epoch 16/40
 - 3s - loss: 0.2411 - acc: 0.9850
Epoch 17/40
 - 3s - loss: 0.2312 - acc: 0.9857
Epoch 18/40
 - 3s - loss: 0.2376 - acc: 0.9853
Epoch 19/40
 - 3s - loss: 0.2449 - acc: 0.9848
Epoch 20/40
 - 3s - loss: 0.2292 - acc: 0.9858
Epoch 21/40
 - 3s - loss: 0.2245 - acc: 0.9861
Epoch 22/40
 - 3s - loss: 0.2299 - acc: 0.9857
Epoch 23/40
 - 3s - loss: 0.2495 - acc: 0.9845
Epoch 24/40
 - 3s - loss: 0.2268 - acc: 0.9859
Epoch 25/40
 - 3s - loss: 0.2456 - acc: 0.9848
Epoch 26/40
 - 3s - loss: 0.2323 - acc: 0.9856
Epoch 27/40
 - 3s - loss: 0.2248 - acc: 0.9861
Epoch 28/40
 - 3s - loss: 0.2296 - acc: 0.9858
Epoch 29/40
 - 3s - loss: 0.2343 - acc: 0.9855
Epoch 30/40
 - 3s - loss: 0.2234 - acc: 0.9861
Epoch 31/40
 - 3s - loss: 0.2447 - acc: 0.9848
Epoch 32/40
 - 3s - loss: 0.2376 - acc: 0.9853
Epoch 33/40
 - 3s - loss: 0.2203 - acc: 0.9863
Epoch 34/40
 - 3s - loss: 0.2391 - acc: 0.9852
Epoch 35/40
 - 3s - loss: 0.2349 - acc: 0.9854
Epoch 36/40
 - 3s - loss: 0.2380 - acc: 0.9852
Epoch 37/40
 - 3s - loss: 0.2334 - acc: 0.9855
Epoch 38/40
 - 3s - loss: 0.2307 - acc: 0.9857
Epoch 39/40
 - 3s - loss: 0.2290 - acc: 0.9858
Epoch 40/40
 - 3s - loss: 0.2411 - acc: 0.9850
# Training time = 0:20:45.247512
# F-Score(Ordinary) = 0.472, Recall: 0.408, Precision: 0.559
# F-Score(lvc) = 0.311, Recall: 0.311, Precision: 0.311
# F-Score(ireflv) = 0.618, Recall: 0.503, Precision: 0.803
# F-Score(id) = 0.434, Recall: 0.364, Precision: 0.539
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_147 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_148 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_147 (Embedding)       (None, 4, 48)        705264      input_147[0][0]                  
__________________________________________________________________________________________________
embedding_148 (Embedding)       (None, 4, 24)        5640        input_148[0][0]                  
__________________________________________________________________________________________________
flatten_147 (Flatten)           (None, 192)          0           embedding_147[0][0]              
__________________________________________________________________________________________________
flatten_148 (Flatten)           (None, 96)           0           embedding_148[0][0]              
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 288)          0           flatten_147[0][0]                
                                                                 flatten_148[0][0]                
__________________________________________________________________________________________________
dense_147 (Dense)               (None, 24)           6936        concatenate_74[0][0]             
__________________________________________________________________________________________________
dropout_74 (Dropout)            (None, 24)           0           dense_147[0][0]                  
__________________________________________________________________________________________________
dense_148 (Dense)               (None, 8)            200         dropout_74[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.0735 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0654 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0688 - acc: 0.9862 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0708 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0720 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0742 - acc: 0.9854 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0781 - acc: 0.9849 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0796 - acc: 0.9842 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0883 - acc: 0.9837 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0894 - acc: 0.9840 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0905 - acc: 0.9836 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0966 - acc: 0.9828 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.1013 - acc: 0.9819 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.0990 - acc: 0.9820 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.1038 - acc: 0.9800 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0988 - acc: 0.9816 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.1009 - acc: 0.9815 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.1098 - acc: 0.9795 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.1098 - acc: 0.9783 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.1126 - acc: 0.9774 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.1239 - acc: 0.9781 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.1205 - acc: 0.9780 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.1178 - acc: 0.9787 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.1288 - acc: 0.9762 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.1511 - acc: 0.9704 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.1400 - acc: 0.9729 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.1494 - acc: 0.9745 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.1777 - acc: 0.9752 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.1674 - acc: 0.9769 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.1956 - acc: 0.9767 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.2177 - acc: 0.9746 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.2237 - acc: 0.9729 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.2492 - acc: 0.9678 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.2364 - acc: 0.9741 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.2394 - acc: 0.9749 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.2486 - acc: 0.9722 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.2305 - acc: 0.9764 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.2728 - acc: 0.9676 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.2382 - acc: 0.9741 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.2900 - acc: 0.9730 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 3.1410 - acc: 0.8031
Epoch 2/40
 - 3s - loss: 3.1298 - acc: 0.8058
Epoch 3/40
 - 3s - loss: 3.0971 - acc: 0.8079
Epoch 4/40
 - 3s - loss: 3.0778 - acc: 0.8091
Epoch 5/40
 - 3s - loss: 3.0753 - acc: 0.8092
Epoch 6/40
 - 3s - loss: 3.0983 - acc: 0.8078
Epoch 7/40
 - 3s - loss: 3.1113 - acc: 0.8070
Epoch 8/40
 - 3s - loss: 3.0677 - acc: 0.8097
Epoch 9/40
 - 3s - loss: 3.1056 - acc: 0.8073
Epoch 10/40
 - 3s - loss: 3.1219 - acc: 0.8063
Epoch 11/40
 - 3s - loss: 3.0762 - acc: 0.8091
Epoch 12/40
 - 3s - loss: 3.0985 - acc: 0.8078
Epoch 13/40
 - 3s - loss: 3.1162 - acc: 0.8067
Epoch 14/40
 - 3s - loss: 3.0739 - acc: 0.8093
Epoch 15/40
 - 3s - loss: 3.0967 - acc: 0.8079
Epoch 16/40
 - 3s - loss: 3.1350 - acc: 0.8055
Epoch 17/40
 - 3s - loss: 3.1235 - acc: 0.8062
Epoch 18/40
 - 3s - loss: 3.0896 - acc: 0.8083
Epoch 19/40
 - 3s - loss: 3.0797 - acc: 0.8089
Epoch 20/40
 - 3s - loss: 3.0219 - acc: 0.8125
Epoch 21/40
 - 3s - loss: 3.1184 - acc: 0.8065
Epoch 22/40
 - 3s - loss: 3.1326 - acc: 0.8056
Epoch 23/40
 - 3s - loss: 3.1173 - acc: 0.8066
Epoch 24/40
 - 3s - loss: 3.0985 - acc: 0.8078
Epoch 25/40
 - 3s - loss: 3.0819 - acc: 0.8088
Epoch 26/40
 - 3s - loss: 3.1219 - acc: 0.8063
Epoch 27/40
 - 3s - loss: 3.0790 - acc: 0.8090
Epoch 28/40
 - 3s - loss: 3.0790 - acc: 0.8090
Epoch 29/40
 - 3s - loss: 3.1047 - acc: 0.8074
Epoch 30/40
 - 3s - loss: 3.0919 - acc: 0.8082
Epoch 31/40
 - 3s - loss: 3.0987 - acc: 0.8077
Epoch 32/40
 - 3s - loss: 3.1173 - acc: 0.8066
Epoch 33/40
 - 3s - loss: 3.1166 - acc: 0.8066
Epoch 34/40
 - 3s - loss: 3.0912 - acc: 0.8082
Epoch 35/40
 - 3s - loss: 3.0901 - acc: 0.8083
Epoch 36/40
 - 3s - loss: 3.0649 - acc: 0.8098
Epoch 37/40
 - 3s - loss: 3.1224 - acc: 0.8063
Epoch 38/40
 - 3s - loss: 3.0585 - acc: 0.8102
Epoch 39/40
 - 3s - loss: 3.0972 - acc: 0.8078
Epoch 40/40
 - 3s - loss: 3.0759 - acc: 0.8092
# Training time = 0:20:45.777888
# F-Score(Ordinary) = 0.423, Recall: 0.309, Precision: 0.667
# F-Score(lvc) = 0.539, Recall: 0.764, Precision: 0.417
# F-Score(id) = 0.489, Recall: 0.42, Precision: 0.585
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_149 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_150 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_149 (Embedding)       (None, 4, 48)        705264      input_149[0][0]                  
__________________________________________________________________________________________________
embedding_150 (Embedding)       (None, 4, 24)        5640        input_150[0][0]                  
__________________________________________________________________________________________________
flatten_149 (Flatten)           (None, 192)          0           embedding_149[0][0]              
__________________________________________________________________________________________________
flatten_150 (Flatten)           (None, 96)           0           embedding_150[0][0]              
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 288)          0           flatten_149[0][0]                
                                                                 flatten_150[0][0]                
__________________________________________________________________________________________________
dense_149 (Dense)               (None, 24)           6936        concatenate_75[0][0]             
__________________________________________________________________________________________________
dropout_75 (Dropout)            (None, 24)           0           dense_149[0][0]                  
__________________________________________________________________________________________________
dense_150 (Dense)               (None, 8)            200         dropout_75[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adam, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 27s - loss: 0.0746 - acc: 0.9829 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 27s - loss: 0.0672 - acc: 0.9859 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 27s - loss: 0.0666 - acc: 0.9865 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 27s - loss: 0.0698 - acc: 0.9861 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 27s - loss: 0.0727 - acc: 0.9864 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 27s - loss: 0.0765 - acc: 0.9855 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 27s - loss: 0.0850 - acc: 0.9847 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0910 - acc: 0.9835 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0922 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0967 - acc: 0.9826 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 27s - loss: 0.0966 - acc: 0.9831 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 27s - loss: 0.0975 - acc: 0.9834 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.1028 - acc: 0.9828 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 27s - loss: 0.1037 - acc: 0.9822 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 27s - loss: 0.1059 - acc: 0.9817 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.1083 - acc: 0.9813 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 27s - loss: 0.1068 - acc: 0.9819 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 27s - loss: 0.1097 - acc: 0.9818 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 27s - loss: 0.1122 - acc: 0.9804 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 27s - loss: 0.1296 - acc: 0.9778 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 27s - loss: 0.1149 - acc: 0.9812 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 27s - loss: 0.1140 - acc: 0.9815 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 27s - loss: 0.1201 - acc: 0.9789 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 27s - loss: 0.1180 - acc: 0.9761 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 27s - loss: 0.1206 - acc: 0.9763 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 27s - loss: 0.1202 - acc: 0.9766 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 27s - loss: 0.1248 - acc: 0.9743 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 27s - loss: 0.1271 - acc: 0.9759 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 27s - loss: 0.1294 - acc: 0.9757 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.1220 - acc: 0.9757 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.1315 - acc: 0.9759 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.1446 - acc: 0.9691 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.1493 - acc: 0.9678 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.1461 - acc: 0.9689 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.1594 - acc: 0.9678 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.1547 - acc: 0.9682 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.1489 - acc: 0.9697 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.1672 - acc: 0.9698 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.1846 - acc: 0.9677 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.1692 - acc: 0.9645 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0100 - acc: 0.9976
Epoch 2/40
 - 3s - loss: 6.8587e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 2.8501e-04 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.4130e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 7.6826e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 4.1525e-05 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 2.3468e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.3012e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 7.4222e-06 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 4.2916e-06 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 2.4728e-06 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.4351e-06 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 8.5999e-07 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 5.2531e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 3.3994e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 2.3848e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.7783e-07 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.4436e-07 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.1950e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1921e-07 - acc: 1.0000
# Training time = 0:20:26.149056
# F-Score(Ordinary) = 0.476, Recall: 0.504, Precision: 0.452
# F-Score(lvc) = 0.464, Recall: 0.459, Precision: 0.47
# F-Score(ireflv) = 0.747, Recall: 0.692, Precision: 0.811
# F-Score(id) = 0.181, Recall: 0.298, Precision: 0.13
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_151 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_152 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_151 (Embedding)       (None, 4, 48)        705264      input_151[0][0]                  
__________________________________________________________________________________________________
embedding_152 (Embedding)       (None, 4, 24)        5640        input_152[0][0]                  
__________________________________________________________________________________________________
flatten_151 (Flatten)           (None, 192)          0           embedding_151[0][0]              
__________________________________________________________________________________________________
flatten_152 (Flatten)           (None, 96)           0           embedding_152[0][0]              
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 288)          0           flatten_151[0][0]                
                                                                 flatten_152[0][0]                
__________________________________________________________________________________________________
dense_151 (Dense)               (None, 24)           6936        concatenate_76[0][0]             
__________________________________________________________________________________________________
dropout_76 (Dropout)            (None, 24)           0           dense_151[0][0]                  
__________________________________________________________________________________________________
dense_152 (Dense)               (None, 8)            200         dropout_76[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.1249 - acc: 0.9733 - val_loss: 5.9056e-04 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0673 - acc: 0.9859 - val_loss: 1.7245e-04 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0604 - acc: 0.9876 - val_loss: 7.7012e-05 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0568 - acc: 0.9884 - val_loss: 3.6479e-05 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0546 - acc: 0.9889 - val_loss: 2.0981e-05 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0531 - acc: 0.9892 - val_loss: 1.3590e-05 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0518 - acc: 0.9895 - val_loss: 9.2984e-06 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0508 - acc: 0.9898 - val_loss: 6.6757e-06 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0497 - acc: 0.9900 - val_loss: 4.7088e-06 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0490 - acc: 0.9900 - val_loss: 3.2783e-06 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0486 - acc: 0.9903 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0483 - acc: 0.9903 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0476 - acc: 0.9905 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0472 - acc: 0.9905 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0469 - acc: 0.9906 - val_loss: 8.9407e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0466 - acc: 0.9907 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0461 - acc: 0.9908 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0459 - acc: 0.9908 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0455 - acc: 0.9909 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0455 - acc: 0.9910 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0452 - acc: 0.9910 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0449 - acc: 0.9911 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 26s - loss: 0.0449 - acc: 0.9911 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 24/40
 - 26s - loss: 0.0446 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 26s - loss: 0.0445 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 26s - loss: 0.0444 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 26s - loss: 0.0441 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 26s - loss: 0.0441 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 26s - loss: 0.0438 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 26s - loss: 0.0439 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 26s - loss: 0.0436 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 26s - loss: 0.0435 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 26s - loss: 0.0434 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 26s - loss: 0.0433 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 26s - loss: 0.0432 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 26s - loss: 0.0431 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 26s - loss: 0.0430 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 26s - loss: 0.0430 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0429 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0427 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0014 - acc: 0.9998
Epoch 2/40
 - 3s - loss: 0.0012 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 0.0013 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 0.0015 - acc: 0.9996
Epoch 5/40
 - 3s - loss: 0.0012 - acc: 0.9997
Epoch 6/40
 - 3s - loss: 0.0012 - acc: 0.9997
Epoch 7/40
 - 3s - loss: 0.0012 - acc: 0.9996
Epoch 8/40
 - 3s - loss: 0.0011 - acc: 0.9997
Epoch 9/40
 - 3s - loss: 0.0010 - acc: 0.9997
Epoch 10/40
 - 3s - loss: 0.0010 - acc: 0.9997
Epoch 11/40
 - 3s - loss: 8.6307e-04 - acc: 0.9998
Epoch 12/40
 - 3s - loss: 9.4053e-04 - acc: 0.9997
Epoch 13/40
 - 3s - loss: 9.5967e-04 - acc: 0.9997
Epoch 14/40
 - 3s - loss: 9.5325e-04 - acc: 0.9997
Epoch 15/40
 - 3s - loss: 9.8063e-04 - acc: 0.9997
Epoch 16/40
 - 3s - loss: 9.6889e-04 - acc: 0.9996
Epoch 17/40
 - 3s - loss: 9.6804e-04 - acc: 0.9996
Epoch 18/40
 - 3s - loss: 8.5831e-04 - acc: 0.9997
Epoch 19/40
 - 3s - loss: 7.6447e-04 - acc: 0.9997
Epoch 20/40
 - 3s - loss: 7.2758e-04 - acc: 0.9997
Epoch 21/40
 - 3s - loss: 9.8093e-04 - acc: 0.9996
Epoch 22/40
 - 3s - loss: 8.0823e-04 - acc: 0.9997
Epoch 23/40
 - 3s - loss: 8.6490e-04 - acc: 0.9997
Epoch 24/40
 - 3s - loss: 7.8201e-04 - acc: 0.9997
Epoch 25/40
 - 3s - loss: 7.0304e-04 - acc: 0.9997
Epoch 26/40
 - 3s - loss: 8.8738e-04 - acc: 0.9996
Epoch 27/40
 - 3s - loss: 8.0656e-04 - acc: 0.9997
Epoch 28/40
 - 3s - loss: 8.5961e-04 - acc: 0.9996
Epoch 29/40
 - 3s - loss: 8.5240e-04 - acc: 0.9996
Epoch 30/40
 - 3s - loss: 8.1683e-04 - acc: 0.9997
Epoch 31/40
 - 3s - loss: 7.6364e-04 - acc: 0.9997
Epoch 32/40
 - 3s - loss: 8.0852e-04 - acc: 0.9996
Epoch 33/40
 - 3s - loss: 7.1160e-04 - acc: 0.9997
Epoch 34/40
 - 3s - loss: 9.0912e-04 - acc: 0.9996
Epoch 35/40
 - 3s - loss: 7.1846e-04 - acc: 0.9997
Epoch 36/40
 - 3s - loss: 0.0010 - acc: 0.9995
Epoch 37/40
 - 3s - loss: 6.9473e-04 - acc: 0.9997
Epoch 38/40
 - 3s - loss: 6.3105e-04 - acc: 0.9997
Epoch 39/40
 - 3s - loss: 8.2980e-04 - acc: 0.9996
Epoch 40/40
 - 3s - loss: 8.3831e-04 - acc: 0.9996
# Training time = 0:19:13.639374
# F-Score(Ordinary) = 0.719, Recall: 0.762, Precision: 0.68
# F-Score(lvc) = 0.609, Recall: 0.761, Precision: 0.508
# F-Score(ireflv) = 0.779, Recall: 0.681, Precision: 0.91
# F-Score(id) = 0.733, Recall: 0.845, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_153 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_154 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_153 (Embedding)       (None, 4, 48)        705264      input_153[0][0]                  
__________________________________________________________________________________________________
embedding_154 (Embedding)       (None, 4, 24)        5640        input_154[0][0]                  
__________________________________________________________________________________________________
flatten_153 (Flatten)           (None, 192)          0           embedding_153[0][0]              
__________________________________________________________________________________________________
flatten_154 (Flatten)           (None, 96)           0           embedding_154[0][0]              
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 288)          0           flatten_153[0][0]                
                                                                 flatten_154[0][0]                
__________________________________________________________________________________________________
dense_153 (Dense)               (None, 24)           6936        concatenate_77[0][0]             
__________________________________________________________________________________________________
dropout_77 (Dropout)            (None, 24)           0           dense_153[0][0]                  
__________________________________________________________________________________________________
dense_154 (Dense)               (None, 8)            200         dropout_77[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.1252 - acc: 0.9735 - val_loss: 5.9336e-04 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0680 - acc: 0.9861 - val_loss: 1.8437e-04 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0613 - acc: 0.9876 - val_loss: 8.6609e-05 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0576 - acc: 0.9885 - val_loss: 4.3930e-05 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0553 - acc: 0.9889 - val_loss: 2.6942e-05 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0536 - acc: 0.9894 - val_loss: 1.7643e-05 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0524 - acc: 0.9896 - val_loss: 1.3054e-05 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0514 - acc: 0.9898 - val_loss: 9.9540e-06 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0506 - acc: 0.9900 - val_loss: 7.9871e-06 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0496 - acc: 0.9901 - val_loss: 6.0797e-06 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0491 - acc: 0.9902 - val_loss: 4.6492e-06 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0483 - acc: 0.9904 - val_loss: 3.7551e-06 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0479 - acc: 0.9905 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0475 - acc: 0.9906 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0472 - acc: 0.9907 - val_loss: 2.4438e-06 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0467 - acc: 0.9908 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0464 - acc: 0.9908 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0462 - acc: 0.9908 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0459 - acc: 0.9909 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0457 - acc: 0.9910 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0454 - acc: 0.9911 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0452 - acc: 0.9911 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0449 - acc: 0.9912 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0446 - acc: 0.9912 - val_loss: 8.9407e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0446 - acc: 0.9912 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0445 - acc: 0.9913 - val_loss: 7.7486e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0443 - acc: 0.9913 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0441 - acc: 0.9913 - val_loss: 6.5565e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0440 - acc: 0.9914 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0438 - acc: 0.9914 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0437 - acc: 0.9914 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0436 - acc: 0.9914 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0434 - acc: 0.9915 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0433 - acc: 0.9915 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 35/40
 - 26s - loss: 0.0434 - acc: 0.9915 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 36/40
 - 26s - loss: 0.0432 - acc: 0.9916 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 37/40
 - 26s - loss: 0.0430 - acc: 0.9916 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 38/40
 - 26s - loss: 0.0430 - acc: 0.9916 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 39/40
 - 26s - loss: 0.0429 - acc: 0.9917 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 40/40
 - 26s - loss: 0.0427 - acc: 0.9916 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0013 - acc: 0.9998
Epoch 2/40
 - 3s - loss: 9.8909e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 9.2248e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 8.3865e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 9.0762e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 7.4219e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 5.9633e-04 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 5.3421e-04 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 6.4458e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 5.4653e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 4.6797e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 5.3999e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 3.5257e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 4.3686e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 3.5807e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 3.8530e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 3.6785e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 4.3079e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 4.2134e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 3.6657e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 3.1915e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 3.1127e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 3.2297e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 2.4684e-04 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 3.5077e-04 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 3.3930e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 3.2302e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 2.5517e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 2.8005e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 2.4977e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 3.1679e-04 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 2.8285e-04 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 2.5799e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 2.5526e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.5246e-04 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 2.1431e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 2.5835e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.0732e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.8430e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 2.4737e-04 - acc: 1.0000
# Training time = 0:19:00.467258
# F-Score(Ordinary) = 0.708, Recall: 0.728, Precision: 0.689
# F-Score(lvc) = 0.524, Recall: 0.511, Precision: 0.538
# F-Score(ireflv) = 0.788, Recall: 0.711, Precision: 0.885
# F-Score(id) = 0.769, Recall: 0.947, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_155 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_156 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_155 (Embedding)       (None, 4, 48)        705264      input_155[0][0]                  
__________________________________________________________________________________________________
embedding_156 (Embedding)       (None, 4, 24)        5640        input_156[0][0]                  
__________________________________________________________________________________________________
flatten_155 (Flatten)           (None, 192)          0           embedding_155[0][0]              
__________________________________________________________________________________________________
flatten_156 (Flatten)           (None, 96)           0           embedding_156[0][0]              
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 288)          0           flatten_155[0][0]                
                                                                 flatten_156[0][0]                
__________________________________________________________________________________________________
dense_155 (Dense)               (None, 24)           6936        concatenate_78[0][0]             
__________________________________________________________________________________________________
dropout_78 (Dropout)            (None, 24)           0           dense_155[0][0]                  
__________________________________________________________________________________________________
dense_156 (Dense)               (None, 8)            200         dropout_78[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 26s - loss: 0.1298 - acc: 0.9728 - val_loss: 3.8649e-04 - val_acc: 1.0000
Epoch 2/40
 - 26s - loss: 0.0688 - acc: 0.9859 - val_loss: 1.1045e-04 - val_acc: 1.0000
Epoch 3/40
 - 26s - loss: 0.0620 - acc: 0.9875 - val_loss: 5.8355e-05 - val_acc: 1.0000
Epoch 4/40
 - 26s - loss: 0.0582 - acc: 0.9883 - val_loss: 4.1784e-05 - val_acc: 1.0000
Epoch 5/40
 - 26s - loss: 0.0559 - acc: 0.9888 - val_loss: 2.9326e-05 - val_acc: 1.0000
Epoch 6/40
 - 26s - loss: 0.0542 - acc: 0.9893 - val_loss: 2.3604e-05 - val_acc: 1.0000
Epoch 7/40
 - 26s - loss: 0.0526 - acc: 0.9895 - val_loss: 1.6570e-05 - val_acc: 1.0000
Epoch 8/40
 - 27s - loss: 0.0514 - acc: 0.9898 - val_loss: 1.4126e-05 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0507 - acc: 0.9900 - val_loss: 1.1504e-05 - val_acc: 1.0000
Epoch 10/40
 - 27s - loss: 0.0499 - acc: 0.9902 - val_loss: 1.1325e-05 - val_acc: 1.0000
Epoch 11/40
 - 26s - loss: 0.0491 - acc: 0.9903 - val_loss: 9.5368e-06 - val_acc: 1.0000
Epoch 12/40
 - 26s - loss: 0.0486 - acc: 0.9904 - val_loss: 8.8215e-06 - val_acc: 1.0000
Epoch 13/40
 - 27s - loss: 0.0482 - acc: 0.9905 - val_loss: 8.0467e-06 - val_acc: 1.0000
Epoch 14/40
 - 26s - loss: 0.0479 - acc: 0.9906 - val_loss: 7.8678e-06 - val_acc: 1.0000
Epoch 15/40
 - 26s - loss: 0.0474 - acc: 0.9907 - val_loss: 6.8546e-06 - val_acc: 1.0000
Epoch 16/40
 - 27s - loss: 0.0469 - acc: 0.9907 - val_loss: 6.5565e-06 - val_acc: 1.0000
Epoch 17/40
 - 26s - loss: 0.0465 - acc: 0.9909 - val_loss: 6.4373e-06 - val_acc: 1.0000
Epoch 18/40
 - 26s - loss: 0.0463 - acc: 0.9909 - val_loss: 6.3181e-06 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0460 - acc: 0.9910 - val_loss: 5.4240e-06 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0456 - acc: 0.9911 - val_loss: 5.1856e-06 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0455 - acc: 0.9911 - val_loss: 4.9472e-06 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0453 - acc: 0.9912 - val_loss: 4.6492e-06 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0451 - acc: 0.9912 - val_loss: 4.4108e-06 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0450 - acc: 0.9912 - val_loss: 4.0531e-06 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0447 - acc: 0.9912 - val_loss: 4.0531e-06 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0446 - acc: 0.9913 - val_loss: 3.6955e-06 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0443 - acc: 0.9913 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0442 - acc: 0.9914 - val_loss: 3.2187e-06 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0441 - acc: 0.9914 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0440 - acc: 0.9914 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0438 - acc: 0.9914 - val_loss: 2.9206e-06 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0435 - acc: 0.9915 - val_loss: 2.9206e-06 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0436 - acc: 0.9915 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0434 - acc: 0.9915 - val_loss: 3.0398e-06 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0434 - acc: 0.9915 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0434 - acc: 0.9915 - val_loss: 2.6226e-06 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0432 - acc: 0.9916 - val_loss: 2.5630e-06 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0431 - acc: 0.9915 - val_loss: 2.6226e-06 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0430 - acc: 0.9916 - val_loss: 2.5034e-06 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0429 - acc: 0.9916 - val_loss: 2.5034e-06 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0011 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 0.0011 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 8.2309e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 6.4742e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 7.0748e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 6.5965e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 6.2736e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 4.7529e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 3.9738e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 4.1585e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 5.2607e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 4.0500e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 3.8618e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 3.9788e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 3.6199e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 3.3242e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 2.9660e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 2.6902e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 3.0376e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 2.5972e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 1.8845e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 2.4539e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 3.2246e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 2.9383e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 2.1518e-04 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 2.3325e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 2.3683e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 2.1962e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 2.2397e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.6465e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 2.0543e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 2.4328e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.2248e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.8450e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 2.5038e-04 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 2.1190e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 1.9507e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.3433e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.6480e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.0851e-04 - acc: 1.0000
# Training time = 0:19:29.942987
# F-Score(Ordinary) = 0.71, Recall: 0.729, Precision: 0.691
# F-Score(lvc) = 0.545, Recall: 0.57, Precision: 0.523
# F-Score(ireflv) = 0.757, Recall: 0.644, Precision: 0.918
# F-Score(id) = 0.776, Recall: 0.969, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_157 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_158 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_157 (Embedding)       (None, 4, 48)        705264      input_157[0][0]                  
__________________________________________________________________________________________________
embedding_158 (Embedding)       (None, 4, 24)        5640        input_158[0][0]                  
__________________________________________________________________________________________________
flatten_157 (Flatten)           (None, 192)          0           embedding_157[0][0]              
__________________________________________________________________________________________________
flatten_158 (Flatten)           (None, 96)           0           embedding_158[0][0]              
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 288)          0           flatten_157[0][0]                
                                                                 flatten_158[0][0]                
__________________________________________________________________________________________________
dense_157 (Dense)               (None, 24)           6936        concatenate_79[0][0]             
__________________________________________________________________________________________________
dropout_79 (Dropout)            (None, 24)           0           dense_157[0][0]                  
__________________________________________________________________________________________________
dense_158 (Dense)               (None, 8)            200         dropout_79[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.1301 - acc: 0.9720 - val_loss: 1.7805e-04 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0678 - acc: 0.9858 - val_loss: 4.9473e-05 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0608 - acc: 0.9874 - val_loss: 2.6584e-05 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0571 - acc: 0.9883 - val_loss: 1.4901e-05 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0549 - acc: 0.9888 - val_loss: 9.7156e-06 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0532 - acc: 0.9892 - val_loss: 6.1989e-06 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0521 - acc: 0.9894 - val_loss: 4.3511e-06 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0509 - acc: 0.9898 - val_loss: 2.8014e-06 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0500 - acc: 0.9899 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0493 - acc: 0.9902 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0487 - acc: 0.9902 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0482 - acc: 0.9904 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0476 - acc: 0.9905 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0472 - acc: 0.9906 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0468 - acc: 0.9907 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0465 - acc: 0.9908 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0461 - acc: 0.9909 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0457 - acc: 0.9909 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0455 - acc: 0.9910 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0455 - acc: 0.9910 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0449 - acc: 0.9911 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0448 - acc: 0.9912 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0446 - acc: 0.9912 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0446 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0443 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0441 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0441 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0439 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0436 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0436 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0435 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0434 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0432 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0430 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0430 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0429 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0429 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0427 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0427 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0427 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 9.6038e-04 - acc: 0.9997
Epoch 2/40
 - 3s - loss: 0.0010 - acc: 0.9996
Epoch 3/40
 - 3s - loss: 8.6737e-04 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 7.4787e-04 - acc: 0.9997
Epoch 5/40
 - 3s - loss: 5.7458e-04 - acc: 0.9998
Epoch 6/40
 - 3s - loss: 6.5981e-04 - acc: 0.9998
Epoch 7/40
 - 3s - loss: 6.6076e-04 - acc: 0.9998
Epoch 8/40
 - 3s - loss: 5.0407e-04 - acc: 0.9998
Epoch 9/40
 - 3s - loss: 4.8514e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 4.5847e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 4.8666e-04 - acc: 0.9998
Epoch 12/40
 - 3s - loss: 3.2086e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 3.2640e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 3.6191e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 2.8940e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 3.0779e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 3.0467e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 3.3958e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 2.5968e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 2.6384e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 2.8388e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 2.4499e-04 - acc: 0.9999
Epoch 23/40
 - 3s - loss: 2.0788e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 1.7639e-04 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 2.6169e-04 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 2.3298e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 2.1989e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.6088e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 2.1859e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.6493e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.7533e-04 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.8573e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.0788e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.7393e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.3298e-04 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.6122e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 2.0349e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.5346e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.6072e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.4744e-04 - acc: 1.0000
# Training time = 0:18:37.920821
# F-Score(Ordinary) = 0.692, Recall: 0.665, Precision: 0.723
# F-Score(lvc) = 0.493, Recall: 0.436, Precision: 0.568
# F-Score(ireflv) = 0.747, Recall: 0.634, Precision: 0.91
# F-Score(id) = 0.765, Recall: 0.914, Precision: 0.658
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_159 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_160 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_159 (Embedding)       (None, 4, 48)        705264      input_159[0][0]                  
__________________________________________________________________________________________________
embedding_160 (Embedding)       (None, 4, 24)        5640        input_160[0][0]                  
__________________________________________________________________________________________________
flatten_159 (Flatten)           (None, 192)          0           embedding_159[0][0]              
__________________________________________________________________________________________________
flatten_160 (Flatten)           (None, 96)           0           embedding_160[0][0]              
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 288)          0           flatten_159[0][0]                
                                                                 flatten_160[0][0]                
__________________________________________________________________________________________________
dense_159 (Dense)               (None, 24)           6936        concatenate_80[0][0]             
__________________________________________________________________________________________________
dropout_80 (Dropout)            (None, 24)           0           dense_159[0][0]                  
__________________________________________________________________________________________________
dense_160 (Dense)               (None, 8)            200         dropout_80[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.005
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.1324 - acc: 0.9723 - val_loss: 0.0012 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0725 - acc: 0.9852 - val_loss: 4.4541e-04 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0648 - acc: 0.9870 - val_loss: 1.3060e-04 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0603 - acc: 0.9880 - val_loss: 7.0753e-05 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0575 - acc: 0.9886 - val_loss: 4.7387e-05 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0558 - acc: 0.9890 - val_loss: 3.2366e-05 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0541 - acc: 0.9894 - val_loss: 2.3723e-05 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0529 - acc: 0.9896 - val_loss: 1.5736e-05 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0518 - acc: 0.9899 - val_loss: 1.1444e-05 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0510 - acc: 0.9899 - val_loss: 8.0467e-06 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0503 - acc: 0.9902 - val_loss: 6.4373e-06 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0497 - acc: 0.9903 - val_loss: 4.4704e-06 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0491 - acc: 0.9903 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0487 - acc: 0.9906 - val_loss: 2.8610e-06 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0481 - acc: 0.9906 - val_loss: 2.2650e-06 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0478 - acc: 0.9907 - val_loss: 1.9670e-06 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0475 - acc: 0.9907 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0472 - acc: 0.9908 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0468 - acc: 0.9909 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0465 - acc: 0.9909 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0462 - acc: 0.9910 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0460 - acc: 0.9910 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0459 - acc: 0.9910 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0456 - acc: 0.9912 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0455 - acc: 0.9912 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0452 - acc: 0.9912 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0450 - acc: 0.9913 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0449 - acc: 0.9913 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0447 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0446 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0445 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0442 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0442 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0440 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0439 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0439 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0439 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0436 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0436 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0436 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0018 - acc: 0.9996
Epoch 2/40
 - 3s - loss: 0.0014 - acc: 0.9997
Epoch 3/40
 - 3s - loss: 0.0013 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 0.0013 - acc: 0.9997
Epoch 5/40
 - 3s - loss: 0.0011 - acc: 0.9996
Epoch 6/40
 - 3s - loss: 0.0011 - acc: 0.9997
Epoch 7/40
 - 3s - loss: 0.0010 - acc: 0.9997
Epoch 8/40
 - 3s - loss: 0.0011 - acc: 0.9996
Epoch 9/40
 - 3s - loss: 0.0010 - acc: 0.9996
Epoch 10/40
 - 3s - loss: 9.9395e-04 - acc: 0.9996
Epoch 11/40
 - 3s - loss: 8.1507e-04 - acc: 0.9997
Epoch 12/40
 - 3s - loss: 8.7183e-04 - acc: 0.9997
Epoch 13/40
 - 3s - loss: 6.6082e-04 - acc: 0.9998
Epoch 14/40
 - 3s - loss: 9.3587e-04 - acc: 0.9996
Epoch 15/40
 - 3s - loss: 8.5588e-04 - acc: 0.9996
Epoch 16/40
 - 3s - loss: 7.8327e-04 - acc: 0.9997
Epoch 17/40
 - 3s - loss: 6.6550e-04 - acc: 0.9997
Epoch 18/40
 - 3s - loss: 4.4683e-04 - acc: 0.9998
Epoch 19/40
 - 3s - loss: 6.6404e-04 - acc: 0.9997
Epoch 20/40
 - 3s - loss: 6.5570e-04 - acc: 0.9997
Epoch 21/40
 - 3s - loss: 6.3202e-04 - acc: 0.9997
Epoch 22/40
 - 3s - loss: 5.3159e-04 - acc: 0.9997
Epoch 23/40
 - 3s - loss: 5.8571e-04 - acc: 0.9997
Epoch 24/40
 - 3s - loss: 4.9358e-04 - acc: 0.9998
Epoch 25/40
 - 3s - loss: 6.4611e-04 - acc: 0.9997
Epoch 26/40
 - 3s - loss: 6.5362e-04 - acc: 0.9997
Epoch 27/40
 - 3s - loss: 6.0900e-04 - acc: 0.9996
Epoch 28/40
 - 3s - loss: 5.6482e-04 - acc: 0.9997
Epoch 29/40
 - 3s - loss: 4.9060e-04 - acc: 0.9997
Epoch 30/40
 - 3s - loss: 6.0481e-04 - acc: 0.9997
Epoch 31/40
 - 3s - loss: 5.9114e-04 - acc: 0.9996
Epoch 32/40
 - 3s - loss: 5.5502e-04 - acc: 0.9997
Epoch 33/40
 - 3s - loss: 5.2365e-04 - acc: 0.9997
Epoch 34/40
 - 3s - loss: 3.9207e-04 - acc: 0.9998
Epoch 35/40
 - 3s - loss: 4.0845e-04 - acc: 0.9997
Epoch 36/40
 - 3s - loss: 5.6687e-04 - acc: 0.9997
Epoch 37/40
 - 3s - loss: 5.6603e-04 - acc: 0.9996
Epoch 38/40
 - 3s - loss: 4.7303e-04 - acc: 0.9998
Epoch 39/40
 - 3s - loss: 3.9069e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 4.9219e-04 - acc: 0.9999
# Training time = 0:18:33.770736
# F-Score(Ordinary) = 0.71, Recall: 0.711, Precision: 0.709
# F-Score(lvc) = 0.512, Recall: 0.466, Precision: 0.568
# F-Score(ireflv) = 0.799, Recall: 0.722, Precision: 0.893
# F-Score(id) = 0.771, Recall: 0.94, Precision: 0.653
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_161 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_162 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_161 (Embedding)       (None, 4, 48)        705264      input_161[0][0]                  
__________________________________________________________________________________________________
embedding_162 (Embedding)       (None, 4, 24)        5640        input_162[0][0]                  
__________________________________________________________________________________________________
flatten_161 (Flatten)           (None, 192)          0           embedding_161[0][0]              
__________________________________________________________________________________________________
flatten_162 (Flatten)           (None, 96)           0           embedding_162[0][0]              
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 288)          0           flatten_161[0][0]                
                                                                 flatten_162[0][0]                
__________________________________________________________________________________________________
dense_161 (Dense)               (None, 24)           6936        concatenate_81[0][0]             
__________________________________________________________________________________________________
dropout_81 (Dropout)            (None, 24)           0           dense_161[0][0]                  
__________________________________________________________________________________________________
dense_162 (Dense)               (None, 8)            200         dropout_81[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0832 - acc: 0.9822 - val_loss: 3.2425e-05 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0542 - acc: 0.9890 - val_loss: 7.5102e-06 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0501 - acc: 0.9899 - val_loss: 2.0862e-06 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0479 - acc: 0.9904 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0466 - acc: 0.9907 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0458 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0450 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0443 - acc: 0.9912 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0437 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0433 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0432 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0429 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0425 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0422 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0421 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0420 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0416 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0416 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0413 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0414 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0411 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0409 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0410 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0409 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0408 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0408 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0406 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0405 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0404 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0405 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0404 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0403 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0402 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0402 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0400 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0399 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0398 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 8.0232e-04 - acc: 0.9998
Epoch 2/40
 - 3s - loss: 6.6956e-04 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 7.8736e-04 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 0.0010 - acc: 0.9996
Epoch 5/40
 - 3s - loss: 7.6019e-04 - acc: 0.9997
Epoch 6/40
 - 3s - loss: 8.2824e-04 - acc: 0.9997
Epoch 7/40
 - 3s - loss: 8.8364e-04 - acc: 0.9996
Epoch 8/40
 - 3s - loss: 7.6440e-04 - acc: 0.9997
Epoch 9/40
 - 3s - loss: 6.7628e-04 - acc: 0.9997
Epoch 10/40
 - 3s - loss: 7.1887e-04 - acc: 0.9997
Epoch 11/40
 - 3s - loss: 5.7675e-04 - acc: 0.9998
Epoch 12/40
 - 3s - loss: 6.7049e-04 - acc: 0.9997
Epoch 13/40
 - 3s - loss: 7.1204e-04 - acc: 0.9997
Epoch 14/40
 - 3s - loss: 7.0981e-04 - acc: 0.9997
Epoch 15/40
 - 3s - loss: 7.3160e-04 - acc: 0.9997
Epoch 16/40
 - 3s - loss: 7.4526e-04 - acc: 0.9996
Epoch 17/40
 - 3s - loss: 7.6140e-04 - acc: 0.9996
Epoch 18/40
 - 3s - loss: 6.5502e-04 - acc: 0.9997
Epoch 19/40
 - 3s - loss: 5.7259e-04 - acc: 0.9997
Epoch 20/40
 - 3s - loss: 5.4253e-04 - acc: 0.9997
Epoch 21/40
 - 3s - loss: 7.9472e-04 - acc: 0.9996
Epoch 22/40
 - 3s - loss: 6.3571e-04 - acc: 0.9997
Epoch 23/40
 - 3s - loss: 6.8561e-04 - acc: 0.9997
Epoch 24/40
 - 3s - loss: 6.0626e-04 - acc: 0.9997
Epoch 25/40
 - 3s - loss: 5.4800e-04 - acc: 0.9997
Epoch 26/40
 - 3s - loss: 7.2339e-04 - acc: 0.9996
Epoch 27/40
 - 3s - loss: 6.4539e-04 - acc: 0.9997
Epoch 28/40
 - 3s - loss: 6.9336e-04 - acc: 0.9996
Epoch 29/40
 - 3s - loss: 6.9003e-04 - acc: 0.9996
Epoch 30/40
 - 3s - loss: 6.6132e-04 - acc: 0.9997
Epoch 31/40
 - 3s - loss: 6.0872e-04 - acc: 0.9997
Epoch 32/40
 - 3s - loss: 6.7495e-04 - acc: 0.9996
Epoch 33/40
 - 3s - loss: 5.7567e-04 - acc: 0.9997
Epoch 34/40
 - 3s - loss: 7.6904e-04 - acc: 0.9996
Epoch 35/40
 - 3s - loss: 5.9288e-04 - acc: 0.9997
Epoch 36/40
 - 3s - loss: 8.8251e-04 - acc: 0.9995
Epoch 37/40
 - 3s - loss: 5.6529e-04 - acc: 0.9997
Epoch 38/40
 - 3s - loss: 5.1266e-04 - acc: 0.9997
Epoch 39/40
 - 3s - loss: 7.0404e-04 - acc: 0.9996
Epoch 40/40
 - 3s - loss: 7.0360e-04 - acc: 0.9996
# Training time = 0:18:40.157474
# F-Score(Ordinary) = 0.744, Recall: 0.846, Precision: 0.664
# F-Score(lvc) = 0.616, Recall: 0.823, Precision: 0.492
# F-Score(ireflv) = 0.814, Recall: 0.786, Precision: 0.844
# F-Score(id) = 0.76, Recall: 0.901, Precision: 0.658
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_163 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_164 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_163 (Embedding)       (None, 4, 48)        705264      input_163[0][0]                  
__________________________________________________________________________________________________
embedding_164 (Embedding)       (None, 4, 24)        5640        input_164[0][0]                  
__________________________________________________________________________________________________
flatten_163 (Flatten)           (None, 192)          0           embedding_163[0][0]              
__________________________________________________________________________________________________
flatten_164 (Flatten)           (None, 96)           0           embedding_164[0][0]              
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 288)          0           flatten_163[0][0]                
                                                                 flatten_164[0][0]                
__________________________________________________________________________________________________
dense_163 (Dense)               (None, 24)           6936        concatenate_82[0][0]             
__________________________________________________________________________________________________
dropout_82 (Dropout)            (None, 24)           0           dense_163[0][0]                  
__________________________________________________________________________________________________
dense_164 (Dense)               (None, 8)            200         dropout_82[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0838 - acc: 0.9824 - val_loss: 3.3796e-05 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0549 - acc: 0.9891 - val_loss: 6.7353e-06 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0509 - acc: 0.9898 - val_loss: 3.2783e-06 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0486 - acc: 0.9903 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0471 - acc: 0.9907 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0461 - acc: 0.9910 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0455 - acc: 0.9911 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0447 - acc: 0.9913 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0442 - acc: 0.9913 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0436 - acc: 0.9915 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0434 - acc: 0.9915 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0429 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0426 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0425 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0423 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0420 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0418 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0417 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0416 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0415 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0413 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0412 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0409 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0408 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0409 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0408 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0407 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0405 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0405 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0405 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0403 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0403 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0402 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0402 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0399 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0399 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0397 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 6.8444e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 4.1104e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 4.4499e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 4.0245e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 4.4278e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 3.5912e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 2.1701e-04 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 2.0254e-04 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 3.1353e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 2.3176e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 1.9384e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 2.7200e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 1.3876e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.9402e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.3912e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.8438e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.6247e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 2.1499e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.7449e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.9245e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.3894e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.3842e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.3235e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 9.4387e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.8196e-04 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.3672e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.5872e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.2903e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.1874e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.0668e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.8876e-04 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.3428e-04 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.3989e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.2418e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 4.5676e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1310e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.3301e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 9.3580e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 6.1743e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.1391e-04 - acc: 1.0000
# Training time = 0:18:32.660249
# F-Score(Ordinary) = 0.739, Recall: 0.832, Precision: 0.664
# F-Score(lvc) = 0.588, Recall: 0.73, Precision: 0.492
# F-Score(ireflv) = 0.813, Recall: 0.791, Precision: 0.836
# F-Score(id) = 0.777, Recall: 0.928, Precision: 0.668
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_165 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_166 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_165 (Embedding)       (None, 4, 48)        705264      input_165[0][0]                  
__________________________________________________________________________________________________
embedding_166 (Embedding)       (None, 4, 24)        5640        input_166[0][0]                  
__________________________________________________________________________________________________
flatten_165 (Flatten)           (None, 192)          0           embedding_165[0][0]              
__________________________________________________________________________________________________
flatten_166 (Flatten)           (None, 96)           0           embedding_166[0][0]              
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 288)          0           flatten_165[0][0]                
                                                                 flatten_166[0][0]                
__________________________________________________________________________________________________
dense_165 (Dense)               (None, 24)           6936        concatenate_83[0][0]             
__________________________________________________________________________________________________
dropout_83 (Dropout)            (None, 24)           0           dense_165[0][0]                  
__________________________________________________________________________________________________
dense_166 (Dense)               (None, 8)            200         dropout_83[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0862 - acc: 0.9818 - val_loss: 4.8281e-05 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0554 - acc: 0.9889 - val_loss: 2.0147e-05 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0509 - acc: 0.9899 - val_loss: 1.5140e-05 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0485 - acc: 0.9904 - val_loss: 1.3948e-05 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0471 - acc: 0.9907 - val_loss: 1.0371e-05 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0460 - acc: 0.9909 - val_loss: 9.7752e-06 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0450 - acc: 0.9911 - val_loss: 6.3181e-06 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0443 - acc: 0.9913 - val_loss: 5.6029e-06 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0440 - acc: 0.9914 - val_loss: 4.6492e-06 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0435 - acc: 0.9915 - val_loss: 5.0664e-06 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0430 - acc: 0.9916 - val_loss: 4.4108e-06 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0428 - acc: 0.9916 - val_loss: 4.5896e-06 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0426 - acc: 0.9916 - val_loss: 4.1723e-06 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0424 - acc: 0.9917 - val_loss: 4.2915e-06 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0421 - acc: 0.9917 - val_loss: 3.9339e-06 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0418 - acc: 0.9918 - val_loss: 3.5763e-06 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0416 - acc: 0.9918 - val_loss: 3.8743e-06 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0415 - acc: 0.9918 - val_loss: 3.8147e-06 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0414 - acc: 0.9919 - val_loss: 3.1591e-06 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0412 - acc: 0.9919 - val_loss: 3.1591e-06 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0411 - acc: 0.9919 - val_loss: 3.1591e-06 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0410 - acc: 0.9920 - val_loss: 2.8610e-06 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0410 - acc: 0.9920 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0408 - acc: 0.9920 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0408 - acc: 0.9920 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0407 - acc: 0.9920 - val_loss: 2.2650e-06 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0405 - acc: 0.9920 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0404 - acc: 0.9920 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0404 - acc: 0.9920 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0404 - acc: 0.9921 - val_loss: 2.2650e-06 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0402 - acc: 0.9920 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0400 - acc: 0.9921 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0401 - acc: 0.9921 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0400 - acc: 0.9922 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0400 - acc: 0.9921 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0398 - acc: 0.9921 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0399 - acc: 0.9921 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0398 - acc: 0.9921 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 5.8423e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 6.5326e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 4.2697e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 2.6398e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 4.3856e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 3.1110e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 4.4103e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 2.2174e-04 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 2.1620e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.4422e-04 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 2.9330e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 3.0174e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 2.3570e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 3.3628e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 2.5540e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 2.5804e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 1.4433e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.7175e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 1.7916e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 1.5104e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 1.0425e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.4878e-04 - acc: 0.9999
Epoch 23/40
 - 3s - loss: 2.7066e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 2.0899e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 1.2554e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 1.3908e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.4558e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 1.1758e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 9.8659e-05 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 7.4300e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.6479e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 1.4316e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 5.7568e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1432e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 1.8353e-04 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 1.7021e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 5.2259e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 5.0309e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.2914e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 5.4707e-05 - acc: 1.0000
# Training time = 0:18:43.916214
# F-Score(Ordinary) = 0.736, Recall: 0.811, Precision: 0.673
# F-Score(lvc) = 0.573, Recall: 0.753, Precision: 0.462
# F-Score(ireflv) = 0.8, Recall: 0.73, Precision: 0.885
# F-Score(id) = 0.764, Recall: 0.901, Precision: 0.663
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_167 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_168 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_167 (Embedding)       (None, 4, 48)        705264      input_167[0][0]                  
__________________________________________________________________________________________________
embedding_168 (Embedding)       (None, 4, 24)        5640        input_168[0][0]                  
__________________________________________________________________________________________________
flatten_167 (Flatten)           (None, 192)          0           embedding_167[0][0]              
__________________________________________________________________________________________________
flatten_168 (Flatten)           (None, 96)           0           embedding_168[0][0]              
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 288)          0           flatten_167[0][0]                
                                                                 flatten_168[0][0]                
__________________________________________________________________________________________________
dense_167 (Dense)               (None, 24)           6936        concatenate_84[0][0]             
__________________________________________________________________________________________________
dropout_84 (Dropout)            (None, 24)           0           dense_167[0][0]                  
__________________________________________________________________________________________________
dense_168 (Dense)               (None, 8)            200         dropout_84[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0853 - acc: 0.9817 - val_loss: 1.6630e-05 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0543 - acc: 0.9889 - val_loss: 3.3379e-06 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0499 - acc: 0.9899 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0477 - acc: 0.9904 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0463 - acc: 0.9907 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0453 - acc: 0.9910 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0446 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0440 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0434 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0430 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0427 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0425 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0420 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0418 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0417 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0415 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0413 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0411 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 26s - loss: 0.0410 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0410 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0407 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0407 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0405 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0406 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0405 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0404 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 26s - loss: 0.0404 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0403 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 27s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 27s - loss: 0.0401 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 27s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 27s - loss: 0.0399 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 27s - loss: 0.0397 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 27s - loss: 0.0398 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 27s - loss: 0.0397 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 27s - loss: 0.0398 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 27s - loss: 0.0397 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 27s - loss: 0.0397 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 27s - loss: 0.0397 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 5.0664e-04 - acc: 0.9998
Epoch 2/40
 - 3s - loss: 5.1657e-04 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 4.5686e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 3.9465e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 2.8598e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 3.0619e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 4.0296e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 2.0955e-04 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 2.1717e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.8975e-04 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 2.5565e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 1.0223e-04 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.3421e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.4681e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1744e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.4902e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.4481e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.4055e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.5227e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.0752e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.2340e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1818e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 6.8293e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 5.3599e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.2414e-04 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 7.7610e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.1511e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 6.4750e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.0355e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 5.5754e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 5.3107e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.1087e-04 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 3.9400e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 7.1058e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 4.4429e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 6.9213e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 7.7425e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 6.3752e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 5.1951e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 5.9217e-05 - acc: 1.0000
# Training time = 0:19:51.752878
# F-Score(Ordinary) = 0.697, Recall: 0.718, Precision: 0.678
# F-Score(lvc) = 0.556, Recall: 0.637, Precision: 0.492
# F-Score(ireflv) = 0.762, Recall: 0.673, Precision: 0.877
# F-Score(id) = 0.723, Recall: 0.795, Precision: 0.663
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_169 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_170 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_169 (Embedding)       (None, 4, 48)        705264      input_169[0][0]                  
__________________________________________________________________________________________________
embedding_170 (Embedding)       (None, 4, 24)        5640        input_170[0][0]                  
__________________________________________________________________________________________________
flatten_169 (Flatten)           (None, 192)          0           embedding_169[0][0]              
__________________________________________________________________________________________________
flatten_170 (Flatten)           (None, 96)           0           embedding_170[0][0]              
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 288)          0           flatten_169[0][0]                
                                                                 flatten_170[0][0]                
__________________________________________________________________________________________________
dense_169 (Dense)               (None, 24)           6936        concatenate_85[0][0]             
__________________________________________________________________________________________________
dropout_85 (Dropout)            (None, 24)           0           dense_169[0][0]                  
__________________________________________________________________________________________________
dense_170 (Dense)               (None, 8)            200         dropout_85[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.01
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0893 - acc: 0.9813 - val_loss: 2.3237e-04 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0577 - acc: 0.9886 - val_loss: 2.5571e-05 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0524 - acc: 0.9897 - val_loss: 1.0729e-05 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0498 - acc: 0.9902 - val_loss: 4.7088e-06 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0480 - acc: 0.9905 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0472 - acc: 0.9908 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0461 - acc: 0.9910 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0454 - acc: 0.9912 - val_loss: 5.3644e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0448 - acc: 0.9913 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0443 - acc: 0.9914 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0439 - acc: 0.9915 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0436 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0433 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0431 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0427 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0426 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0425 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0423 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0421 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0419 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0417 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0416 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0415 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0413 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 26s - loss: 0.0412 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 26s - loss: 0.0412 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 26s - loss: 0.0410 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 26s - loss: 0.0410 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 26s - loss: 0.0408 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 26s - loss: 0.0408 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 26s - loss: 0.0408 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 26s - loss: 0.0406 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0406 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0405 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0405 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0404 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0405 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0402 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0403 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0403 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 5.6582e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 4.7542e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 4.8023e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 3.5289e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 4.7218e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 2.3683e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 3.1004e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 3.8480e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 4.6377e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 2.8231e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 1.9623e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 3.2747e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 3.0263e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 3.3981e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 2.6564e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 2.6309e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 2.8789e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 1.2008e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.7187e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 3.4594e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 2.2752e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 2.0702e-04 - acc: 0.9999
Epoch 23/40
 - 3s - loss: 1.3007e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 2.0416e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 1.8933e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 1.6976e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 2.2264e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 9.3414e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.4515e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.1159e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 3.0260e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 1.5200e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.7249e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 1.8948e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 9.6125e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.5636e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 2.4895e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 1.1836e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.0068e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.2438e-04 - acc: 1.0000
# Training time = 0:19:28.752525
# F-Score(Ordinary) = 0.707, Recall: 0.713, Precision: 0.7
# F-Score(lvc) = 0.502, Recall: 0.454, Precision: 0.561
# F-Score(ireflv) = 0.814, Recall: 0.759, Precision: 0.877
# F-Score(id) = 0.762, Recall: 0.926, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_171 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_172 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_171 (Embedding)       (None, 4, 48)        705264      input_171[0][0]                  
__________________________________________________________________________________________________
embedding_172 (Embedding)       (None, 4, 24)        5640        input_172[0][0]                  
__________________________________________________________________________________________________
flatten_171 (Flatten)           (None, 192)          0           embedding_171[0][0]              
__________________________________________________________________________________________________
flatten_172 (Flatten)           (None, 96)           0           embedding_172[0][0]              
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 288)          0           flatten_171[0][0]                
                                                                 flatten_172[0][0]                
__________________________________________________________________________________________________
dense_171 (Dense)               (None, 24)           6936        concatenate_86[0][0]             
__________________________________________________________________________________________________
dropout_86 (Dropout)            (None, 24)           0           dense_171[0][0]                  
__________________________________________________________________________________________________
dense_172 (Dense)               (None, 8)            200         dropout_86[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0661 - acc: 0.9860 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0477 - acc: 0.9903 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0449 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0433 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0425 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0420 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0415 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0410 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0407 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0404 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0403 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0402 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0399 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0398 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0396 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0396 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0394 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0394 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0391 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0393 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0390 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0391 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0387 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 6.7710e-04 - acc: 0.9998
Epoch 2/40
 - 3s - loss: 5.5816e-04 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 7.1049e-04 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 9.8390e-04 - acc: 0.9996
Epoch 5/40
 - 3s - loss: 7.0219e-04 - acc: 0.9997
Epoch 6/40
 - 3s - loss: 7.7990e-04 - acc: 0.9997
Epoch 7/40
 - 3s - loss: 8.5254e-04 - acc: 0.9996
Epoch 8/40
 - 3s - loss: 7.2620e-04 - acc: 0.9997
Epoch 9/40
 - 3s - loss: 6.3383e-04 - acc: 0.9997
Epoch 10/40
 - 3s - loss: 6.8438e-04 - acc: 0.9997
Epoch 11/40
 - 3s - loss: 5.3590e-04 - acc: 0.9998
Epoch 12/40
 - 3s - loss: 6.4174e-04 - acc: 0.9997
Epoch 13/40
 - 3s - loss: 6.9046e-04 - acc: 0.9997
Epoch 14/40
 - 3s - loss: 6.8603e-04 - acc: 0.9997
Epoch 15/40
 - 3s - loss: 7.0807e-04 - acc: 0.9997
Epoch 16/40
 - 3s - loss: 7.2806e-04 - acc: 0.9996
Epoch 17/40
 - 3s - loss: 7.4832e-04 - acc: 0.9996
Epoch 18/40
 - 3s - loss: 6.3526e-04 - acc: 0.9997
Epoch 19/40
 - 3s - loss: 5.5059e-04 - acc: 0.9997
Epoch 20/40
 - 3s - loss: 5.2033e-04 - acc: 0.9997
Epoch 21/40
 - 3s - loss: 7.8151e-04 - acc: 0.9996
Epoch 22/40
 - 3s - loss: 6.1792e-04 - acc: 0.9997
Epoch 23/40
 - 3s - loss: 6.6650e-04 - acc: 0.9997
Epoch 24/40
 - 3s - loss: 5.8442e-04 - acc: 0.9997
Epoch 25/40
 - 3s - loss: 5.2811e-04 - acc: 0.9997
Epoch 26/40
 - 3s - loss: 7.0525e-04 - acc: 0.9996
Epoch 27/40
 - 3s - loss: 6.2433e-04 - acc: 0.9997
Epoch 28/40
 - 3s - loss: 6.7094e-04 - acc: 0.9996
Epoch 29/40
 - 3s - loss: 6.6667e-04 - acc: 0.9996
Epoch 30/40
 - 3s - loss: 6.3727e-04 - acc: 0.9997
Epoch 31/40
 - 3s - loss: 5.8331e-04 - acc: 0.9997
Epoch 32/40
 - 3s - loss: 6.5311e-04 - acc: 0.9996
Epoch 33/40
 - 3s - loss: 5.5098e-04 - acc: 0.9997
Epoch 34/40
 - 3s - loss: 7.4357e-04 - acc: 0.9996
Epoch 35/40
 - 3s - loss: 5.6791e-04 - acc: 0.9997
Epoch 36/40
 - 3s - loss: 8.5444e-04 - acc: 0.9995
Epoch 37/40
 - 3s - loss: 5.3666e-04 - acc: 0.9997
Epoch 38/40
 - 3s - loss: 4.8550e-04 - acc: 0.9997
Epoch 39/40
 - 3s - loss: 6.7340e-04 - acc: 0.9996
Epoch 40/40
 - 3s - loss: 6.6967e-04 - acc: 0.9996
# Training time = 0:18:48.057428
# F-Score(Ordinary) = 0.737, Recall: 0.86, Precision: 0.644
# F-Score(lvc) = 0.612, Recall: 0.831, Precision: 0.485
# F-Score(ireflv) = 0.824, Recall: 0.845, Precision: 0.803
# F-Score(id) = 0.746, Recall: 0.88, Precision: 0.648
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_173 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_174 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_173 (Embedding)       (None, 4, 48)        705264      input_173[0][0]                  
__________________________________________________________________________________________________
embedding_174 (Embedding)       (None, 4, 24)        5640        input_174[0][0]                  
__________________________________________________________________________________________________
flatten_173 (Flatten)           (None, 192)          0           embedding_173[0][0]              
__________________________________________________________________________________________________
flatten_174 (Flatten)           (None, 96)           0           embedding_174[0][0]              
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 288)          0           flatten_173[0][0]                
                                                                 flatten_174[0][0]                
__________________________________________________________________________________________________
dense_173 (Dense)               (None, 24)           6936        concatenate_87[0][0]             
__________________________________________________________________________________________________
dropout_87 (Dropout)            (None, 24)           0           dense_173[0][0]                  
__________________________________________________________________________________________________
dense_174 (Dense)               (None, 8)            200         dropout_87[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0674 - acc: 0.9860 - val_loss: 7.7486e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0484 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0453 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0437 - acc: 0.9913 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0426 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0420 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0416 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0411 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0408 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0404 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0402 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0398 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0397 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0396 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0395 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0393 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0393 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0392 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0391 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0389 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0389 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0388 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0387 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0388 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0387 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0386 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0383 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0383 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0382 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0381 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0381 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0381 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0379 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 4.2440e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 2.1395e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 2.5232e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 2.4742e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 2.7384e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 2.3502e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 9.0209e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 9.0848e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 2.0860e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 1.1540e-04 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 9.5851e-05 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.9193e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 6.8914e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.2814e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 6.4597e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.1360e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 9.7977e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.3856e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 7.7126e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.0808e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 6.4001e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 6.8358e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 5.9453e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 3.8489e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.0855e-04 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 5.8989e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 9.2828e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 8.3686e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 5.2858e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 4.7827e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.3243e-04 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 5.5973e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 8.0249e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 7.5913e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.6997e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 7.0522e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 7.5699e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 3.7235e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 2.0389e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 4.7543e-05 - acc: 1.0000
# Training time = 0:18:39.274481
# F-Score(Ordinary) = 0.745, Recall: 0.816, Precision: 0.685
# F-Score(lvc) = 0.617, Recall: 0.737, Precision: 0.53
# F-Score(ireflv) = 0.806, Recall: 0.765, Precision: 0.852
# F-Score(id) = 0.766, Recall: 0.896, Precision: 0.668
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_175 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_176 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_175 (Embedding)       (None, 4, 48)        705264      input_175[0][0]                  
__________________________________________________________________________________________________
embedding_176 (Embedding)       (None, 4, 24)        5640        input_176[0][0]                  
__________________________________________________________________________________________________
flatten_175 (Flatten)           (None, 192)          0           embedding_175[0][0]              
__________________________________________________________________________________________________
flatten_176 (Flatten)           (None, 96)           0           embedding_176[0][0]              
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 288)          0           flatten_175[0][0]                
                                                                 flatten_176[0][0]                
__________________________________________________________________________________________________
dense_175 (Dense)               (None, 24)           6936        concatenate_88[0][0]             
__________________________________________________________________________________________________
dropout_88 (Dropout)            (None, 24)           0           dense_175[0][0]                  
__________________________________________________________________________________________________
dense_176 (Dense)               (None, 8)            200         dropout_88[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0671 - acc: 0.9861 - val_loss: 2.8313e-05 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0479 - acc: 0.9905 - val_loss: 1.3352e-05 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0449 - acc: 0.9911 - val_loss: 1.3113e-05 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0432 - acc: 0.9915 - val_loss: 1.1683e-05 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0424 - acc: 0.9916 - val_loss: 7.7486e-06 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0418 - acc: 0.9918 - val_loss: 6.5565e-06 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0412 - acc: 0.9919 - val_loss: 4.0531e-06 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0408 - acc: 0.9919 - val_loss: 3.9339e-06 - val_acc: 1.0000
Epoch 9/40
 - 27s - loss: 0.0406 - acc: 0.9920 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 10/40
 - 26s - loss: 0.0403 - acc: 0.9920 - val_loss: 3.5763e-06 - val_acc: 1.0000
Epoch 11/40
 - 26s - loss: 0.0401 - acc: 0.9921 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 12/40
 - 26s - loss: 0.0400 - acc: 0.9921 - val_loss: 3.6955e-06 - val_acc: 1.0000
Epoch 13/40
 - 26s - loss: 0.0398 - acc: 0.9921 - val_loss: 3.3975e-06 - val_acc: 1.0000
Epoch 14/40
 - 26s - loss: 0.0397 - acc: 0.9922 - val_loss: 3.3379e-06 - val_acc: 1.0000
Epoch 15/40
 - 26s - loss: 0.0395 - acc: 0.9922 - val_loss: 3.5763e-06 - val_acc: 1.0000
Epoch 16/40
 - 26s - loss: 0.0393 - acc: 0.9922 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 17/40
 - 26s - loss: 0.0392 - acc: 0.9922 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 18/40
 - 26s - loss: 0.0391 - acc: 0.9922 - val_loss: 3.6359e-06 - val_acc: 1.0000
Epoch 19/40
 - 26s - loss: 0.0391 - acc: 0.9923 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 20/40
 - 26s - loss: 0.0389 - acc: 0.9923 - val_loss: 2.8014e-06 - val_acc: 1.0000
Epoch 21/40
 - 26s - loss: 0.0389 - acc: 0.9923 - val_loss: 2.8610e-06 - val_acc: 1.0000
Epoch 22/40
 - 26s - loss: 0.0388 - acc: 0.9923 - val_loss: 2.5034e-06 - val_acc: 1.0000
Epoch 23/40
 - 26s - loss: 0.0389 - acc: 0.9923 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 24/40
 - 26s - loss: 0.0387 - acc: 0.9923 - val_loss: 2.6226e-06 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0387 - acc: 0.9923 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0386 - acc: 0.9924 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0385 - acc: 0.9923 - val_loss: 1.9670e-06 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.8477e-06 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0384 - acc: 0.9924 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0384 - acc: 0.9924 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.5497e-06 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0381 - acc: 0.9925 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.9670e-06 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.9670e-06 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0380 - acc: 0.9925 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0380 - acc: 0.9924 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 2.4633e-04 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 3.1981e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 1.7681e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 1.0629e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.5776e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 1.1887e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.5771e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 7.3585e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 8.3145e-05 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 3.5530e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 7.5243e-05 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 8.3162e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 6.5621e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 9.0255e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 5.9928e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 5.5171e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 5.0439e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 4.7677e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 3.7908e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 4.2317e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 3.3744e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 4.0244e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 7.9063e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 4.8941e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 4.3196e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 4.0241e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 3.5314e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 2.9239e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.6594e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 2.2271e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 4.3429e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 5.7145e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.5965e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 4.4462e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 8.5804e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 3.7307e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 7.8455e-06 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.7316e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 2.5519e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 7.3556e-06 - acc: 1.0000
# Training time = 0:19:23.322421
# F-Score(Ordinary) = 0.721, Recall: 0.779, Precision: 0.671
# F-Score(lvc) = 0.618, Recall: 0.853, Precision: 0.485
# F-Score(ireflv) = 0.811, Recall: 0.795, Precision: 0.828
# F-Score(id) = 0.691, Recall: 0.71, Precision: 0.674
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_177 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_178 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_177 (Embedding)       (None, 4, 48)        705264      input_177[0][0]                  
__________________________________________________________________________________________________
embedding_178 (Embedding)       (None, 4, 24)        5640        input_178[0][0]                  
__________________________________________________________________________________________________
flatten_177 (Flatten)           (None, 192)          0           embedding_177[0][0]              
__________________________________________________________________________________________________
flatten_178 (Flatten)           (None, 96)           0           embedding_178[0][0]              
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 288)          0           flatten_177[0][0]                
                                                                 flatten_178[0][0]                
__________________________________________________________________________________________________
dense_177 (Dense)               (None, 24)           6936        concatenate_89[0][0]             
__________________________________________________________________________________________________
dropout_89 (Dropout)            (None, 24)           0           dense_177[0][0]                  
__________________________________________________________________________________________________
dense_178 (Dense)               (None, 8)            200         dropout_89[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0671 - acc: 0.9859 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0478 - acc: 0.9904 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0448 - acc: 0.9911 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0434 - acc: 0.9914 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0425 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0418 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0414 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0410 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0406 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0404 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0402 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0400 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0398 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0396 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0396 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0395 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0394 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0391 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0391 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0391 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0389 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0388 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0388 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0388 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0387 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0386 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 3.8361e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 3.3952e-04 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 3.1444e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 1.6876e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 1.1305e-04 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.3726e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 3.0790e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 1.0164e-04 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 6.6609e-05 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 6.0395e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.0362e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 5.5815e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.2120e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.3105e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 4.0061e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 8.9779e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 8.4339e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 7.3696e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 9.6148e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 3.7001e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 9.8153e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 6.2627e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 2.1768e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.9883e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 8.6438e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 3.2250e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 6.7916e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.9134e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 2.6443e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 9.1596e-06 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.0475e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.0692e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.0148e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.7592e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.3169e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 3.7875e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.8845e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 7.9363e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 2.5152e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 7.5743e-05 - acc: 1.0000
# Training time = 0:18:40.604105
# F-Score(Ordinary) = 0.714, Recall: 0.725, Precision: 0.702
# F-Score(lvc) = 0.601, Recall: 0.79, Precision: 0.485
# F-Score(ireflv) = 0.787, Recall: 0.724, Precision: 0.861
# F-Score(id) = 0.685, Recall: 0.662, Precision: 0.71
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_179 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_180 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_179 (Embedding)       (None, 4, 48)        705264      input_179[0][0]                  
__________________________________________________________________________________________________
embedding_180 (Embedding)       (None, 4, 24)        5640        input_180[0][0]                  
__________________________________________________________________________________________________
flatten_179 (Flatten)           (None, 192)          0           embedding_179[0][0]              
__________________________________________________________________________________________________
flatten_180 (Flatten)           (None, 96)           0           embedding_180[0][0]              
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 288)          0           flatten_179[0][0]                
                                                                 flatten_180[0][0]                
__________________________________________________________________________________________________
dense_179 (Dense)               (None, 24)           6936        concatenate_90[0][0]             
__________________________________________________________________________________________________
dropout_90 (Dropout)            (None, 24)           0           dense_179[0][0]                  
__________________________________________________________________________________________________
dense_180 (Dense)               (None, 8)            200         dropout_90[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.02
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0693 - acc: 0.9855 - val_loss: 1.4782e-05 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0487 - acc: 0.9902 - val_loss: 6.9142e-06 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0456 - acc: 0.9910 - val_loss: 5.3644e-06 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0441 - acc: 0.9913 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0429 - acc: 0.9915 - val_loss: 2.6822e-06 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0425 - acc: 0.9917 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0419 - acc: 0.9918 - val_loss: 2.0862e-06 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0415 - acc: 0.9919 - val_loss: 1.2517e-06 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0412 - acc: 0.9920 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0408 - acc: 0.9920 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0407 - acc: 0.9920 - val_loss: 5.3644e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0404 - acc: 0.9920 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0403 - acc: 0.9921 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0402 - acc: 0.9921 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0400 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0399 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0398 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0398 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0396 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0396 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0394 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0394 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0394 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0392 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0392 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0392 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0390 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0390 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0389 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0389 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0389 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0387 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0387 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0386 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0386 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0385 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 6.2363e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 4.6226e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 3.6577e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 3.7106e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 2.6353e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 2.5732e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 2.4065e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 2.7161e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 3.2409e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 2.0819e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 1.9952e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 2.3316e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 1.5385e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 2.3505e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 2.5285e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 3.5080e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 1.4477e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 9.9343e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.4208e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 2.0385e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 1.3373e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 9.7185e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.6580e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 6.4833e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.7070e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 2.9814e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 1.5943e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 1.5918e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 1.8711e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 2.8763e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 1.2645e-04 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.8977e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.8790e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 8.7563e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.1666e-04 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 2.5107e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 2.1879e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 1.4880e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 1.7950e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 2.4505e-04 - acc: 0.9999
# Training time = 0:18:42.609769
# F-Score(Ordinary) = 0.739, Recall: 0.858, Precision: 0.649
# F-Score(lvc) = 0.615, Recall: 0.779, Precision: 0.508
# F-Score(ireflv) = 0.812, Recall: 0.829, Precision: 0.795
# F-Score(id) = 0.756, Recall: 0.919, Precision: 0.642
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_181 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_182 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_181 (Embedding)       (None, 4, 48)        705264      input_181[0][0]                  
__________________________________________________________________________________________________
embedding_182 (Embedding)       (None, 4, 24)        5640        input_182[0][0]                  
__________________________________________________________________________________________________
flatten_181 (Flatten)           (None, 192)          0           embedding_181[0][0]              
__________________________________________________________________________________________________
flatten_182 (Flatten)           (None, 96)           0           embedding_182[0][0]              
__________________________________________________________________________________________________
concatenate_91 (Concatenate)    (None, 288)          0           flatten_181[0][0]                
                                                                 flatten_182[0][0]                
__________________________________________________________________________________________________
dense_181 (Dense)               (None, 24)           6936        concatenate_91[0][0]             
__________________________________________________________________________________________________
dropout_91 (Dropout)            (None, 24)           0           dense_181[0][0]                  
__________________________________________________________________________________________________
dense_182 (Dense)               (None, 8)            200         dropout_91[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0576 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0439 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0417 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0406 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0398 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0395 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0392 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0381 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0368 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0368 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0368 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 26s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 6.4834e-04 - acc: 0.9998
Epoch 2/40
 - 3s - loss: 5.3247e-04 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 7.1009e-04 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 0.0010 - acc: 0.9996
Epoch 5/40
 - 3s - loss: 7.0702e-04 - acc: 0.9997
Epoch 6/40
 - 3s - loss: 7.8745e-04 - acc: 0.9997
Epoch 7/40
 - 3s - loss: 8.6293e-04 - acc: 0.9996
Epoch 8/40
 - 3s - loss: 7.2721e-04 - acc: 0.9997
Epoch 9/40
 - 3s - loss: 6.2724e-04 - acc: 0.9997
Epoch 10/40
 - 3s - loss: 6.7609e-04 - acc: 0.9997
Epoch 11/40
 - 3s - loss: 5.2266e-04 - acc: 0.9998
Epoch 12/40
 - 3s - loss: 6.2868e-04 - acc: 0.9997
Epoch 13/40
 - 3s - loss: 6.7476e-04 - acc: 0.9997
Epoch 14/40
 - 3s - loss: 6.6450e-04 - acc: 0.9997
Epoch 15/40
 - 3s - loss: 6.8110e-04 - acc: 0.9997
Epoch 16/40
 - 3s - loss: 6.9639e-04 - acc: 0.9996
Epoch 17/40
 - 3s - loss: 7.1071e-04 - acc: 0.9996
Epoch 18/40
 - 3s - loss: 5.9633e-04 - acc: 0.9997
Epoch 19/40
 - 3s - loss: 5.1188e-04 - acc: 0.9997
Epoch 20/40
 - 3s - loss: 4.8034e-04 - acc: 0.9997
Epoch 21/40
 - 3s - loss: 7.2098e-04 - acc: 0.9996
Epoch 22/40
 - 3s - loss: 5.6313e-04 - acc: 0.9997
Epoch 23/40
 - 3s - loss: 6.0293e-04 - acc: 0.9997
Epoch 24/40
 - 3s - loss: 5.2324e-04 - acc: 0.9997
Epoch 25/40
 - 3s - loss: 4.6952e-04 - acc: 0.9997
Epoch 26/40
 - 3s - loss: 6.2425e-04 - acc: 0.9996
Epoch 27/40
 - 3s - loss: 5.4660e-04 - acc: 0.9997
Epoch 28/40
 - 3s - loss: 5.8308e-04 - acc: 0.9996
Epoch 29/40
 - 3s - loss: 5.7402e-04 - acc: 0.9996
Epoch 30/40
 - 3s - loss: 5.4363e-04 - acc: 0.9997
Epoch 31/40
 - 3s - loss: 4.9304e-04 - acc: 0.9997
Epoch 32/40
 - 3s - loss: 5.4893e-04 - acc: 0.9996
Epoch 33/40
 - 3s - loss: 4.5815e-04 - acc: 0.9997
Epoch 34/40
 - 3s - loss: 6.1477e-04 - acc: 0.9996
Epoch 35/40
 - 3s - loss: 4.6425e-04 - acc: 0.9997
Epoch 36/40
 - 3s - loss: 6.9398e-04 - acc: 0.9995
Epoch 37/40
 - 3s - loss: 4.2952e-04 - acc: 0.9997
Epoch 38/40
 - 3s - loss: 3.8576e-04 - acc: 0.9998
Epoch 39/40
 - 3s - loss: 5.3222e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 5.2380e-04 - acc: 1.0000
# Training time = 0:18:53.670000
# F-Score(Ordinary) = 0.737, Recall: 0.831, Precision: 0.662
# F-Score(lvc) = 0.608, Recall: 0.776, Precision: 0.5
# F-Score(ireflv) = 0.816, Recall: 0.797, Precision: 0.836
# F-Score(id) = 0.756, Recall: 0.888, Precision: 0.658
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_183 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_184 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_183 (Embedding)       (None, 4, 48)        705264      input_183[0][0]                  
__________________________________________________________________________________________________
embedding_184 (Embedding)       (None, 4, 24)        5640        input_184[0][0]                  
__________________________________________________________________________________________________
flatten_183 (Flatten)           (None, 192)          0           embedding_183[0][0]              
__________________________________________________________________________________________________
flatten_184 (Flatten)           (None, 96)           0           embedding_184[0][0]              
__________________________________________________________________________________________________
concatenate_92 (Concatenate)    (None, 288)          0           flatten_183[0][0]                
                                                                 flatten_184[0][0]                
__________________________________________________________________________________________________
dense_183 (Dense)               (None, 24)           6936        concatenate_92[0][0]             
__________________________________________________________________________________________________
dropout_92 (Dropout)            (None, 24)           0           dense_183[0][0]                  
__________________________________________________________________________________________________
dense_184 (Dense)               (None, 8)            200         dropout_92[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0580 - acc: 0.9877 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0439 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0419 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0409 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0401 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0396 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0394 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0391 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0389 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0387 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0384 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0378 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0378 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0376 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0375 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0375 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0374 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0373 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0372 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0371 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0368 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0368 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0368 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0368 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 26s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 26s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 26s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.7386e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 1.2584e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 6.6058e-05 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 1.1213e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 2.4743e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 3.9842e-05 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 4.6611e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 2.0709e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 2.6070e-05 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 6.4224e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.8777e-05 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 6.5016e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 9.2833e-06 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.0935e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 8.4322e-06 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.4552e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.2335e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.0445e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 7.4250e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 4.5693e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 8.3355e-06 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 3.5847e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.2196e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 9.7335e-06 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 7.1144e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 3.3350e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 9.1243e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 4.8242e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 6.3631e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 4.7292e-06 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 4.2931e-06 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 6.4237e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 6.9020e-06 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 5.7716e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 7.0141e-06 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 4.7880e-06 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 7.1217e-06 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.4303e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 3.6371e-06 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 2.4310e-05 - acc: 1.0000
# Training time = 0:19:08.430774
# F-Score(Ordinary) = 0.73, Recall: 0.795, Precision: 0.676
# F-Score(lvc) = 0.569, Recall: 0.636, Precision: 0.515
# F-Score(ireflv) = 0.8, Recall: 0.781, Precision: 0.82
# F-Score(id) = 0.769, Recall: 0.897, Precision: 0.674
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_185 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_186 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_185 (Embedding)       (None, 4, 48)        705264      input_185[0][0]                  
__________________________________________________________________________________________________
embedding_186 (Embedding)       (None, 4, 24)        5640        input_186[0][0]                  
__________________________________________________________________________________________________
flatten_185 (Flatten)           (None, 192)          0           embedding_185[0][0]              
__________________________________________________________________________________________________
flatten_186 (Flatten)           (None, 96)           0           embedding_186[0][0]              
__________________________________________________________________________________________________
concatenate_93 (Concatenate)    (None, 288)          0           flatten_185[0][0]                
                                                                 flatten_186[0][0]                
__________________________________________________________________________________________________
dense_185 (Dense)               (None, 24)           6936        concatenate_93[0][0]             
__________________________________________________________________________________________________
dropout_93 (Dropout)            (None, 24)           0           dense_185[0][0]                  
__________________________________________________________________________________________________
dense_186 (Dense)               (None, 8)            200         dropout_93[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0577 - acc: 0.9876 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0439 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0418 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0406 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0401 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0393 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0387 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 26s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 26s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 26s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 26s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 26s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 26s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 26s - loss: 0.0377 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 26s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 26s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 26s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0368 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0367 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 2.1803e-04 - acc: 1.0000
Epoch 2/40
 - 3s - loss: 9.7360e-05 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 1.5110e-04 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 3.3547e-05 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 6.1079e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 1.1625e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 7.7105e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 3.9073e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 7.7536e-05 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 8.6189e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 7.2059e-05 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 6.2543e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 6.1609e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 2.7300e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 6.6340e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.2921e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 6.0539e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.5209e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.2743e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 9.7838e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.2901e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.2557e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 5.1090e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.3738e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 5.3062e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 5.0187e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 4.9949e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 9.1786e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.3107e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.2902e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 4.7827e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 9.4559e-06 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 4.9410e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 6.9036e-06 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 5.7290e-06 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 8.6136e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 6.3187e-06 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 5.5569e-06 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 6.1162e-06 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 8.4781e-05 - acc: 1.0000
# Training time = 0:18:58.468440
# F-Score(Ordinary) = 0.683, Recall: 0.67, Precision: 0.696
# F-Score(lvc) = 0.496, Recall: 0.479, Precision: 0.515
# F-Score(ireflv) = 0.782, Recall: 0.734, Precision: 0.836
# F-Score(id) = 0.691, Recall: 0.71, Precision: 0.674
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_187 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_188 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_187 (Embedding)       (None, 4, 48)        705264      input_187[0][0]                  
__________________________________________________________________________________________________
embedding_188 (Embedding)       (None, 4, 24)        5640        input_188[0][0]                  
__________________________________________________________________________________________________
flatten_187 (Flatten)           (None, 192)          0           embedding_187[0][0]              
__________________________________________________________________________________________________
flatten_188 (Flatten)           (None, 96)           0           embedding_188[0][0]              
__________________________________________________________________________________________________
concatenate_94 (Concatenate)    (None, 288)          0           flatten_187[0][0]                
                                                                 flatten_188[0][0]                
__________________________________________________________________________________________________
dense_187 (Dense)               (None, 24)           6936        concatenate_94[0][0]             
__________________________________________________________________________________________________
dropout_94 (Dropout)            (None, 24)           0           dense_187[0][0]                  
__________________________________________________________________________________________________
dense_188 (Dense)               (None, 8)            200         dropout_94[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0587 - acc: 0.9875 - val_loss: 6.5565e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0444 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0420 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0409 - acc: 0.9917 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0402 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0396 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0393 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0390 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0384 - acc: 0.9922 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0382 - acc: 0.9922 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0381 - acc: 0.9923 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0380 - acc: 0.9923 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0378 - acc: 0.9924 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0378 - acc: 0.9924 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0375 - acc: 0.9925 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0374 - acc: 0.9925 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0373 - acc: 0.9924 - val_loss: 5.3644e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0374 - acc: 0.9925 - val_loss: 4.1723e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0371 - acc: 0.9926 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0371 - acc: 0.9926 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0370 - acc: 0.9926 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0370 - acc: 0.9926 - val_loss: 9.5367e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0369 - acc: 0.9927 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.9670e-06 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0368 - acc: 0.9927 - val_loss: 3.5763e-06 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0368 - acc: 0.9927 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0366 - acc: 0.9928 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0366 - acc: 0.9928 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0366 - acc: 0.9928 - val_loss: 3.6359e-06 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0365 - acc: 0.9928 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0365 - acc: 0.9928 - val_loss: 2.9206e-06 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0364 - acc: 0.9928 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.5477e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 1.6287e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 7.1925e-05 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 5.2944e-05 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 5.4181e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 8.3634e-05 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 5.8347e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.8536e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.3698e-05 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 4.6064e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 6.1609e-05 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 2.1042e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 2.4611e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.3479e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 8.2447e-06 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.7896e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.1212e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.3729e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.8545e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.9620e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 2.2871e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.2208e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.2414e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 3.5734e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 6.0209e-06 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 4.6565e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 2.3770e-06 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 9.2597e-06 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 2.0860e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.5663e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 9.2291e-06 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 2.0200e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 4.2813e-06 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 5.5816e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 5.8905e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 6.5278e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 4.8613e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.1746e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 6.9335e-06 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 2.6620e-05 - acc: 1.0000
# Training time = 0:18:36.246318
# F-Score(Ordinary) = 0.723, Recall: 0.753, Precision: 0.696
# F-Score(lvc) = 0.612, Recall: 0.851, Precision: 0.477
# F-Score(ireflv) = 0.816, Recall: 0.782, Precision: 0.852
# F-Score(id) = 0.662, Recall: 0.641, Precision: 0.684
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_189 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_190 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_189 (Embedding)       (None, 4, 48)        705264      input_189[0][0]                  
__________________________________________________________________________________________________
embedding_190 (Embedding)       (None, 4, 24)        5640        input_190[0][0]                  
__________________________________________________________________________________________________
flatten_189 (Flatten)           (None, 192)          0           embedding_189[0][0]              
__________________________________________________________________________________________________
flatten_190 (Flatten)           (None, 96)           0           embedding_190[0][0]              
__________________________________________________________________________________________________
concatenate_95 (Concatenate)    (None, 288)          0           flatten_189[0][0]                
                                                                 flatten_190[0][0]                
__________________________________________________________________________________________________
dense_189 (Dense)               (None, 24)           6936        concatenate_95[0][0]             
__________________________________________________________________________________________________
dropout_95 (Dropout)            (None, 24)           0           dense_189[0][0]                  
__________________________________________________________________________________________________
dense_190 (Dense)               (None, 8)            200         dropout_95[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.05
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0588 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0445 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0424 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0413 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0404 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0402 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0396 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0394 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0391 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0388 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0387 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0384 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0383 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0382 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0380 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0379 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0378 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0377 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0375 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0374 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0371 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 3.9079e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 1.7295e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 2.0120e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 3.4902e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 1.7211e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 2.5212e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 9.9357e-05 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 4.9085e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.7542e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 9.1071e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 4.2324e-05 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 7.2863e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 8.1616e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 4.1346e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1811e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.8026e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 2.3782e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 7.2643e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 1.6097e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.6555e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 5.8170e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 2.3412e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 4.7756e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 6.5079e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.8063e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 5.3205e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 4.9140e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.0250e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 5.9718e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.0884e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.0435e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 9.9003e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 9.6852e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 8.7408e-06 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.3622e-04 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 4.9741e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.1180e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 5.3933e-06 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 5.4301e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 9.2360e-05 - acc: 1.0000
# Training time = 0:18:37.716202
# F-Score(Ordinary) = 0.723, Recall: 0.768, Precision: 0.682
# F-Score(lvc) = 0.585, Recall: 0.775, Precision: 0.47
# F-Score(ireflv) = 0.843, Recall: 0.85, Precision: 0.836
# F-Score(id) = 0.687, Recall: 0.68, Precision: 0.694
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_191 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_192 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_191 (Embedding)       (None, 4, 48)        705264      input_191[0][0]                  
__________________________________________________________________________________________________
embedding_192 (Embedding)       (None, 4, 24)        5640        input_192[0][0]                  
__________________________________________________________________________________________________
flatten_191 (Flatten)           (None, 192)          0           embedding_191[0][0]              
__________________________________________________________________________________________________
flatten_192 (Flatten)           (None, 96)           0           embedding_192[0][0]              
__________________________________________________________________________________________________
concatenate_96 (Concatenate)    (None, 288)          0           flatten_191[0][0]                
                                                                 flatten_192[0][0]                
__________________________________________________________________________________________________
dense_191 (Dense)               (None, 24)           6936        concatenate_96[0][0]             
__________________________________________________________________________________________________
dropout_96 (Dropout)            (None, 24)           0           dense_191[0][0]                  
__________________________________________________________________________________________________
dense_192 (Dense)               (None, 8)            200         dropout_96[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0567 - acc: 0.9876 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0427 - acc: 0.9911 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0407 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0396 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0385 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0382 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0363 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0361 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0361 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0362 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0361 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0360 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0361 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0360 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0360 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0359 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0360 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0359 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 1.4675e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 1.2867e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 1.1908e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 1.4012e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 1.6107e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 1.3410e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 1.2981e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 5.6175e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 5.3961e-05 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 2.9940e-05 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.0070e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 9.9505e-05 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 1.2190e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 5.0770e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.1981e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 1.4178e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 9.3798e-05 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 1.3812e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 4.7213e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 9.1363e-05 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 1.5606e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 1.3222e-04 - acc: 0.9999
Epoch 23/40
 - 3s - loss: 1.3092e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 1.0763e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 1.4832e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 1.0512e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 1.0383e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 1.6342e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 8.1252e-05 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 1.2028e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 9.9175e-05 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 7.8796e-05 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 2.0508e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 9.7040e-05 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 9.6027e-05 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 9.5362e-05 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 1.1264e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 5.6290e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 5.5883e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.6421e-04 - acc: 0.9999
# Training time = 0:18:26.343102
# F-Score(Ordinary) = 0.723, Recall: 0.75, Precision: 0.698
# F-Score(lvc) = 0.575, Recall: 0.617, Precision: 0.538
# F-Score(ireflv) = 0.846, Recall: 0.839, Precision: 0.852
# F-Score(id) = 0.703, Recall: 0.734, Precision: 0.674
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_193 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_194 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_193 (Embedding)       (None, 4, 48)        705264      input_193[0][0]                  
__________________________________________________________________________________________________
embedding_194 (Embedding)       (None, 4, 24)        5640        input_194[0][0]                  
__________________________________________________________________________________________________
flatten_193 (Flatten)           (None, 192)          0           embedding_193[0][0]              
__________________________________________________________________________________________________
flatten_194 (Flatten)           (None, 96)           0           embedding_194[0][0]              
__________________________________________________________________________________________________
concatenate_97 (Concatenate)    (None, 288)          0           flatten_193[0][0]                
                                                                 flatten_194[0][0]                
__________________________________________________________________________________________________
dense_193 (Dense)               (None, 24)           6936        concatenate_97[0][0]             
__________________________________________________________________________________________________
dropout_97 (Dropout)            (None, 24)           0           dense_193[0][0]                  
__________________________________________________________________________________________________
dense_194 (Dense)               (None, 8)            200         dropout_97[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 24s - loss: 0.0594 - acc: 0.9870 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0441 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0418 - acc: 0.9914 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0407 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0396 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0393 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0390 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0388 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0386 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0384 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0380 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0378 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0378 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0377 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 24s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 24s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 24s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 24s - loss: 0.0374 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 24s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 24s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 24s - loss: 0.0372 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 24s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 24s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 24s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 24s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 24s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 24s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 24s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 24s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 24s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 24s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 24s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 24s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 24s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 24s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 9.2109e-04 - acc: 0.9997
Epoch 2/40
 - 3s - loss: 0.0011 - acc: 0.9996
Epoch 3/40
 - 3s - loss: 4.9906e-04 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 5.5050e-04 - acc: 0.9997
Epoch 5/40
 - 3s - loss: 4.4900e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 1.8830e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 1.7283e-04 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 2.3009e-04 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.5295e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 2.2052e-04 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 2.8918e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 2.8919e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 6.1216e-06 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 4.2766e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 3.5483e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 5.6196e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 2.1116e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 4.8720e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 3.4857e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 2.0893e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.3937e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 2.0789e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.3875e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.3799e-04 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 5.4660e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 5.4361e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 2.0372e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 5.3863e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 3.3555e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 3.3447e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 3.3353e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 1.4639e-06 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 3.3168e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 1.9872e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 2.6441e-04 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 1.9785e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 3.2817e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 1.1192e-06 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 3.2676e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 1.3079e-04 - acc: 1.0000
# Training time = 0:18:20.705239
# F-Score(Ordinary) = 0.71, Recall: 0.708, Precision: 0.711
# F-Score(lvc) = 0.584, Recall: 0.64, Precision: 0.538
# F-Score(ireflv) = 0.802, Recall: 0.75, Precision: 0.861
# F-Score(id) = 0.706, Recall: 0.697, Precision: 0.715
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_195 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_196 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_195 (Embedding)       (None, 4, 48)        705264      input_195[0][0]                  
__________________________________________________________________________________________________
embedding_196 (Embedding)       (None, 4, 24)        5640        input_196[0][0]                  
__________________________________________________________________________________________________
flatten_195 (Flatten)           (None, 192)          0           embedding_195[0][0]              
__________________________________________________________________________________________________
flatten_196 (Flatten)           (None, 96)           0           embedding_196[0][0]              
__________________________________________________________________________________________________
concatenate_98 (Concatenate)    (None, 288)          0           flatten_195[0][0]                
                                                                 flatten_196[0][0]                
__________________________________________________________________________________________________
dense_195 (Dense)               (None, 24)           6936        concatenate_98[0][0]             
__________________________________________________________________________________________________
dropout_98 (Dropout)            (None, 24)           0           dense_195[0][0]                  
__________________________________________________________________________________________________
dense_196 (Dense)               (None, 8)            200         dropout_98[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0576 - acc: 0.9872 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 24s - loss: 0.0432 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 24s - loss: 0.0411 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 24s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 24s - loss: 0.0391 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 24s - loss: 0.0387 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 24s - loss: 0.0383 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 24s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 24s - loss: 0.0379 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 24s - loss: 0.0376 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 24s - loss: 0.0375 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 24s - loss: 0.0373 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 24s - loss: 0.0372 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 24s - loss: 0.0371 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 24s - loss: 0.0370 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 24s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 24s - loss: 0.0369 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 24s - loss: 0.0367 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 24s - loss: 0.0367 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0366 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0364 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0365 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0363 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0363 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0363 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0363 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0363 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0362 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0362 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0361 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0361 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0361 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0361 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0360 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0360 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0359 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 5.7163e-04 - acc: 0.9999
Epoch 2/40
 - 3s - loss: 1.3438e-04 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 2.9913e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 1.2330e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 2.9078e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 2.3279e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 3.9775e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 2.8261e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 5.8777e-05 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 2.2456e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 2.7872e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 3.8563e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 1.1153e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 3.8130e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 1.1010e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.0915e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 2.1619e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 3.7378e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 1.6001e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 4.2175e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 3.1383e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 1.5685e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.0460e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 3.0988e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 3.0757e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 6.5781e-04 - acc: 0.9998
Epoch 27/40
 - 3s - loss: 3.0075e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 5.0797e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 2.4897e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 2.4759e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 3.4357e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 2.9244e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.4591e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 3.3709e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 1.9179e-04 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 2.3830e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 9.5340e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.4223e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 9.4734e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 3.2815e-04 - acc: 0.9999
# Training time = 0:18:43.911089
# F-Score(Ordinary) = 0.64, Recall: 0.569, Precision: 0.732
# F-Score(lvc) = 0.585, Recall: 0.663, Precision: 0.523
# F-Score(ireflv) = 0.796, Recall: 0.728, Precision: 0.877
# F-Score(id) = 0.534, Recall: 0.426, Precision: 0.715
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_197 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_198 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_197 (Embedding)       (None, 4, 48)        705264      input_197[0][0]                  
__________________________________________________________________________________________________
embedding_198 (Embedding)       (None, 4, 24)        5640        input_198[0][0]                  
__________________________________________________________________________________________________
flatten_197 (Flatten)           (None, 192)          0           embedding_197[0][0]              
__________________________________________________________________________________________________
flatten_198 (Flatten)           (None, 96)           0           embedding_198[0][0]              
__________________________________________________________________________________________________
concatenate_99 (Concatenate)    (None, 288)          0           flatten_197[0][0]                
                                                                 flatten_198[0][0]                
__________________________________________________________________________________________________
dense_197 (Dense)               (None, 24)           6936        concatenate_99[0][0]             
__________________________________________________________________________________________________
dropout_99 (Dropout)            (None, 24)           0           dense_197[0][0]                  
__________________________________________________________________________________________________
dense_198 (Dense)               (None, 8)            200         dropout_99[0][0]                 
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 25s - loss: 0.0580 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 25s - loss: 0.0435 - acc: 0.9910 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 25s - loss: 0.0412 - acc: 0.9916 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 25s - loss: 0.0402 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 26s - loss: 0.0394 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 26s - loss: 0.0389 - acc: 0.9921 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 26s - loss: 0.0385 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 26s - loss: 0.0381 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 26s - loss: 0.0379 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 26s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 26s - loss: 0.0374 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 26s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 26s - loss: 0.0371 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 26s - loss: 0.0370 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 26s - loss: 0.0369 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 26s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 26s - loss: 0.0368 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 26s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 26s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0366 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0364 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0364 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 25s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 25s - loss: 0.0361 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 25s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 25s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 25s - loss: 0.0362 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 25s - loss: 0.0361 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 25s - loss: 0.0361 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 25s - loss: 0.0360 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 25s - loss: 0.0361 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 25s - loss: 0.0359 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 25s - loss: 0.0360 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 25s - loss: 0.0359 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 25s - loss: 0.0360 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0359 - acc: 0.9930 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 8.4735e-04 - acc: 0.9997
Epoch 2/40
 - 3s - loss: 8.6187e-04 - acc: 0.9996
Epoch 3/40
 - 3s - loss: 7.9476e-04 - acc: 0.9996
Epoch 4/40
 - 3s - loss: 5.3281e-04 - acc: 0.9998
Epoch 5/40
 - 3s - loss: 6.2934e-04 - acc: 0.9997
Epoch 6/40
 - 3s - loss: 9.8331e-04 - acc: 0.9995
Epoch 7/40
 - 3s - loss: 5.7707e-04 - acc: 0.9997
Epoch 8/40
 - 3s - loss: 5.8288e-04 - acc: 0.9997
Epoch 9/40
 - 3s - loss: 8.4865e-04 - acc: 0.9995
Epoch 10/40
 - 3s - loss: 6.7123e-04 - acc: 0.9996
Epoch 11/40
 - 3s - loss: 5.3443e-04 - acc: 0.9997
Epoch 12/40
 - 3s - loss: 5.9996e-04 - acc: 0.9996
Epoch 13/40
 - 3s - loss: 4.5498e-04 - acc: 0.9997
Epoch 14/40
 - 3s - loss: 4.6027e-04 - acc: 0.9997
Epoch 15/40
 - 3s - loss: 5.2106e-04 - acc: 0.9996
Epoch 16/40
 - 3s - loss: 3.7341e-04 - acc: 0.9997
Epoch 17/40
 - 3s - loss: 4.5182e-04 - acc: 0.9997
Epoch 18/40
 - 3s - loss: 3.6743e-04 - acc: 0.9997
Epoch 19/40
 - 3s - loss: 2.8971e-04 - acc: 0.9998
Epoch 20/40
 - 3s - loss: 4.7800e-04 - acc: 0.9996
Epoch 21/40
 - 3s - loss: 3.0319e-04 - acc: 0.9997
Epoch 22/40
 - 3s - loss: 3.7159e-04 - acc: 0.9997
Epoch 23/40
 - 3s - loss: 2.5609e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 2.9368e-04 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 2.7179e-04 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 4.5553e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 2.5366e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 3.2409e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.2706e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 2.1193e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 2.4337e-04 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.7866e-04 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 2.9004e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 2.7014e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 2.5152e-04 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 2.3373e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 2.0727e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.3230e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.9705e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.3535e-04 - acc: 1.0000
# Training time = 0:19:11.510296
# F-Score(Ordinary) = 0.68, Recall: 0.656, Precision: 0.705
# F-Score(lvc) = 0.565, Recall: 0.663, Precision: 0.492
# F-Score(ireflv) = 0.839, Recall: 0.868, Precision: 0.811
# F-Score(id) = 0.607, Recall: 0.522, Precision: 0.725
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_199 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_200 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_199 (Embedding)       (None, 4, 48)        705264      input_199[0][0]                  
__________________________________________________________________________________________________
embedding_200 (Embedding)       (None, 4, 24)        5640        input_200[0][0]                  
__________________________________________________________________________________________________
flatten_199 (Flatten)           (None, 192)          0           embedding_199[0][0]              
__________________________________________________________________________________________________
flatten_200 (Flatten)           (None, 96)           0           embedding_200[0][0]              
__________________________________________________________________________________________________
concatenate_100 (Concatenate)   (None, 288)          0           flatten_199[0][0]                
                                                                 flatten_200[0][0]                
__________________________________________________________________________________________________
dense_199 (Dense)               (None, 24)           6936        concatenate_100[0][0]            
__________________________________________________________________________________________________
dropout_100 (Dropout)           (None, 24)           0           dense_199[0][0]                  
__________________________________________________________________________________________________
dense_200 (Dense)               (None, 8)            200         dropout_100[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adagrad, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 26s - loss: 0.0587 - acc: 0.9873 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 26s - loss: 0.0441 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 26s - loss: 0.0417 - acc: 0.9915 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 26s - loss: 0.0405 - acc: 0.9918 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 25s - loss: 0.0397 - acc: 0.9919 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 25s - loss: 0.0394 - acc: 0.9920 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 25s - loss: 0.0388 - acc: 0.9922 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 25s - loss: 0.0386 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 25s - loss: 0.0383 - acc: 0.9923 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 25s - loss: 0.0380 - acc: 0.9924 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 25s - loss: 0.0378 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 25s - loss: 0.0377 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 25s - loss: 0.0376 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 25s - loss: 0.0375 - acc: 0.9925 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 25s - loss: 0.0374 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 25s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 25s - loss: 0.0373 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 25s - loss: 0.0371 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 25s - loss: 0.0371 - acc: 0.9926 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 25s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 25s - loss: 0.0369 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 25s - loss: 0.0368 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 25s - loss: 0.0368 - acc: 0.9927 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 25s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 25s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 25s - loss: 0.0367 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 26s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 26s - loss: 0.0366 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 26s - loss: 0.0366 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 26s - loss: 0.0365 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 26s - loss: 0.0364 - acc: 0.9928 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 26s - loss: 0.0364 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 26s - loss: 0.0364 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 26s - loss: 0.0364 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 26s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 26s - loss: 0.0364 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 26s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 26s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 26s - loss: 0.0363 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 25s - loss: 0.0362 - acc: 0.9929 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0010 - acc: 0.9995
Epoch 2/40
 - 3s - loss: 5.4512e-04 - acc: 0.9997
Epoch 3/40
 - 3s - loss: 5.2299e-04 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 7.6074e-04 - acc: 0.9996
Epoch 5/40
 - 3s - loss: 4.0686e-04 - acc: 0.9998
Epoch 6/40
 - 3s - loss: 5.6388e-04 - acc: 0.9997
Epoch 7/40
 - 3s - loss: 4.0469e-04 - acc: 0.9998
Epoch 8/40
 - 3s - loss: 7.0652e-04 - acc: 0.9996
Epoch 9/40
 - 3s - loss: 4.8207e-04 - acc: 0.9997
Epoch 10/40
 - 3s - loss: 4.2412e-04 - acc: 0.9997
Epoch 11/40
 - 3s - loss: 3.9097e-04 - acc: 0.9997
Epoch 12/40
 - 3s - loss: 5.1605e-04 - acc: 0.9996
Epoch 13/40
 - 3s - loss: 3.6495e-04 - acc: 0.9997
Epoch 14/40
 - 3s - loss: 4.6382e-04 - acc: 0.9997
Epoch 15/40
 - 3s - loss: 3.7661e-04 - acc: 0.9997
Epoch 16/40
 - 3s - loss: 5.1728e-04 - acc: 0.9996
Epoch 17/40
 - 3s - loss: 3.4895e-04 - acc: 0.9997
Epoch 18/40
 - 3s - loss: 3.0664e-04 - acc: 0.9997
Epoch 19/40
 - 3s - loss: 2.5190e-04 - acc: 0.9998
Epoch 20/40
 - 3s - loss: 2.6120e-04 - acc: 0.9998
Epoch 21/40
 - 3s - loss: 3.1370e-04 - acc: 0.9997
Epoch 22/40
 - 3s - loss: 2.9016e-04 - acc: 0.9997
Epoch 23/40
 - 3s - loss: 3.5114e-04 - acc: 0.9998
Epoch 24/40
 - 3s - loss: 2.3217e-04 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 4.3457e-04 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 4.0310e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 2.9129e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 2.7065e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 2.4006e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 3.2075e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 2.8819e-04 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 2.7858e-04 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 2.7921e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 2.7924e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 2.0557e-04 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 2.6282e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 2.2828e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.2156e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.8253e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 2.0996e-04 - acc: 1.0000
# Training time = 0:19:14.853614
# F-Score(Ordinary) = 0.704, Recall: 0.72, Precision: 0.689
# F-Score(lvc) = 0.595, Recall: 0.836, Precision: 0.462
# F-Score(ireflv) = 0.836, Recall: 0.836, Precision: 0.836
# F-Score(id) = 0.634, Recall: 0.579, Precision: 0.699
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_201 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_202 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_201 (Embedding)       (None, 4, 48)        705264      input_201[0][0]                  
__________________________________________________________________________________________________
embedding_202 (Embedding)       (None, 4, 24)        5640        input_202[0][0]                  
__________________________________________________________________________________________________
flatten_201 (Flatten)           (None, 192)          0           embedding_201[0][0]              
__________________________________________________________________________________________________
flatten_202 (Flatten)           (None, 96)           0           embedding_202[0][0]              
__________________________________________________________________________________________________
concatenate_101 (Concatenate)   (None, 288)          0           flatten_201[0][0]                
                                                                 flatten_202[0][0]                
__________________________________________________________________________________________________
dense_201 (Dense)               (None, 24)           6936        concatenate_101[0][0]            
__________________________________________________________________________________________________
dropout_101 (Dropout)           (None, 24)           0           dense_201[0][0]                  
__________________________________________________________________________________________________
dense_202 (Dense)               (None, 8)            200         dropout_101[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 31s - loss: 0.2876 - acc: 0.9222 - val_loss: 3.5316e-04 - val_acc: 1.0000
Epoch 2/40
 - 31s - loss: 0.0946 - acc: 0.9787 - val_loss: 4.7029e-05 - val_acc: 1.0000
Epoch 3/40
 - 31s - loss: 0.0749 - acc: 0.9834 - val_loss: 1.2577e-05 - val_acc: 1.0000
Epoch 4/40
 - 31s - loss: 0.0676 - acc: 0.9853 - val_loss: 4.0531e-06 - val_acc: 1.0000
Epoch 5/40
 - 31s - loss: 0.0636 - acc: 0.9863 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 6/40
 - 31s - loss: 0.0610 - acc: 0.9870 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 7/40
 - 31s - loss: 0.0590 - acc: 0.9876 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 8/40
 - 31s - loss: 0.0574 - acc: 0.9879 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 31s - loss: 0.0561 - acc: 0.9882 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 31s - loss: 0.0551 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 31s - loss: 0.0544 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 31s - loss: 0.0539 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 31s - loss: 0.0531 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 31s - loss: 0.0525 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 31s - loss: 0.0521 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 31s - loss: 0.0516 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 31s - loss: 0.0511 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 30s - loss: 0.0508 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 30s - loss: 0.0504 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 30s - loss: 0.0503 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 30s - loss: 0.0500 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 30s - loss: 0.0497 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0497 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0493 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0492 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0490 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0489 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0488 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0486 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0485 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0482 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0482 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0480 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0480 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0479 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0477 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0477 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0476 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0477 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0475 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0014 - acc: 0.9996
Epoch 2/40
 - 3s - loss: 7.2086e-04 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 8.9610e-04 - acc: 0.9998
Epoch 4/40
 - 3s - loss: 6.5285e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 6.0798e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 6.6335e-04 - acc: 0.9998
Epoch 7/40
 - 3s - loss: 6.9142e-04 - acc: 0.9998
Epoch 8/40
 - 3s - loss: 3.8601e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 4.9943e-04 - acc: 0.9998
Epoch 10/40
 - 3s - loss: 4.1413e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 2.7148e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 3.3105e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 2.8969e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 2.9408e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 2.3581e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 2.8244e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 3.2037e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 2.1059e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 3.5294e-04 - acc: 0.9998
Epoch 20/40
 - 3s - loss: 2.6821e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 2.3575e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 1.9714e-04 - acc: 0.9999
Epoch 23/40
 - 3s - loss: 2.5640e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 2.8542e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 2.3846e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 2.9323e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 2.3240e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 3.1843e-04 - acc: 0.9998
Epoch 29/40
 - 3s - loss: 2.3785e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 2.1839e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 1.6229e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 1.2824e-04 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 2.5440e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 1.3886e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.5102e-04 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 2.0019e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 1.9794e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 2.0625e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 1.6735e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 1.2226e-04 - acc: 0.9999
# Training time = 0:22:46.104143
# F-Score(Ordinary) = 0.615, Recall: 0.826, Precision: 0.49
# F-Score(lvc) = 0.424, Recall: 0.947, Precision: 0.273
# F-Score(ireflv) = 0.759, Recall: 0.833, Precision: 0.697
# F-Score(id) = 0.616, Recall: 0.784, Precision: 0.508
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_203 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_204 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_203 (Embedding)       (None, 4, 48)        705264      input_203[0][0]                  
__________________________________________________________________________________________________
embedding_204 (Embedding)       (None, 4, 24)        5640        input_204[0][0]                  
__________________________________________________________________________________________________
flatten_203 (Flatten)           (None, 192)          0           embedding_203[0][0]              
__________________________________________________________________________________________________
flatten_204 (Flatten)           (None, 96)           0           embedding_204[0][0]              
__________________________________________________________________________________________________
concatenate_102 (Concatenate)   (None, 288)          0           flatten_203[0][0]                
                                                                 flatten_204[0][0]                
__________________________________________________________________________________________________
dense_203 (Dense)               (None, 24)           6936        concatenate_102[0][0]            
__________________________________________________________________________________________________
dropout_102 (Dropout)           (None, 24)           0           dense_203[0][0]                  
__________________________________________________________________________________________________
dense_204 (Dense)               (None, 8)            200         dropout_102[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.2997 - acc: 0.9179 - val_loss: 3.4750e-04 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0974 - acc: 0.9781 - val_loss: 6.9740e-05 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0763 - acc: 0.9834 - val_loss: 2.3246e-05 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0686 - acc: 0.9852 - val_loss: 6.3181e-06 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0643 - acc: 0.9862 - val_loss: 2.5034e-06 - val_acc: 1.0000
Epoch 6/40
 - 30s - loss: 0.0616 - acc: 0.9870 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 7/40
 - 30s - loss: 0.0596 - acc: 0.9876 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0580 - acc: 0.9880 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0566 - acc: 0.9882 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 10/40
 - 30s - loss: 0.0556 - acc: 0.9885 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 11/40
 - 30s - loss: 0.0548 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0538 - acc: 0.9889 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0535 - acc: 0.9890 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0528 - acc: 0.9892 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0525 - acc: 0.9893 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0519 - acc: 0.9894 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0517 - acc: 0.9894 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0510 - acc: 0.9894 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0508 - acc: 0.9895 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0506 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0503 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0500 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0496 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0493 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0494 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0492 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0490 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0488 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0486 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0484 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0482 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0482 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0480 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0479 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0477 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0477 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0475 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0474 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0474 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0472 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0019 - acc: 0.9991
Epoch 2/40
 - 3s - loss: 0.0011 - acc: 0.9996
Epoch 3/40
 - 3s - loss: 8.4974e-04 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 6.1169e-04 - acc: 0.9998
Epoch 5/40
 - 3s - loss: 7.1811e-04 - acc: 0.9998
Epoch 6/40
 - 3s - loss: 6.0456e-04 - acc: 0.9998
Epoch 7/40
 - 3s - loss: 2.9632e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 3.4497e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 4.0470e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 3.2241e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 1.9835e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 3.2232e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 1.8903e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 1.9726e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 1.6788e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 2.1459e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 2.1483e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 2.7016e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 2.1146e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 2.8833e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 1.3373e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 2.1753e-04 - acc: 0.9999
Epoch 23/40
 - 3s - loss: 2.0500e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 8.9015e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.6361e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 1.8244e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 1.3604e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 1.2044e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 1.9001e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 1.2081e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 2.0291e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 1.8248e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.5557e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 1.2752e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 3.6666e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 9.4624e-05 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 1.3951e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 7.9373e-05 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 3.7386e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.9258e-04 - acc: 0.9999
# Training time = 0:22:13.609595
# F-Score(Ordinary) = 0.492, Recall: 0.86, Precision: 0.345
# F-Score(lvc) = 0.293, Recall: 0.75, Precision: 0.182
# F-Score(ireflv) = 0.58, Recall: 0.789, Precision: 0.459
# F-Score(id) = 0.55, Recall: 0.974, Precision: 0.383
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_205 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_206 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_205 (Embedding)       (None, 4, 48)        705264      input_205[0][0]                  
__________________________________________________________________________________________________
embedding_206 (Embedding)       (None, 4, 24)        5640        input_206[0][0]                  
__________________________________________________________________________________________________
flatten_205 (Flatten)           (None, 192)          0           embedding_205[0][0]              
__________________________________________________________________________________________________
flatten_206 (Flatten)           (None, 96)           0           embedding_206[0][0]              
__________________________________________________________________________________________________
concatenate_103 (Concatenate)   (None, 288)          0           flatten_205[0][0]                
                                                                 flatten_206[0][0]                
__________________________________________________________________________________________________
dense_205 (Dense)               (None, 24)           6936        concatenate_103[0][0]            
__________________________________________________________________________________________________
dropout_103 (Dropout)           (None, 24)           0           dense_205[0][0]                  
__________________________________________________________________________________________________
dense_206 (Dense)               (None, 8)            200         dropout_103[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.2958 - acc: 0.9177 - val_loss: 4.1165e-04 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0960 - acc: 0.9784 - val_loss: 1.0831e-04 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0764 - acc: 0.9836 - val_loss: 5.8891e-05 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0693 - acc: 0.9853 - val_loss: 3.8386e-05 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0655 - acc: 0.9863 - val_loss: 3.2664e-05 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0629 - acc: 0.9869 - val_loss: 2.7061e-05 - val_acc: 1.0000
Epoch 7/40
 - 30s - loss: 0.0606 - acc: 0.9875 - val_loss: 2.1816e-05 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0587 - acc: 0.9878 - val_loss: 2.1219e-05 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0578 - acc: 0.9880 - val_loss: 1.7345e-05 - val_acc: 1.0000
Epoch 10/40
 - 30s - loss: 0.0567 - acc: 0.9883 - val_loss: 1.9372e-05 - val_acc: 1.0000
Epoch 11/40
 - 31s - loss: 0.0556 - acc: 0.9886 - val_loss: 1.6570e-05 - val_acc: 1.0000
Epoch 12/40
 - 31s - loss: 0.0549 - acc: 0.9888 - val_loss: 1.7822e-05 - val_acc: 1.0000
Epoch 13/40
 - 31s - loss: 0.0544 - acc: 0.9889 - val_loss: 1.6093e-05 - val_acc: 1.0000
Epoch 14/40
 - 31s - loss: 0.0537 - acc: 0.9890 - val_loss: 1.3590e-05 - val_acc: 1.0000
Epoch 15/40
 - 30s - loss: 0.0531 - acc: 0.9891 - val_loss: 1.2159e-05 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0526 - acc: 0.9893 - val_loss: 1.1563e-05 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0520 - acc: 0.9893 - val_loss: 1.0371e-05 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0519 - acc: 0.9894 - val_loss: 1.0967e-05 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0515 - acc: 0.9895 - val_loss: 8.4639e-06 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0509 - acc: 0.9897 - val_loss: 7.8678e-06 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0510 - acc: 0.9896 - val_loss: 7.9274e-06 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0507 - acc: 0.9897 - val_loss: 7.8678e-06 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0504 - acc: 0.9898 - val_loss: 8.2255e-06 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0502 - acc: 0.9898 - val_loss: 6.4373e-06 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0500 - acc: 0.9899 - val_loss: 6.9142e-06 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0499 - acc: 0.9898 - val_loss: 5.1856e-06 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0496 - acc: 0.9899 - val_loss: 4.5896e-06 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0493 - acc: 0.9899 - val_loss: 4.7684e-06 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0493 - acc: 0.9900 - val_loss: 4.1723e-06 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0490 - acc: 0.9900 - val_loss: 4.4108e-06 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0489 - acc: 0.9900 - val_loss: 3.6359e-06 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0486 - acc: 0.9901 - val_loss: 3.4571e-06 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0485 - acc: 0.9901 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0486 - acc: 0.9901 - val_loss: 3.2187e-06 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0485 - acc: 0.9902 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0484 - acc: 0.9902 - val_loss: 2.2650e-06 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0483 - acc: 0.9902 - val_loss: 2.6226e-06 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0481 - acc: 0.9902 - val_loss: 2.2650e-06 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0481 - acc: 0.9902 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0480 - acc: 0.9903 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0018 - acc: 0.9994
Epoch 2/40
 - 3s - loss: 0.0014 - acc: 0.9995
Epoch 3/40
 - 3s - loss: 0.0011 - acc: 0.9996
Epoch 4/40
 - 3s - loss: 9.5392e-04 - acc: 0.9995
Epoch 5/40
 - 3s - loss: 9.4259e-04 - acc: 0.9995
Epoch 6/40
 - 3s - loss: 7.2814e-04 - acc: 0.9996
Epoch 7/40
 - 3s - loss: 5.9793e-04 - acc: 0.9998
Epoch 8/40
 - 3s - loss: 4.9490e-04 - acc: 0.9998
Epoch 9/40
 - 3s - loss: 2.9155e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 2.8399e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 4.1508e-04 - acc: 0.9998
Epoch 12/40
 - 3s - loss: 1.7379e-04 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.9640e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 1.9620e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 1.3742e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.2514e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.6793e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.5904e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 8.9129e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.0075e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 9.8566e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.1879e-04 - acc: 0.9999
Epoch 23/40
 - 3s - loss: 7.3159e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.2120e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 5.2727e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 9.6367e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 4.5584e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.0954e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 4.5622e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 3.5309e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 5.2245e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 9.3092e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 3.1167e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 3.9089e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 6.5136e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 5.3650e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 5.4082e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.2880e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 3.5628e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 4.6912e-05 - acc: 1.0000
# Training time = 0:22:21.738475
# F-Score(Ordinary) = 0.605, Recall: 0.88, Precision: 0.461
# F-Score(lvc) = 0.46, Recall: 0.952, Precision: 0.303
# F-Score(ireflv) = 0.816, Recall: 0.797, Precision: 0.836
# F-Score(id) = 0.498, Recall: 1.0, Precision: 0.332
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_207 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_208 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_207 (Embedding)       (None, 4, 48)        705264      input_207[0][0]                  
__________________________________________________________________________________________________
embedding_208 (Embedding)       (None, 4, 24)        5640        input_208[0][0]                  
__________________________________________________________________________________________________
flatten_207 (Flatten)           (None, 192)          0           embedding_207[0][0]              
__________________________________________________________________________________________________
flatten_208 (Flatten)           (None, 96)           0           embedding_208[0][0]              
__________________________________________________________________________________________________
concatenate_104 (Concatenate)   (None, 288)          0           flatten_207[0][0]                
                                                                 flatten_208[0][0]                
__________________________________________________________________________________________________
dense_207 (Dense)               (None, 24)           6936        concatenate_104[0][0]            
__________________________________________________________________________________________________
dropout_104 (Dropout)           (None, 24)           0           dense_207[0][0]                  
__________________________________________________________________________________________________
dense_208 (Dense)               (None, 8)            200         dropout_104[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.3115 - acc: 0.9145 - val_loss: 3.0027e-04 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.1019 - acc: 0.9767 - val_loss: 2.8372e-05 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0781 - acc: 0.9828 - val_loss: 8.3447e-06 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0695 - acc: 0.9851 - val_loss: 2.6822e-06 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0652 - acc: 0.9863 - val_loss: 1.3113e-06 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0626 - acc: 0.9869 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0608 - acc: 0.9873 - val_loss: 2.9802e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0591 - acc: 0.9878 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0579 - acc: 0.9881 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0570 - acc: 0.9884 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0559 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0551 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0546 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0540 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0535 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0529 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0525 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0521 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0518 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0518 - acc: 0.9895 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0511 - acc: 0.9896 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0509 - acc: 0.9896 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0505 - acc: 0.9898 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0506 - acc: 0.9897 - val_loss: 6.5565e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0502 - acc: 0.9898 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0499 - acc: 0.9898 - val_loss: 1.0133e-06 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0498 - acc: 0.9899 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0495 - acc: 0.9899 - val_loss: 1.6093e-06 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0494 - acc: 0.9899 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0492 - acc: 0.9900 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0489 - acc: 0.9900 - val_loss: 2.6226e-06 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0488 - acc: 0.9900 - val_loss: 2.6226e-06 - val_acc: 1.0000
Epoch 33/40
 - 31s - loss: 0.0488 - acc: 0.9901 - val_loss: 3.0398e-06 - val_acc: 1.0000
Epoch 34/40
 - 31s - loss: 0.0487 - acc: 0.9900 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 35/40
 - 31s - loss: 0.0486 - acc: 0.9902 - val_loss: 3.0994e-06 - val_acc: 1.0000
Epoch 36/40
 - 31s - loss: 0.0483 - acc: 0.9901 - val_loss: 3.5763e-06 - val_acc: 1.0000
Epoch 37/40
 - 31s - loss: 0.0484 - acc: 0.9901 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 38/40
 - 31s - loss: 0.0482 - acc: 0.9902 - val_loss: 2.9206e-06 - val_acc: 1.0000
Epoch 39/40
 - 31s - loss: 0.0482 - acc: 0.9903 - val_loss: 2.9802e-06 - val_acc: 1.0000
Epoch 40/40
 - 31s - loss: 0.0482 - acc: 0.9902 - val_loss: 3.0398e-06 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0017 - acc: 0.9995
Epoch 2/40
 - 3s - loss: 0.0015 - acc: 0.9995
Epoch 3/40
 - 3s - loss: 9.3012e-04 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 8.5200e-04 - acc: 0.9997
Epoch 5/40
 - 3s - loss: 6.7582e-04 - acc: 0.9998
Epoch 6/40
 - 3s - loss: 6.0465e-04 - acc: 0.9998
Epoch 7/40
 - 3s - loss: 5.1336e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 5.6618e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 3.3279e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 4.0549e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 3.9629e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 2.5356e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 2.2047e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 2.6363e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 1.1303e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 2.9724e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 2.8447e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 2.0076e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 2.1016e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 1.2341e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 1.4328e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 7.6491e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.7749e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 7.7168e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.5051e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 1.6843e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 1.8882e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 7.8656e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 9.9296e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.0063e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.2294e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 1.8265e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 9.0437e-05 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 1.1735e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 1.0521e-04 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.2664e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 1.5620e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 1.3829e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 1.0232e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 2.1314e-04 - acc: 0.9999
# Training time = 0:22:20.004003
# F-Score(Ordinary) = 0.518, Recall: 0.509, Precision: 0.528
# F-Score(lvc) = 0.356, Recall: 0.935, Precision: 0.22
# F-Score(ireflv) = 0.744, Recall: 0.637, Precision: 0.893
# F-Score(id) = 0.433, Recall: 0.401, Precision: 0.472
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_209 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_210 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_209 (Embedding)       (None, 4, 48)        705264      input_209[0][0]                  
__________________________________________________________________________________________________
embedding_210 (Embedding)       (None, 4, 24)        5640        input_210[0][0]                  
__________________________________________________________________________________________________
flatten_209 (Flatten)           (None, 192)          0           embedding_209[0][0]              
__________________________________________________________________________________________________
flatten_210 (Flatten)           (None, 96)           0           embedding_210[0][0]              
__________________________________________________________________________________________________
concatenate_105 (Concatenate)   (None, 288)          0           flatten_209[0][0]                
                                                                 flatten_210[0][0]                
__________________________________________________________________________________________________
dense_209 (Dense)               (None, 24)           6936        concatenate_105[0][0]            
__________________________________________________________________________________________________
dropout_105 (Dropout)           (None, 24)           0           dense_209[0][0]                  
__________________________________________________________________________________________________
dense_210 (Dense)               (None, 8)            200         dropout_105[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.1
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 31s - loss: 0.2921 - acc: 0.9191 - val_loss: 6.3589e-04 - val_acc: 1.0000
Epoch 2/40
 - 31s - loss: 0.0998 - acc: 0.9774 - val_loss: 1.8199e-04 - val_acc: 1.0000
Epoch 3/40
 - 31s - loss: 0.0785 - acc: 0.9828 - val_loss: 6.0799e-05 - val_acc: 1.0000
Epoch 4/40
 - 31s - loss: 0.0709 - acc: 0.9846 - val_loss: 2.4319e-05 - val_acc: 1.0000
Epoch 5/40
 - 31s - loss: 0.0667 - acc: 0.9858 - val_loss: 9.7752e-06 - val_acc: 1.0000
Epoch 6/40
 - 31s - loss: 0.0640 - acc: 0.9865 - val_loss: 4.3511e-06 - val_acc: 1.0000
Epoch 7/40
 - 31s - loss: 0.0617 - acc: 0.9870 - val_loss: 2.3842e-06 - val_acc: 1.0000
Epoch 8/40
 - 31s - loss: 0.0601 - acc: 0.9873 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 9/40
 - 31s - loss: 0.0587 - acc: 0.9878 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 10/40
 - 31s - loss: 0.0575 - acc: 0.9881 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 11/40
 - 31s - loss: 0.0566 - acc: 0.9883 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 12/40
 - 31s - loss: 0.0557 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 31s - loss: 0.0550 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 31s - loss: 0.0543 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 31s - loss: 0.0536 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 31s - loss: 0.0531 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 31s - loss: 0.0528 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0524 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0518 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0514 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0511 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0509 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0506 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0505 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0501 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0500 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0498 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0495 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0494 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0493 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0492 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0489 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0488 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0485 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0485 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0484 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0484 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0483 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0482 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0481 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0023 - acc: 0.9993
Epoch 2/40
 - 3s - loss: 0.0027 - acc: 0.9992
Epoch 3/40
 - 3s - loss: 0.0022 - acc: 0.9993
Epoch 4/40
 - 3s - loss: 0.0017 - acc: 0.9993
Epoch 5/40
 - 3s - loss: 0.0020 - acc: 0.9994
Epoch 6/40
 - 3s - loss: 0.0015 - acc: 0.9996
Epoch 7/40
 - 3s - loss: 0.0016 - acc: 0.9996
Epoch 8/40
 - 3s - loss: 0.0016 - acc: 0.9995
Epoch 9/40
 - 3s - loss: 0.0013 - acc: 0.9996
Epoch 10/40
 - 3s - loss: 0.0010 - acc: 0.9996
Epoch 11/40
 - 3s - loss: 8.3533e-04 - acc: 0.9997
Epoch 12/40
 - 3s - loss: 9.1678e-04 - acc: 0.9997
Epoch 13/40
 - 3s - loss: 9.2183e-04 - acc: 0.9996
Epoch 14/40
 - 3s - loss: 8.1718e-04 - acc: 0.9997
Epoch 15/40
 - 3s - loss: 9.4914e-04 - acc: 0.9995
Epoch 16/40
 - 3s - loss: 7.2498e-04 - acc: 0.9996
Epoch 17/40
 - 3s - loss: 5.1822e-04 - acc: 0.9997
Epoch 18/40
 - 3s - loss: 6.2934e-04 - acc: 0.9997
Epoch 19/40
 - 3s - loss: 6.7601e-04 - acc: 0.9996
Epoch 20/40
 - 3s - loss: 5.1175e-04 - acc: 0.9997
Epoch 21/40
 - 3s - loss: 5.2872e-04 - acc: 0.9998
Epoch 22/40
 - 3s - loss: 5.2965e-04 - acc: 0.9998
Epoch 23/40
 - 3s - loss: 3.4565e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 4.1923e-04 - acc: 0.9998
Epoch 25/40
 - 3s - loss: 2.9748e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 5.1939e-04 - acc: 0.9998
Epoch 27/40
 - 3s - loss: 4.0408e-04 - acc: 0.9998
Epoch 28/40
 - 3s - loss: 6.6188e-04 - acc: 0.9998
Epoch 29/40
 - 3s - loss: 4.4895e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 4.0979e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 3.3294e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 3.4806e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 2.3641e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 3.4039e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 4.8212e-04 - acc: 0.9998
Epoch 36/40
 - 3s - loss: 2.3373e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 4.4858e-04 - acc: 0.9998
Epoch 38/40
 - 3s - loss: 1.9802e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 4.4653e-04 - acc: 0.9998
Epoch 40/40
 - 3s - loss: 4.4145e-04 - acc: 0.9998
# Training time = 0:22:31.124859
# F-Score(Ordinary) = 0.57, Recall: 0.698, Precision: 0.481
# F-Score(lvc) = 0.402, Recall: 0.919, Precision: 0.258
# F-Score(ireflv) = 0.785, Recall: 0.792, Precision: 0.779
# F-Score(id) = 0.488, Recall: 0.556, Precision: 0.435
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_211 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_212 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_211 (Embedding)       (None, 4, 48)        705264      input_211[0][0]                  
__________________________________________________________________________________________________
embedding_212 (Embedding)       (None, 4, 24)        5640        input_212[0][0]                  
__________________________________________________________________________________________________
flatten_211 (Flatten)           (None, 192)          0           embedding_211[0][0]              
__________________________________________________________________________________________________
flatten_212 (Flatten)           (None, 96)           0           embedding_212[0][0]              
__________________________________________________________________________________________________
concatenate_106 (Concatenate)   (None, 288)          0           flatten_211[0][0]                
                                                                 flatten_212[0][0]                
__________________________________________________________________________________________________
dense_211 (Dense)               (None, 24)           6936        concatenate_106[0][0]            
__________________________________________________________________________________________________
dropout_106 (Dropout)           (None, 24)           0           dense_211[0][0]                  
__________________________________________________________________________________________________
dense_212 (Dense)               (None, 8)            200         dropout_106[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.1183 - acc: 0.9710 - val_loss: 2.0266e-06 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0590 - acc: 0.9876 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0540 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0516 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0504 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0494 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0489 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0485 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0480 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0478 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0476 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0475 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0475 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0474 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0473 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0472 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0471 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0470 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0470 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0471 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0473 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0472 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0473 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0472 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0475 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0477 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0474 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0477 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0478 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0479 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0478 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0481 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0483 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0483 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0484 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0486 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0491 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0487 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0492 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0493 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 8.9269e-04 - acc: 0.9996
Epoch 2/40
 - 3s - loss: 3.5674e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 4.3586e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 1.3829e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 3.6719e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 2.9570e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 3.4301e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 2.2278e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 1.6022e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 2.0286e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 1.4463e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 2.6944e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 4.3690e-05 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 2.1063e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 8.4474e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.4001e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 2.3652e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 1.9443e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 2.5960e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 1.2155e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 2.4040e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 5.6483e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 8.0151e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 2.1379e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 2.7362e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 3.4257e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 1.1521e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.6965e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 2.6397e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 2.0436e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 1.6617e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 1.6549e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.4637e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 1.8117e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 9.0980e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 2.3369e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 1.2580e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.1168e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 1.9412e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 2.2725e-04 - acc: 0.9999
# Training time = 0:21:51.554326
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_213 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_214 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_213 (Embedding)       (None, 4, 48)        705264      input_213[0][0]                  
__________________________________________________________________________________________________
embedding_214 (Embedding)       (None, 4, 24)        5640        input_214[0][0]                  
__________________________________________________________________________________________________
flatten_213 (Flatten)           (None, 192)          0           embedding_213[0][0]              
__________________________________________________________________________________________________
flatten_214 (Flatten)           (None, 96)           0           embedding_214[0][0]              
__________________________________________________________________________________________________
concatenate_107 (Concatenate)   (None, 288)          0           flatten_213[0][0]                
                                                                 flatten_214[0][0]                
__________________________________________________________________________________________________
dense_213 (Dense)               (None, 24)           6936        concatenate_107[0][0]            
__________________________________________________________________________________________________
dropout_107 (Dropout)           (None, 24)           0           dense_213[0][0]                  
__________________________________________________________________________________________________
dense_214 (Dense)               (None, 8)            200         dropout_107[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 28s - loss: 0.1222 - acc: 0.9700 - val_loss: 3.3379e-06 - val_acc: 1.0000
Epoch 2/40
 - 28s - loss: 0.0598 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 28s - loss: 0.0549 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 28s - loss: 0.0523 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 28s - loss: 0.0508 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 28s - loss: 0.0497 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 28s - loss: 0.0490 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 28s - loss: 0.0482 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 28s - loss: 0.0478 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 28s - loss: 0.0475 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 28s - loss: 0.0474 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 28s - loss: 0.0469 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 28s - loss: 0.0470 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 28s - loss: 0.0469 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 28s - loss: 0.0468 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 28s - loss: 0.0468 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 28s - loss: 0.0468 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 28s - loss: 0.0464 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 28s - loss: 0.0466 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 28s - loss: 0.0466 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 28s - loss: 0.0465 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 28s - loss: 0.0466 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 28s - loss: 0.0465 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 28s - loss: 0.0463 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 28s - loss: 0.0469 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 28s - loss: 0.0469 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 28s - loss: 0.0470 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 28s - loss: 0.0467 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 28s - loss: 0.0468 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 28s - loss: 0.0469 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 28s - loss: 0.0471 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 28s - loss: 0.0472 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 28s - loss: 0.0473 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 28s - loss: 0.0474 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 28s - loss: 0.0476 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 28s - loss: 0.0477 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 28s - loss: 0.0478 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 28s - loss: 0.0479 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 28s - loss: 0.0478 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 28s - loss: 0.0478 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0016 - acc: 0.9995
Epoch 2/40
 - 3s - loss: 0.0012 - acc: 0.9997
Epoch 3/40
 - 3s - loss: 0.0011 - acc: 0.9997
Epoch 4/40
 - 3s - loss: 8.0938e-04 - acc: 0.9997
Epoch 5/40
 - 3s - loss: 8.3331e-04 - acc: 0.9995
Epoch 6/40
 - 3s - loss: 8.6158e-04 - acc: 0.9998
Epoch 7/40
 - 3s - loss: 5.4950e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 4.3302e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 2.3443e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 2.7885e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 2.1865e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 3.1564e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 1.1603e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.5500e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.5830e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 1.0706e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 3.1129e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 4.8870e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 2.5584e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 5.9995e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 3.4008e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 1.0254e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 2.0047e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 1.8713e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 2.8315e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 4.8039e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 3.7123e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 3.1870e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 2.8316e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 5.1164e-04 - acc: 0.9998
Epoch 31/40
 - 3s - loss: 1.8537e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 2.7928e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 4.5571e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.4042e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 8.9323e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 8.9201e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.8309e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 1.7704e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 9.4081e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 3.1970e-04 - acc: 0.9999
# Training time = 0:21:25.657628
# F-Score(Ordinary) = 0.119, Recall: 0.725, Precision: 0.065
# F-Score(lvc) = 0.125, Recall: 0.75, Precision: 0.068
# F-Score(ireflv) = 0.016, Recall: 1.0, Precision: 0.008
# F-Score(id) = 0.176, Recall: 0.826, Precision: 0.098
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_215 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_216 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_215 (Embedding)       (None, 4, 48)        705264      input_215[0][0]                  
__________________________________________________________________________________________________
embedding_216 (Embedding)       (None, 4, 24)        5640        input_216[0][0]                  
__________________________________________________________________________________________________
flatten_215 (Flatten)           (None, 192)          0           embedding_215[0][0]              
__________________________________________________________________________________________________
flatten_216 (Flatten)           (None, 96)           0           embedding_216[0][0]              
__________________________________________________________________________________________________
concatenate_108 (Concatenate)   (None, 288)          0           flatten_215[0][0]                
                                                                 flatten_216[0][0]                
__________________________________________________________________________________________________
dense_215 (Dense)               (None, 24)           6936        concatenate_108[0][0]            
__________________________________________________________________________________________________
dropout_108 (Dropout)           (None, 24)           0           dense_215[0][0]                  
__________________________________________________________________________________________________
dense_216 (Dense)               (None, 8)            200         dropout_108[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.1206 - acc: 0.9701 - val_loss: 1.8716e-05 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0602 - acc: 0.9875 - val_loss: 1.3352e-05 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0554 - acc: 0.9887 - val_loss: 1.3113e-05 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0526 - acc: 0.9893 - val_loss: 1.4424e-05 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0512 - acc: 0.9896 - val_loss: 1.0967e-05 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0501 - acc: 0.9898 - val_loss: 5.9009e-06 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0491 - acc: 0.9900 - val_loss: 5.4836e-06 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0485 - acc: 0.9901 - val_loss: 3.9935e-06 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0485 - acc: 0.9901 - val_loss: 3.0398e-06 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0482 - acc: 0.9902 - val_loss: 2.1458e-06 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0481 - acc: 0.9903 - val_loss: 1.9074e-06 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0479 - acc: 0.9904 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0479 - acc: 0.9904 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0477 - acc: 0.9904 - val_loss: 1.4901e-06 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0477 - acc: 0.9905 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0477 - acc: 0.9905 - val_loss: 1.7881e-06 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0476 - acc: 0.9904 - val_loss: 1.4305e-06 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0480 - acc: 0.9905 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0478 - acc: 0.9905 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0478 - acc: 0.9906 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0480 - acc: 0.9906 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0481 - acc: 0.9905 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0479 - acc: 0.9907 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0480 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0481 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0485 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0484 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0485 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0483 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0484 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0484 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0485 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0484 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0485 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0487 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0487 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0489 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0487 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0490 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0490 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0012 - acc: 0.9996
Epoch 2/40
 - 3s - loss: 7.2238e-04 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 2.3568e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 2.3706e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 3.2251e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 1.3846e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 2.8205e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 2.2932e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 1.9246e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 2.2599e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 3.4077e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 7.4537e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.4011e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 9.9531e-05 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.0907e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 6.6942e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 3.2910e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 2.2652e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 1.7476e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 1.9014e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 6.6864e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.2590e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 6.9370e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1423e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 7.8261e-05 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 3.9727e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.6394e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 2.0988e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 4.7868e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 9.7943e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.4203e-04 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.8190e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.7737e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.1151e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.4659e-04 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.0531e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 6.0409e-06 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.1592e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 9.0245e-06 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.0751e-04 - acc: 1.0000
# Training time = 0:21:49.937797
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_217 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_218 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_217 (Embedding)       (None, 4, 48)        705264      input_217[0][0]                  
__________________________________________________________________________________________________
embedding_218 (Embedding)       (None, 4, 24)        5640        input_218[0][0]                  
__________________________________________________________________________________________________
flatten_217 (Flatten)           (None, 192)          0           embedding_217[0][0]              
__________________________________________________________________________________________________
flatten_218 (Flatten)           (None, 96)           0           embedding_218[0][0]              
__________________________________________________________________________________________________
concatenate_109 (Concatenate)   (None, 288)          0           flatten_217[0][0]                
                                                                 flatten_218[0][0]                
__________________________________________________________________________________________________
dense_217 (Dense)               (None, 24)           6936        concatenate_109[0][0]            
__________________________________________________________________________________________________
dropout_109 (Dropout)           (None, 24)           0           dense_217[0][0]                  
__________________________________________________________________________________________________
dense_218 (Dense)               (None, 8)            200         dropout_109[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.1243 - acc: 0.9694 - val_loss: 8.3447e-07 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0600 - acc: 0.9875 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0549 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0524 - acc: 0.9893 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0510 - acc: 0.9896 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0500 - acc: 0.9898 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0493 - acc: 0.9898 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0488 - acc: 0.9901 - val_loss: 2.7418e-06 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0483 - acc: 0.9901 - val_loss: 5.1260e-06 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0480 - acc: 0.9902 - val_loss: 1.1921e-06 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0476 - acc: 0.9902 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0475 - acc: 0.9903 - val_loss: 1.0729e-06 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0474 - acc: 0.9904 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0474 - acc: 0.9904 - val_loss: 5.9605e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0473 - acc: 0.9905 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0473 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0471 - acc: 0.9905 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0472 - acc: 0.9905 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0471 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0471 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0469 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0470 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0469 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0473 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0471 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0471 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0472 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0471 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0471 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0473 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0474 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0474 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0474 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0475 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0479 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0475 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0477 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0477 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0482 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0485 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 8.9646e-04 - acc: 0.9997
Epoch 2/40
 - 3s - loss: 4.7362e-04 - acc: 0.9997
Epoch 3/40
 - 3s - loss: 4.1240e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 4.0792e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 3.1029e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 2.0338e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 2.9971e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 1.6816e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 1.4115e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 1.4956e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 2.5805e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 1.1232e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.0212e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.1935e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 4.7785e-05 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 7.8275e-05 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 8.8786e-05 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.0013e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 2.0426e-05 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.3884e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.8777e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 6.5602e-06 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.2975e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 5.9437e-06 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 4.2222e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 9.1500e-06 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.5033e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 4.0207e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 8.1196e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 3.9739e-06 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 6.4229e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 3.1454e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.1598e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.0455e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 6.4565e-06 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 4.0053e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 4.5360e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 6.9590e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 2.5080e-06 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 2.1789e-06 - acc: 1.0000
# Training time = 0:21:47.700047
# F-Score(Ordinary) = 0.147, Recall: 0.214, Precision: 0.112
# F-Score(ireflv) = 0.092, Recall: 0.75, Precision: 0.049
# F-Score(id) = 0.335, Recall: 0.87, Precision: 0.207
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_219 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_220 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_219 (Embedding)       (None, 4, 48)        705264      input_219[0][0]                  
__________________________________________________________________________________________________
embedding_220 (Embedding)       (None, 4, 24)        5640        input_220[0][0]                  
__________________________________________________________________________________________________
flatten_219 (Flatten)           (None, 192)          0           embedding_219[0][0]              
__________________________________________________________________________________________________
flatten_220 (Flatten)           (None, 96)           0           embedding_220[0][0]              
__________________________________________________________________________________________________
concatenate_110 (Concatenate)   (None, 288)          0           flatten_219[0][0]                
                                                                 flatten_220[0][0]                
__________________________________________________________________________________________________
dense_219 (Dense)               (None, 24)           6936        concatenate_110[0][0]            
__________________________________________________________________________________________________
dropout_110 (Dropout)           (None, 24)           0           dense_219[0][0]                  
__________________________________________________________________________________________________
dense_220 (Dense)               (None, 8)            200         dropout_110[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 0.5
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.1202 - acc: 0.9702 - val_loss: 3.5883e-05 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0605 - acc: 0.9874 - val_loss: 3.3379e-06 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0552 - acc: 0.9886 - val_loss: 7.7486e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0525 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0509 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0500 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0490 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0487 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0482 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0477 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0476 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0473 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0471 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0471 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0470 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0471 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0470 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0471 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0468 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0470 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0467 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0469 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0468 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0468 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0467 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0469 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0468 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0468 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0470 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0472 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0472 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0471 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0475 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0474 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0473 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0473 - acc: 0.9909 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0475 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0475 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0475 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0478 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0019 - acc: 0.9993
Epoch 2/40
 - 3s - loss: 9.6753e-04 - acc: 0.9995
Epoch 3/40
 - 3s - loss: 6.1039e-04 - acc: 0.9998
Epoch 4/40
 - 3s - loss: 6.0890e-04 - acc: 0.9997
Epoch 5/40
 - 3s - loss: 4.1637e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 3.1190e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 2.8615e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 8.2971e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 4.8206e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 2.4106e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 4.1345e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 1.1989e-04 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 2.9318e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 2.3470e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 1.7564e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 2.9131e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 1.1683e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.7455e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 4.6306e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 2.3136e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 2.8865e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 2.8819e-04 - acc: 0.9999
Epoch 23/40
 - 3s - loss: 6.4852e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.1553e-04 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 6.3086e-04 - acc: 0.9998
Epoch 26/40
 - 3s - loss: 4.0057e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 1.7166e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 3.4221e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 3.4159e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 2.8420e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 2.8375e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 3.3973e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 1.6993e-04 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 3.3877e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 1.6928e-04 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.1298e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 3.9359e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 2.2465e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 1.6841e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 3.3590e-04 - acc: 0.9999
# Training time = 0:21:51.197504
# F-Score(Ordinary) = 0.026, Recall: 0.462, Precision: 0.013
# F-Score(ireflv) = 0.089, Recall: 0.462, Precision: 0.049
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_221 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_222 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_221 (Embedding)       (None, 4, 48)        705264      input_221[0][0]                  
__________________________________________________________________________________________________
embedding_222 (Embedding)       (None, 4, 24)        5640        input_222[0][0]                  
__________________________________________________________________________________________________
flatten_221 (Flatten)           (None, 192)          0           embedding_221[0][0]              
__________________________________________________________________________________________________
flatten_222 (Flatten)           (None, 96)           0           embedding_222[0][0]              
__________________________________________________________________________________________________
concatenate_111 (Concatenate)   (None, 288)          0           flatten_221[0][0]                
                                                                 flatten_222[0][0]                
__________________________________________________________________________________________________
dense_221 (Dense)               (None, 24)           6936        concatenate_111[0][0]            
__________________________________________________________________________________________________
dropout_111 (Dropout)           (None, 24)           0           dense_221[0][0]                  
__________________________________________________________________________________________________
dense_222 (Dense)               (None, 8)            200         dropout_111[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 1.0
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.0909 - acc: 0.9786 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0540 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0508 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0496 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0488 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0482 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0481 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0480 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0478 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0479 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0477 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0481 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0482 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0483 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0485 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0488 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0491 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0493 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0495 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0497 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0501 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0500 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 31s - loss: 0.0503 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 31s - loss: 0.0505 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 31s - loss: 0.0509 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 31s - loss: 0.0511 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 31s - loss: 0.0516 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 31s - loss: 0.0518 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 31s - loss: 0.0520 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 31s - loss: 0.0523 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 31s - loss: 0.0523 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 31s - loss: 0.0524 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 31s - loss: 0.0524 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 31s - loss: 0.0522 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 31s - loss: 0.0529 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 31s - loss: 0.0534 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 31s - loss: 0.0535 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 31s - loss: 0.0532 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 31s - loss: 0.0533 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 31s - loss: 0.0535 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0014 - acc: 0.9997
Epoch 2/40
 - 3s - loss: 0.0013 - acc: 0.9997
Epoch 3/40
 - 3s - loss: 0.0015 - acc: 0.9996
Epoch 4/40
 - 3s - loss: 6.1992e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 6.5601e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 7.7676e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 5.3107e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 3.2201e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 4.2647e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 5.2762e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 4.2009e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 3.3625e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 2.6458e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 3.4249e-04 - acc: 0.9998
Epoch 15/40
 - 3s - loss: 2.2135e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 2.0287e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 2.1677e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 1.7370e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 1.1093e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 3.0576e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.3023e-05 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 8.6499e-05 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 6.1640e-06 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 7.8258e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 1.0123e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.5044e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 3.6064e-06 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 7.4525e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 6.5383e-06 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 7.4431e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 7.3352e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 7.3099e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 2.7040e-06 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 2.9319e-06 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 7.1384e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 7.3208e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 7.1591e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.4119e-06 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 7.1965e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 7.1968e-05 - acc: 1.0000
# Training time = 0:22:36.402475
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_223 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_224 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_223 (Embedding)       (None, 4, 48)        705264      input_223[0][0]                  
__________________________________________________________________________________________________
embedding_224 (Embedding)       (None, 4, 24)        5640        input_224[0][0]                  
__________________________________________________________________________________________________
flatten_223 (Flatten)           (None, 192)          0           embedding_223[0][0]              
__________________________________________________________________________________________________
flatten_224 (Flatten)           (None, 96)           0           embedding_224[0][0]              
__________________________________________________________________________________________________
concatenate_112 (Concatenate)   (None, 288)          0           flatten_223[0][0]                
                                                                 flatten_224[0][0]                
__________________________________________________________________________________________________
dense_223 (Dense)               (None, 24)           6936        concatenate_112[0][0]            
__________________________________________________________________________________________________
dropout_112 (Dropout)           (None, 24)           0           dense_223[0][0]                  
__________________________________________________________________________________________________
dense_224 (Dense)               (None, 8)            200         dropout_112[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 1.0
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 30s - loss: 0.0925 - acc: 0.9784 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0543 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0512 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0495 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0486 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0483 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0479 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0477 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0476 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0475 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0474 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0473 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0472 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0473 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0474 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0476 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0476 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0474 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0479 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0480 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0480 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0484 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0483 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0481 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 31s - loss: 0.0486 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 31s - loss: 0.0486 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0487 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0491 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0496 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0495 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0496 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0497 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0495 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0496 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0499 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0504 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0502 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0506 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0504 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0503 - acc: 0.9904 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0019 - acc: 0.9995
Epoch 2/40
 - 3s - loss: 6.6598e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 3.1954e-04 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 2.7060e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 5.3162e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 6.1745e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 7.0282e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 6.1319e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 4.3674e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 3.4883e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 8.7418e-05 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 5.2159e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 2.6037e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 6.0575e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 5.1780e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 4.3049e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 4.2954e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 8.6013e-05 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 4.2839e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 8.5768e-05 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 4.2721e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 1.7076e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 3.4083e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 4.2514e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 3.3949e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 5.9245e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 3.3781e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 3.3721e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 5.0456e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 4.1948e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 5.0208e-04 - acc: 0.9999
Epoch 32/40
 - 3s - loss: 7.5050e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 3.3267e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 3.3208e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 6.6218e-04 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 3.3028e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 4.1196e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 3.2893e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 2.4633e-04 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 7.3669e-04 - acc: 0.9999
# Training time = 0:22:15.713930
# F-Score(Ordinary) = 0.42, Recall: 0.574, Precision: 0.331
# F-Score(lvc) = 0.515, Recall: 0.569, Precision: 0.47
# F-Score(ireflv) = 0.29, Recall: 0.337, Precision: 0.254
# F-Score(id) = 0.376, Recall: 0.825, Precision: 0.244
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_225 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_226 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_225 (Embedding)       (None, 4, 48)        705264      input_225[0][0]                  
__________________________________________________________________________________________________
embedding_226 (Embedding)       (None, 4, 24)        5640        input_226[0][0]                  
__________________________________________________________________________________________________
flatten_225 (Flatten)           (None, 192)          0           embedding_225[0][0]              
__________________________________________________________________________________________________
flatten_226 (Flatten)           (None, 96)           0           embedding_226[0][0]              
__________________________________________________________________________________________________
concatenate_113 (Concatenate)   (None, 288)          0           flatten_225[0][0]                
                                                                 flatten_226[0][0]                
__________________________________________________________________________________________________
dense_225 (Dense)               (None, 24)           6936        concatenate_113[0][0]            
__________________________________________________________________________________________________
dropout_113 (Dropout)           (None, 24)           0           dense_225[0][0]                  
__________________________________________________________________________________________________
dense_226 (Dense)               (None, 8)            200         dropout_113[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 1.0
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 30s - loss: 0.0926 - acc: 0.9781 - val_loss: 1.2040e-05 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0549 - acc: 0.9887 - val_loss: 1.5736e-05 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0517 - acc: 0.9895 - val_loss: 1.3471e-05 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0499 - acc: 0.9898 - val_loss: 9.1196e-06 - val_acc: 1.0000
Epoch 5/40
 - 30s - loss: 0.0495 - acc: 0.9900 - val_loss: 7.8678e-06 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0489 - acc: 0.9901 - val_loss: 1.6689e-06 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0485 - acc: 0.9902 - val_loss: 3.6955e-06 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0483 - acc: 0.9902 - val_loss: 3.2187e-06 - val_acc: 1.0000
Epoch 9/40
 - 30s - loss: 0.0484 - acc: 0.9903 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 10/40
 - 30s - loss: 0.0485 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 30s - loss: 0.0487 - acc: 0.9903 - val_loss: 1.7881e-07 - val_acc: 1.0000
Epoch 12/40
 - 30s - loss: 0.0485 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 30s - loss: 0.0490 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 30s - loss: 0.0487 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 30s - loss: 0.0487 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 30s - loss: 0.0487 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 30s - loss: 0.0486 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 30s - loss: 0.0491 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 30s - loss: 0.0489 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 30s - loss: 0.0490 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0492 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 30s - loss: 0.0494 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0491 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 30s - loss: 0.0495 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 30s - loss: 0.0495 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 30s - loss: 0.0496 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 30s - loss: 0.0496 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0500 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 30s - loss: 0.0501 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0499 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0499 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0496 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0500 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 30s - loss: 0.0504 - acc: 0.9908 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0507 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 30s - loss: 0.0504 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 30s - loss: 0.0510 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 30s - loss: 0.0505 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0508 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 30s - loss: 0.0510 - acc: 0.9907 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0017 - acc: 0.9994
Epoch 2/40
 - 3s - loss: 6.7607e-04 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 2.4895e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 2.3783e-04 - acc: 1.0000
Epoch 5/40
 - 3s - loss: 1.2149e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 3.3819e-05 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 4.4566e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.8690e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 1.1686e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 2.2198e-04 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 5.5780e-06 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.4721e-06 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.1603e-06 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 7.4234e-07 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 8.9928e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 6.0862e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 2.1807e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.0906e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 3.2503e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.0900e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 3.2741e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 2.1733e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.0869e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.0875e-04 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 2.2918e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 2.3837e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 2.1700e-04 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 2.1684e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 2.9193e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.0848e-04 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 2.1658e-04 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 2.4784e-07 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.8952e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.8541e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 2.1639e-04 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.0822e-04 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.6332e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 2.1615e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.9537e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.0811e-04 - acc: 1.0000
# Training time = 0:22:27.750037
# F-Score(Ordinary) = 0, Recall: 0, Precision: 0
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_227 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_228 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_227 (Embedding)       (None, 4, 48)        705264      input_227[0][0]                  
__________________________________________________________________________________________________
embedding_228 (Embedding)       (None, 4, 24)        5640        input_228[0][0]                  
__________________________________________________________________________________________________
flatten_227 (Flatten)           (None, 192)          0           embedding_227[0][0]              
__________________________________________________________________________________________________
flatten_228 (Flatten)           (None, 96)           0           embedding_228[0][0]              
__________________________________________________________________________________________________
concatenate_114 (Concatenate)   (None, 288)          0           flatten_227[0][0]                
                                                                 flatten_228[0][0]                
__________________________________________________________________________________________________
dense_227 (Dense)               (None, 24)           6936        concatenate_114[0][0]            
__________________________________________________________________________________________________
dropout_114 (Dropout)           (None, 24)           0           dense_227[0][0]                  
__________________________________________________________________________________________________
dense_228 (Dense)               (None, 8)            200         dropout_114[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 1.0
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 30s - loss: 0.0941 - acc: 0.9778 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0549 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0512 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0498 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0489 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0484 - acc: 0.9901 - val_loss: 3.5763e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0482 - acc: 0.9901 - val_loss: 2.3842e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0481 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0480 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0478 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0476 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0479 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0476 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0478 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0481 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0484 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0484 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0488 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0486 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0488 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0489 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0488 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 31s - loss: 0.0491 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 31s - loss: 0.0495 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 31s - loss: 0.0499 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 31s - loss: 0.0499 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 31s - loss: 0.0497 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 31s - loss: 0.0496 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 31s - loss: 0.0497 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 31s - loss: 0.0499 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 31s - loss: 0.0499 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 31s - loss: 0.0503 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 31s - loss: 0.0505 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0505 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0508 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0508 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0513 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0512 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0516 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0518 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 9.2050e-04 - acc: 0.9997
Epoch 2/40
 - 3s - loss: 4.3136e-04 - acc: 0.9998
Epoch 3/40
 - 3s - loss: 5.1970e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 6.9001e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 2.9501e-04 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 5.8757e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 6.8353e-04 - acc: 0.9999
Epoch 8/40
 - 3s - loss: 3.8987e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 4.8641e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 5.8228e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 7.7415e-04 - acc: 0.9999
Epoch 12/40
 - 3s - loss: 9.6825e-05 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 6.7512e-04 - acc: 0.9999
Epoch 14/40
 - 3s - loss: 7.6929e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 3.8387e-04 - acc: 0.9999
Epoch 16/40
 - 3s - loss: 7.6569e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 6.6804e-04 - acc: 0.9999
Epoch 18/40
 - 3s - loss: 6.6622e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 4.7480e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 3.7922e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 7.5643e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 4.7162e-04 - acc: 0.9999
Epoch 23/40
 - 3s - loss: 7.5256e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 2.8170e-04 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 5.6224e-04 - acc: 0.9999
Epoch 26/40
 - 3s - loss: 4.6753e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 3.7339e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 4.6587e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 5.5776e-04 - acc: 0.9999
Epoch 30/40
 - 3s - loss: 4.6381e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 1.6672e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 2.7791e-04 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 3.6997e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 2.7713e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 4.6102e-04 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 3.6818e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 5.5108e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 7.3264e-04 - acc: 0.9999
Epoch 39/40
 - 3s - loss: 3.6551e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 3.6491e-04 - acc: 0.9999
# Training time = 0:23:07.469452
# F-Score(Ordinary) = 0.303, Recall: 0.631, Precision: 0.199
# F-Score(lvc) = 0.073, Recall: 1.0, Precision: 0.038
# F-Score(ireflv) = 0.505, Recall: 0.56, Precision: 0.459
# F-Score(id) = 0.237, Recall: 0.771, Precision: 0.14
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_229 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_230 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_229 (Embedding)       (None, 4, 48)        705264      input_229[0][0]                  
__________________________________________________________________________________________________
embedding_230 (Embedding)       (None, 4, 24)        5640        input_230[0][0]                  
__________________________________________________________________________________________________
flatten_229 (Flatten)           (None, 192)          0           embedding_229[0][0]              
__________________________________________________________________________________________________
flatten_230 (Flatten)           (None, 96)           0           embedding_230[0][0]              
__________________________________________________________________________________________________
concatenate_115 (Concatenate)   (None, 288)          0           flatten_229[0][0]                
                                                                 flatten_230[0][0]                
__________________________________________________________________________________________________
dense_229 (Dense)               (None, 24)           6936        concatenate_115[0][0]            
__________________________________________________________________________________________________
dropout_115 (Dropout)           (None, 24)           0           dense_229[0][0]                  
__________________________________________________________________________________________________
dense_230 (Dense)               (None, 8)            200         dropout_115[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 1.0
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 30s - loss: 0.0934 - acc: 0.9780 - val_loss: 4.7684e-07 - val_acc: 1.0000
Epoch 2/40
 - 30s - loss: 0.0552 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0516 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0499 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0489 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0484 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0479 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0479 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0477 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0476 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0475 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0477 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0478 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0481 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0478 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0482 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0484 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0484 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0483 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0485 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0482 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 31s - loss: 0.0488 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 31s - loss: 0.0489 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 31s - loss: 0.0488 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 31s - loss: 0.0487 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 31s - loss: 0.0492 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 31s - loss: 0.0491 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 31s - loss: 0.0492 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 31s - loss: 0.0491 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 31s - loss: 0.0493 - acc: 0.9906 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 31s - loss: 0.0495 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 31s - loss: 0.0494 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 31s - loss: 0.0500 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 31s - loss: 0.0503 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0504 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0503 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0511 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0511 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0517 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0518 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0011 - acc: 0.9997
Epoch 2/40
 - 3s - loss: 4.6711e-04 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 1.7410e-04 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 4.2811e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 3.4153e-04 - acc: 0.9999
Epoch 6/40
 - 3s - loss: 6.0894e-07 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 2.5567e-04 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 2.5531e-04 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 4.2442e-04 - acc: 0.9999
Epoch 10/40
 - 3s - loss: 1.6966e-04 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.6946e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 1.6933e-04 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 1.6917e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 3.3759e-04 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 2.5285e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 5.0445e-04 - acc: 0.9999
Epoch 17/40
 - 3s - loss: 1.6800e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 4.1897e-04 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 4.1806e-04 - acc: 0.9999
Epoch 20/40
 - 3s - loss: 2.0848e-07 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 2.5046e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.6684e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.6669e-04 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 3.3275e-04 - acc: 0.9999
Epoch 25/40
 - 3s - loss: 1.8769e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 2.4921e-04 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 3.3169e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 5.7890e-04 - acc: 0.9999
Epoch 29/40
 - 3s - loss: 1.6519e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 5.7656e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 8.2359e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 7.3823e-04 - acc: 0.9999
Epoch 33/40
 - 3s - loss: 6.5370e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 2.4463e-04 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 5.6928e-04 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 6.4837e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 4.0409e-04 - acc: 0.9999
Epoch 38/40
 - 3s - loss: 2.4206e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 4.0260e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 4.0167e-04 - acc: 0.9999
# Training time = 0:23:38.145345
# F-Score(Ordinary) = 0.553, Recall: 0.685, Precision: 0.463
# F-Score(lvc) = 0.533, Recall: 0.718, Precision: 0.424
# F-Score(ireflv) = 0.717, Recall: 0.595, Precision: 0.902
# F-Score(id) = 0.319, Recall: 0.949, Precision: 0.192
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_231 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_232 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_231 (Embedding)       (None, 4, 48)        705264      input_231[0][0]                  
__________________________________________________________________________________________________
embedding_232 (Embedding)       (None, 4, 24)        5640        input_232[0][0]                  
__________________________________________________________________________________________________
flatten_231 (Flatten)           (None, 192)          0           embedding_231[0][0]              
__________________________________________________________________________________________________
flatten_232 (Flatten)           (None, 96)           0           embedding_232[0][0]              
__________________________________________________________________________________________________
concatenate_116 (Concatenate)   (None, 288)          0           flatten_231[0][0]                
                                                                 flatten_232[0][0]                
__________________________________________________________________________________________________
dense_231 (Dense)               (None, 24)           6936        concatenate_116[0][0]            
__________________________________________________________________________________________________
dropout_116 (Dropout)           (None, 24)           0           dense_231[0][0]                  
__________________________________________________________________________________________________
dense_232 (Dense)               (None, 8)            200         dropout_116[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 2
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 30s - loss: 0.0756 - acc: 0.9827 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0515 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0500 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0495 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0493 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0494 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0495 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0499 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0496 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0501 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0503 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0507 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0510 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0514 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0515 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0523 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0522 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0531 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0536 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0537 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0548 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0546 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0550 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0554 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0558 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0559 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0558 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0567 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0577 - acc: 0.9891 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0582 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0588 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0590 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 31s - loss: 0.0593 - acc: 0.9890 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 31s - loss: 0.0599 - acc: 0.9889 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 31s - loss: 0.0602 - acc: 0.9888 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 31s - loss: 0.0607 - acc: 0.9887 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 31s - loss: 0.0618 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 31s - loss: 0.0623 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 31s - loss: 0.0636 - acc: 0.9886 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 31s - loss: 0.0636 - acc: 0.9885 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0037 - acc: 0.9995
Epoch 2/40
 - 3s - loss: 0.0022 - acc: 0.9997
Epoch 3/40
 - 3s - loss: 0.0032 - acc: 0.9996
Epoch 4/40
 - 3s - loss: 0.0024 - acc: 0.9997
Epoch 5/40
 - 3s - loss: 0.0028 - acc: 0.9997
Epoch 6/40
 - 3s - loss: 0.0034 - acc: 0.9996
Epoch 7/40
 - 3s - loss: 0.0028 - acc: 0.9996
Epoch 8/40
 - 3s - loss: 0.0015 - acc: 0.9998
Epoch 9/40
 - 3s - loss: 0.0025 - acc: 0.9997
Epoch 10/40
 - 3s - loss: 0.0019 - acc: 0.9997
Epoch 11/40
 - 3s - loss: 0.0019 - acc: 0.9997
Epoch 12/40
 - 3s - loss: 0.0019 - acc: 0.9997
Epoch 13/40
 - 3s - loss: 0.0018 - acc: 0.9998
Epoch 14/40
 - 3s - loss: 0.0021 - acc: 0.9997
Epoch 15/40
 - 3s - loss: 0.0019 - acc: 0.9997
Epoch 16/40
 - 3s - loss: 0.0024 - acc: 0.9996
Epoch 17/40
 - 3s - loss: 0.0016 - acc: 0.9998
Epoch 18/40
 - 3s - loss: 0.0018 - acc: 0.9997
Epoch 19/40
 - 3s - loss: 0.0025 - acc: 0.9996
Epoch 20/40
 - 3s - loss: 0.0020 - acc: 0.9997
Epoch 21/40
 - 3s - loss: 0.0024 - acc: 0.9996
Epoch 22/40
 - 3s - loss: 0.0012 - acc: 0.9998
Epoch 23/40
 - 3s - loss: 0.0016 - acc: 0.9997
Epoch 24/40
 - 3s - loss: 0.0021 - acc: 0.9996
Epoch 25/40
 - 3s - loss: 0.0015 - acc: 0.9997
Epoch 26/40
 - 3s - loss: 0.0016 - acc: 0.9997
Epoch 27/40
 - 3s - loss: 0.0017 - acc: 0.9997
Epoch 28/40
 - 3s - loss: 0.0022 - acc: 0.9996
Epoch 29/40
 - 3s - loss: 0.0019 - acc: 0.9996
Epoch 30/40
 - 3s - loss: 0.0017 - acc: 0.9997
Epoch 31/40
 - 3s - loss: 0.0015 - acc: 0.9997
Epoch 32/40
 - 3s - loss: 0.0014 - acc: 0.9997
Epoch 33/40
 - 3s - loss: 0.0014 - acc: 0.9997
Epoch 34/40
 - 3s - loss: 0.0013 - acc: 0.9997
Epoch 35/40
 - 3s - loss: 0.0013 - acc: 0.9997
Epoch 36/40
 - 3s - loss: 0.0019 - acc: 0.9996
Epoch 37/40
 - 3s - loss: 0.0015 - acc: 0.9996
Epoch 38/40
 - 3s - loss: 0.0013 - acc: 0.9997
Epoch 39/40
 - 3s - loss: 0.0014 - acc: 0.9996
Epoch 40/40
 - 3s - loss: 0.0019 - acc: 0.9995
# Training time = 0:22:58.937235
# F-Score(Ordinary) = 0.431, Recall: 0.325, Precision: 0.642
# F-Score(lvc) = 0.455, Recall: 0.472, Precision: 0.439
# F-Score(ireflv) = 0.752, Recall: 0.694, Precision: 0.82
# F-Score(id) = 0.287, Recall: 0.188, Precision: 0.601
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_233 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_234 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_233 (Embedding)       (None, 4, 48)        705264      input_233[0][0]                  
__________________________________________________________________________________________________
embedding_234 (Embedding)       (None, 4, 24)        5640        input_234[0][0]                  
__________________________________________________________________________________________________
flatten_233 (Flatten)           (None, 192)          0           embedding_233[0][0]              
__________________________________________________________________________________________________
flatten_234 (Flatten)           (None, 96)           0           embedding_234[0][0]              
__________________________________________________________________________________________________
concatenate_117 (Concatenate)   (None, 288)          0           flatten_233[0][0]                
                                                                 flatten_234[0][0]                
__________________________________________________________________________________________________
dense_233 (Dense)               (None, 24)           6936        concatenate_117[0][0]            
__________________________________________________________________________________________________
dropout_117 (Dropout)           (None, 24)           0           dense_233[0][0]                  
__________________________________________________________________________________________________
dense_234 (Dense)               (None, 8)            200         dropout_117[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 2
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.0767 - acc: 0.9827 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0518 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0500 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0490 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0488 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0491 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0490 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0491 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0495 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0500 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0496 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0499 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0500 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0500 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0503 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0509 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0515 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0512 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0521 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0523 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0526 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0527 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0533 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0532 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0534 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0544 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0542 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0551 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0558 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0557 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0564 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0567 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0568 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0573 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0584 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0594 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0608 - acc: 0.9895 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0605 - acc: 0.9894 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0620 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0611 - acc: 0.9893 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0128 - acc: 0.9984
Epoch 2/40
 - 3s - loss: 0.0128 - acc: 0.9983
Epoch 3/40
 - 3s - loss: 0.0103 - acc: 0.9985
Epoch 4/40
 - 3s - loss: 0.0094 - acc: 0.9985
Epoch 5/40
 - 3s - loss: 0.0085 - acc: 0.9985
Epoch 6/40
 - 3s - loss: 0.0081 - acc: 0.9984
Epoch 7/40
 - 3s - loss: 0.0063 - acc: 0.9986
Epoch 8/40
 - 3s - loss: 0.0067 - acc: 0.9983
Epoch 9/40
 - 3s - loss: 0.0055 - acc: 0.9984
Epoch 10/40
 - 3s - loss: 0.0045 - acc: 0.9984
Epoch 11/40
 - 3s - loss: 0.0032 - acc: 0.9986
Epoch 12/40
 - 3s - loss: 0.0030 - acc: 0.9983
Epoch 13/40
 - 3s - loss: 0.0018 - acc: 0.9987
Epoch 14/40
 - 3s - loss: 0.0015 - acc: 0.9999
Epoch 15/40
 - 3s - loss: 0.0011 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 7.6933e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 4.9039e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 3.8172e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 2.5139e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.6199e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 1.2962e-04 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.0940e-04 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 8.5251e-05 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 6.5117e-05 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 6.9185e-05 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 5.6797e-05 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 4.8139e-05 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 4.2893e-05 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 3.8930e-05 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 3.1883e-05 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 2.9699e-05 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 3.1293e-05 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 2.7534e-05 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 2.3732e-05 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 2.8388e-05 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 2.7147e-05 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 2.0225e-05 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.7348e-05 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.7873e-05 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 2.0270e-05 - acc: 1.0000
# Training time = 0:21:53.613714
# F-Score(Ordinary) = 0.524, Recall: 0.431, Precision: 0.667
# F-Score(lvc) = 0.448, Recall: 0.401, Precision: 0.508
# F-Score(ireflv) = 0.694, Recall: 0.568, Precision: 0.893
# F-Score(id) = 0.557, Recall: 0.509, Precision: 0.617
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_235 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_236 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_235 (Embedding)       (None, 4, 48)        705264      input_235[0][0]                  
__________________________________________________________________________________________________
embedding_236 (Embedding)       (None, 4, 24)        5640        input_236[0][0]                  
__________________________________________________________________________________________________
flatten_235 (Flatten)           (None, 192)          0           embedding_235[0][0]              
__________________________________________________________________________________________________
flatten_236 (Flatten)           (None, 96)           0           embedding_236[0][0]              
__________________________________________________________________________________________________
concatenate_118 (Concatenate)   (None, 288)          0           flatten_235[0][0]                
                                                                 flatten_236[0][0]                
__________________________________________________________________________________________________
dense_235 (Dense)               (None, 24)           6936        concatenate_118[0][0]            
__________________________________________________________________________________________________
dropout_118 (Dropout)           (None, 24)           0           dense_235[0][0]                  
__________________________________________________________________________________________________
dense_236 (Dense)               (None, 8)            200         dropout_118[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 2
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.0771 - acc: 0.9825 - val_loss: 1.0550e-05 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0521 - acc: 0.9893 - val_loss: 3.3975e-06 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0504 - acc: 0.9897 - val_loss: 4.0531e-06 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0494 - acc: 0.9899 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0494 - acc: 0.9900 - val_loss: 7.1526e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0492 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0493 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0498 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0495 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0497 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0499 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0497 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0502 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0498 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0500 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0502 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0506 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0513 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0515 - acc: 0.9904 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0510 - acc: 0.9905 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0516 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0517 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0524 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0524 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0519 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0528 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0518 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0528 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0533 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0526 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0539 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0536 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0541 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0544 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0550 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0551 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0549 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0552 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0555 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0557 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 0.0015 - acc: 0.9997
Epoch 2/40
 - 3s - loss: 0.0010 - acc: 0.9999
Epoch 3/40
 - 3s - loss: 2.9604e-04 - acc: 1.0000
Epoch 4/40
 - 3s - loss: 0.0010 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 4.4158e-04 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 7.3427e-04 - acc: 0.9999
Epoch 7/40
 - 3s - loss: 2.9329e-04 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 7.3160e-04 - acc: 0.9999
Epoch 9/40
 - 3s - loss: 4.3813e-04 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 7.2857e-04 - acc: 0.9999
Epoch 11/40
 - 3s - loss: 2.9100e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 8.7084e-04 - acc: 0.9999
Epoch 13/40
 - 3s - loss: 2.8977e-04 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 4.3401e-04 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 2.8902e-04 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 4.3288e-04 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 0.0017 - acc: 0.9998
Epoch 18/40
 - 3s - loss: 0.0010 - acc: 0.9999
Epoch 19/40
 - 3s - loss: 4.2789e-04 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 7.1156e-04 - acc: 0.9999
Epoch 21/40
 - 3s - loss: 5.6792e-04 - acc: 0.9999
Epoch 22/40
 - 3s - loss: 8.4955e-04 - acc: 0.9999
Epoch 23/40
 - 3s - loss: 7.0590e-04 - acc: 0.9999
Epoch 24/40
 - 3s - loss: 4.2269e-04 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 4.2201e-04 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 5.6159e-04 - acc: 0.9999
Epoch 27/40
 - 3s - loss: 9.7978e-04 - acc: 0.9999
Epoch 28/40
 - 3s - loss: 4.1885e-04 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 4.1818e-04 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 9.7292e-04 - acc: 0.9999
Epoch 31/40
 - 3s - loss: 4.1591e-04 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 4.1523e-04 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 6.9046e-04 - acc: 0.9999
Epoch 34/40
 - 3s - loss: 5.5103e-04 - acc: 0.9999
Epoch 35/40
 - 3s - loss: 8.2423e-04 - acc: 0.9999
Epoch 36/40
 - 3s - loss: 8.2152e-04 - acc: 0.9999
Epoch 37/40
 - 3s - loss: 2.7332e-04 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 4.0936e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 5.4471e-04 - acc: 0.9999
Epoch 40/40
 - 3s - loss: 4.0777e-04 - acc: 1.0000
# Training time = 0:21:52.793377
# F-Score(Ordinary) = 0.517, Recall: 0.785, Precision: 0.385
# F-Score(lvc) = 0.128, Recall: 1.0, Precision: 0.068
# F-Score(ireflv) = 0.606, Recall: 0.789, Precision: 0.492
# F-Score(id) = 0.624, Recall: 0.761, Precision: 0.528
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_237 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_238 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_237 (Embedding)       (None, 4, 48)        705264      input_237[0][0]                  
__________________________________________________________________________________________________
embedding_238 (Embedding)       (None, 4, 24)        5640        input_238[0][0]                  
__________________________________________________________________________________________________
flatten_237 (Flatten)           (None, 192)          0           embedding_237[0][0]              
__________________________________________________________________________________________________
flatten_238 (Flatten)           (None, 96)           0           embedding_238[0][0]              
__________________________________________________________________________________________________
concatenate_119 (Concatenate)   (None, 288)          0           flatten_237[0][0]                
                                                                 flatten_238[0][0]                
__________________________________________________________________________________________________
dense_237 (Dense)               (None, 24)           6936        concatenate_119[0][0]            
__________________________________________________________________________________________________
dropout_119 (Dropout)           (None, 24)           0           dense_237[0][0]                  
__________________________________________________________________________________________________
dense_238 (Dense)               (None, 8)            200         dropout_119[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 2
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 29s - loss: 0.0771 - acc: 0.9823 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0518 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0496 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0491 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0493 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0496 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0497 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0497 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0501 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0501 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0499 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 31s - loss: 0.0504 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 31s - loss: 0.0504 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 31s - loss: 0.0511 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 31s - loss: 0.0512 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 31s - loss: 0.0514 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 31s - loss: 0.0514 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 31s - loss: 0.0518 - acc: 0.9903 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 31s - loss: 0.0524 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 31s - loss: 0.0528 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 31s - loss: 0.0527 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0527 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0534 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0537 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0548 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0547 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0553 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0556 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0559 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 30/40
 - 29s - loss: 0.0562 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 31/40
 - 29s - loss: 0.0558 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 32/40
 - 29s - loss: 0.0568 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 33/40
 - 29s - loss: 0.0568 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 34/40
 - 29s - loss: 0.0566 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 35/40
 - 29s - loss: 0.0567 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 36/40
 - 29s - loss: 0.0576 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 37/40
 - 29s - loss: 0.0575 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 38/40
 - 29s - loss: 0.0573 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 39/40
 - 29s - loss: 0.0587 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 40/40
 - 29s - loss: 0.0592 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 1/40
 - 3s - loss: 8.1052e-04 - acc: 0.9998
Epoch 2/40
 - 3s - loss: 6.2058e-05 - acc: 1.0000
Epoch 3/40
 - 3s - loss: 2.2981e-04 - acc: 0.9999
Epoch 4/40
 - 3s - loss: 1.5389e-04 - acc: 0.9999
Epoch 5/40
 - 3s - loss: 4.4018e-05 - acc: 1.0000
Epoch 6/40
 - 3s - loss: 2.1952e-04 - acc: 1.0000
Epoch 7/40
 - 3s - loss: 2.1394e-05 - acc: 1.0000
Epoch 8/40
 - 3s - loss: 1.0309e-05 - acc: 1.0000
Epoch 9/40
 - 3s - loss: 3.2677e-05 - acc: 1.0000
Epoch 10/40
 - 3s - loss: 6.6908e-06 - acc: 1.0000
Epoch 11/40
 - 3s - loss: 1.8771e-04 - acc: 1.0000
Epoch 12/40
 - 3s - loss: 3.6722e-07 - acc: 1.0000
Epoch 13/40
 - 3s - loss: 2.6968e-06 - acc: 1.0000
Epoch 14/40
 - 3s - loss: 1.2559e-06 - acc: 1.0000
Epoch 15/40
 - 3s - loss: 1.3963e-07 - acc: 1.0000
Epoch 16/40
 - 3s - loss: 9.2188e-07 - acc: 1.0000
Epoch 17/40
 - 3s - loss: 1.5980e-04 - acc: 1.0000
Epoch 18/40
 - 3s - loss: 1.5984e-04 - acc: 1.0000
Epoch 19/40
 - 3s - loss: 6.2121e-07 - acc: 1.0000
Epoch 20/40
 - 3s - loss: 1.5936e-04 - acc: 1.0000
Epoch 21/40
 - 3s - loss: 3.1445e-07 - acc: 1.0000
Epoch 22/40
 - 3s - loss: 1.7417e-07 - acc: 1.0000
Epoch 23/40
 - 3s - loss: 1.5439e-07 - acc: 1.0000
Epoch 24/40
 - 3s - loss: 1.9832e-07 - acc: 1.0000
Epoch 25/40
 - 3s - loss: 2.0569e-07 - acc: 1.0000
Epoch 26/40
 - 3s - loss: 1.5635e-07 - acc: 1.0000
Epoch 27/40
 - 3s - loss: 1.6291e-07 - acc: 1.0000
Epoch 28/40
 - 3s - loss: 1.3642e-07 - acc: 1.0000
Epoch 29/40
 - 3s - loss: 1.4088e-07 - acc: 1.0000
Epoch 30/40
 - 3s - loss: 1.4617e-07 - acc: 1.0000
Epoch 31/40
 - 3s - loss: 1.2582e-07 - acc: 1.0000
Epoch 32/40
 - 3s - loss: 1.5922e-04 - acc: 1.0000
Epoch 33/40
 - 3s - loss: 1.4380e-07 - acc: 1.0000
Epoch 34/40
 - 3s - loss: 1.3682e-07 - acc: 1.0000
Epoch 35/40
 - 3s - loss: 1.4837e-07 - acc: 1.0000
Epoch 36/40
 - 3s - loss: 1.3209e-07 - acc: 1.0000
Epoch 37/40
 - 3s - loss: 1.5134e-07 - acc: 1.0000
Epoch 38/40
 - 3s - loss: 1.5912e-04 - acc: 1.0000
Epoch 39/40
 - 3s - loss: 1.4706e-07 - acc: 1.0000
Epoch 40/40
 - 3s - loss: 1.4643e-07 - acc: 1.0000
# Training time = 0:22:25.464940
# F-Score(Ordinary) = 0.081, Recall: 0.426, Precision: 0.045
# F-Score(ireflv) = 0.226, Recall: 0.413, Precision: 0.156
# F-Score(id) = 0.01, Recall: 1.0, Precision: 0.005
********************
********************
# XP = FR: 
********************
********************
Deep model(No padding)
# Language = FR
# Train file = train.conllu.autoPOS.autoDep
# Test file = test.conllu.autoPOS.autoDep
FRidxDic
# Train = 16091, Test = 1788
# Tokens vocabulary = 14693
# POSs vocabulary = 235
# embedding: frWac_non_lem_no_postag_no_phrase_200_cbow_cut100.bin
# token vocabulary = 14693
# POS vocabulary = 235
# Padding = False
# Embedding = True
# Initialisation = False
# Concatenation = False
# Lemma  = True
# Token/Lemma emb = 48
# POS = True
# POS emb = 24
# Features = False
# Parameters = 718040
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_239 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
input_240 (InputLayer)          (None, 4)            0                                            
__________________________________________________________________________________________________
embedding_239 (Embedding)       (None, 4, 48)        705264      input_239[0][0]                  
__________________________________________________________________________________________________
embedding_240 (Embedding)       (None, 4, 24)        5640        input_240[0][0]                  
__________________________________________________________________________________________________
flatten_239 (Flatten)           (None, 192)          0           embedding_239[0][0]              
__________________________________________________________________________________________________
flatten_240 (Flatten)           (None, 96)           0           embedding_240[0][0]              
__________________________________________________________________________________________________
concatenate_120 (Concatenate)   (None, 288)          0           flatten_239[0][0]                
                                                                 flatten_240[0][0]                
__________________________________________________________________________________________________
dense_239 (Dense)               (None, 24)           6936        concatenate_120[0][0]            
__________________________________________________________________________________________________
dropout_120 (Dropout)           (None, 24)           0           dense_239[0][0]                  
__________________________________________________________________________________________________
dense_240 (Dense)               (None, 8)            200         dropout_120[0][0]                
==================================================================================================
Total params: 718,040
Trainable params: 718,040
Non-trainable params: 0
__________________________________________________________________________________________________
None
data size before sampling = 212145
data size after sampling = 728574
class_weight{0.0: 1.0, 1.0: 1.0, 2.0: 10.0, 4.0: 10.0, 5.0: 10.0, 6.0: 10.0, 7.0: 10.0}
# Network optimizer = adadelta, learning rate = 2
Train on 655716 samples, validate on 72858 samples
Epoch 1/40
 - 30s - loss: 0.0778 - acc: 0.9824 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 2/40
 - 29s - loss: 0.0523 - acc: 0.9892 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 3/40
 - 29s - loss: 0.0500 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 4/40
 - 29s - loss: 0.0490 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 5/40
 - 29s - loss: 0.0488 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 6/40
 - 29s - loss: 0.0487 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 7/40
 - 29s - loss: 0.0484 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 8/40
 - 29s - loss: 0.0489 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 9/40
 - 29s - loss: 0.0488 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 10/40
 - 29s - loss: 0.0492 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 11/40
 - 29s - loss: 0.0493 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 12/40
 - 29s - loss: 0.0493 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 13/40
 - 29s - loss: 0.0494 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 14/40
 - 29s - loss: 0.0497 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 15/40
 - 29s - loss: 0.0495 - acc: 0.9902 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 16/40
 - 29s - loss: 0.0501 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 17/40
 - 29s - loss: 0.0504 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 18/40
 - 29s - loss: 0.0504 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 19/40
 - 29s - loss: 0.0509 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 20/40
 - 29s - loss: 0.0514 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 21/40
 - 29s - loss: 0.0515 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 22/40
 - 29s - loss: 0.0518 - acc: 0.9901 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 23/40
 - 29s - loss: 0.0523 - acc: 0.9900 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 24/40
 - 29s - loss: 0.0527 - acc: 0.9898 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 25/40
 - 29s - loss: 0.0529 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 26/40
 - 29s - loss: 0.0534 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 27/40
 - 29s - loss: 0.0532 - acc: 0.9899 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 28/40
 - 29s - loss: 0.0541 - acc: 0.9896 - val_loss: 1.1921e-07 - val_acc: 1.0000
Epoch 29/40
 - 29s - loss: 0.0541 - acc: 0.9897 - val_loss: 1.1921e-07 - val_acc: 1.0000
